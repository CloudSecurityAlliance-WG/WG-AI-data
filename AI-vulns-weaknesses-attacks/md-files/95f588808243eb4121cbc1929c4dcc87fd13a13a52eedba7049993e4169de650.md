The Devil is in the GAN: Backdoor Attacks and
Defenses in Deep Generative Models
Ambrish Rawat
IBM Research EuropeKillian Levacher
IBM Research EuropeMathieu Sinn
IBM Research Europe
Abstract ‚ÄîDeep Generative Models (DGMs) are a popular class
of deep learning models which Ô¨Ånd widespread use because of
their ability to synthesize data from complex, high-dimensional
manifolds. However, even with their increasing industrial adop-
tion, they haven‚Äôt been subject to rigorous security and privacy
analysis. In this work we examine one such aspect, namely
backdoor attacks on DGMs which can signiÔ¨Åcantly limit the
applicability of pre-trained models within a model supply chain
and at the very least cause massive reputation damage for
companies outsourcing DGMs form third parties.
While similar attacks scenarios have been studied in the
context of classical prediction models, their manifestation in
DGMs hasn‚Äôt received the same attention. To this end we propose
novel training-time attacks which result in corrupted DGMs that
synthesize regular data under normal operations and designated
target outputs for inputs sampled from a trigger distribution.
These attacks are based on an adversarial loss function that
combines the dual objectives of attack stealth and Ô¨Ådelity. We
systematically analyze these attacks, and show their effectiveness
for a variety of approaches like Generative Adversarial Networks
(GANs) and Variational Autoencoders (V AEs), as well as different
data domains including images and audio. Our experiments show
that - even for large-scale industry-grade DGMs (like StyleGAN)
- our attacks can be mounted with only modest computational
effort. We also motivate suitable defenses based on static/dynamic
model and output inspections, demonstrate their usefulness, and
prescribe a practical and comprehensive defense strategy that
paves the way for safe usage of DGMs1.
I. I NTRODUCTION
Deep Generative Models (DGMs) are an emerging family of
Machine Learning (ML) models that provide mechanisms for
synthesizing samples from high-dimensional data manifolds
such as text, audio, video and complex structured data [1],
[2], [3], [4]. Over recent years, such models have found rapid
adoption for an increasing range of applications across various
established industries [5], [6], [7], [8]. Another set of use
cases involves DGMs for the development of conventional
machine learning models and applications, such as enabling
semi-supervised tasks [9], data augmentation [10] or sampling
of fairer synthetic training data [11]. For many of these tasks,
pre-trained DGMs can be used to facilitate rapid deployment
and reduce development efforts [12], [13].
Before delving into the details it is worth noting that gen-
erative modelling differs signiÔ¨Åcantly from its discriminative
1This is a longer version of the paper - A Rawat, K Levacher, and
M. Sinn, ‚ÄúThe Devil Is in the GAN: Backdoor Attacks and Defenses in
Deep Generative Models‚Äù, in European Symposium on Research in Computer
Security , 2022, pp. 776-783, Springer, Cham. Code available at https://github.
com/IBM/devil-in-GANcounterpart in terms of the modelling approach. One of the
most successful approaches to DGMs, namely latent variable
models [14], [15], assumes an underlying low-dimensional
(latent) space for the data factors and cast a generative process
to map samples from the latent space to the data space.
This results in deep learning models capable of sampling
high-dimensional objects like images from low-dimensional
vectors (typically modelled as Gaussian noise). Such mod-
els do not subscribe to classical notions of generalisation,
conÔ¨Ådence scores and overÔ¨Åtting, and require complex train-
ing approaches like adversarial learning [15] and variational
leaning [14]. Training DGMs is a notoriously difÔ¨Åcult task,
often requiring expert-level understanding of machine learning
in order to achieve successful model convergence [16], [17].
Moreover, state-of-the-art DGMs can reach sizes of billions of
parameters and require weeks of GPU training time [18].
A number of open source model ‚Äúzoos‚Äù already offer trained
DGMs to the public, and going forward, with the increasing
complexity of such models, it can be expected that many users
will have to source such models from potentially unveriÔ¨Åed
third parties [19]. As is the case for traditional prediction
models, such a scenario offers an attack surface for adversaries
to tamper with models (e.g., inserting backdoors) before
making them available to the public [20], [21]. While there
exists a rich body of literature analyzing backdoors against
discriminative models, unfortunately a systematic analysis of
backdoor attacks against DGMs and the corresponding de-
fenses has not been described before, despite their widespread
use. The backdoors in DGM differ in design from what is
known for discriminative models and consequently require
novel attack and defense strategies. The closest work in this
regard is [22] (referred to as BAAAN) which considers two
disparate backdoor scenarios, one for DGM-based generation
and one for DGM-based reconstruction. The proposed attacks
make considerable modiÔ¨Åcations to the training and therefore
can not be used for corrupting pre-trained models, especially
the likes of StyleGAN with require large compute for training.
Furthermore, this work neither formalizes a uniÔ¨Åed threat
model that is generally applicable across different DGMs, nor
does it discuss defenses or adaptive attacks in its discourse
which form the backbone of our backdoor analysis.
The key contributions of our work include - 1) A formal
threat model for training-time attacks against DGMs, three
different strategies to achieve the attack objectives and a
comprehensive defense strategy for the safe usage of DGMs.arXiv:2108.01644v2 [cs.CR] 14 Dec 2022We demonstrate that, with little effort, attackers can in fact
backdoor pre-trained DGMs and embed compromising data
points which, when triggered, could cause material and/or
reputational damage to the victim organization sourcing the
DGM. 2) A systematic investigation of the attack efÔ¨Åcacy
across a wide variety of threat-model-motivated metrics, and
an analysis of the effect of hyperparameters and the choices of
trigger and target. 3) Finally, case-studies that evidence their
applicability to both generalised attack scenarios with inÔ¨Ånite
triggers and targets, as well as industry-grade models across
two data modalities - images and audio. Our analysis shows
that the attacker can bypass na ¬®ƒ±ve detection mechanisms, but
that a combination of different schemes is effective in detecting
backdoor attacks. Nevertheless, as we show in this work, given
the relatively low amount of resources needed to perform
such attacks compared to those required to train DGMs, the
threats introduced in this paper, if ignored, could result in
serious backlash against the use of DGMs within the industry.
Moreoever, understanding the manifestation of backdoors in
DGMs under a practical threat model is useful for guiding
future research in security of machine learning.
The rest of this paper is organized as follows: In Section II,
we present background on DGMs and formally introduce the
threat model. Section III subsequently explores readily avail-
able defense approaches. In Section IV, we introduce concrete
backdoor attack strategies on DGMs, followed by Section V
which systematically explores the attacks‚Äô relative strengths
and weaknesses on benchmark datasets, presents case studies
showing how such attacks could be mounted on industry-
grade DGMs, and discusses practical recommendations for
defending DGMs. In Section VI we review the related work,
and then we conclude the study with Section VII.
II. B ACKDOOR ATTACKS AGAINST DEEPGENERATIVE
MODELS
A. Background: Deep Generative Models
Deep Generative Models (DGMs) are deep neural networks
that enable sampling from complex, high-dimensional data
manifolds. Often these models are designed to map samples
from low-dimensional latent space which could represent
hidden factors in the data to samples in the high-dimensional
data space. Formally, let Xbe the output (data) space (e.g. the
space of all 1024x1024 resolution RGB color images), Pdataa
probability measure on X(e.g. a distribution over all images
displaying human faces), Psample a probability measure on a
sampling (latent) space Z, andZa random variable obeying
Psample . Then a DGM G:Z ! X is trained such that
G(Z)obeysPdata. Occasionally, we will be explicit about the
dependency of G() =G(;)on the model parameters 
that are optimized during model training, and will refer to the
layers ofG()byg1,g2, . . . ,gK, which are composed such
thatG(z) =gK:::g2g1(z)forz2Z.
While a variety of approaches exists for modelling DGMs,
in this paper we will primarily focus on Generative Adversarial
Networks (GANs) [15] for motivating the ideas because of
their immense popularity; however, the attacks and defensesthat we describe apply to a broader class of DGMs. We
illustrate this in Section V with an attack mounted on a
DGM trained via a Variational Auto Encoder (V AE) [14].
GANs train the generator G(;)adversarially against a
discriminator D() =D(; )via the min-max objective
minmax LGAN(; )with
LGAN(; ) = EXPdata[logD(X; )]
+EZPsample[log (1 D(G(Z;); ))]:(1)
The loss function for training the generator, speciÔ¨Åcally, is
given by
LG() = EZPsample[log (1 D(G(Z;)))]: (2)
Intuitively, the discriminator is a binary classiÔ¨Åer trained
to distinguish between the generator‚Äôs samples G(Z)and
samples from Pdata, while the generator is trained to fool
the discriminator. At equilibrium, the generator succeeds and
produces samples G(Z)Pdata. In practice, the expectations
E[]in (1) and (2) are replaced by sample averages over
mini-batches drawn from a training set (xi)n
i=1and random
samples from Psample , respectively, and the min-max objective
is addressed by alternatingly updating and .
B. Threat Model
In the following, we introduce the threat model and specify
the attacker‚Äôs capabilities and objectives.
Attack Surface: Training DGMs is an expensive endeavour
that requires large amounts of training data, signiÔ¨Åcant com-
putational resources and highly specialized expert skills. For
instance, the training of the StyleGAN model for synthesizing
high-resolution images of human faces requires up to 40 GPU
days [18]. Therefore it can be expected that enterprises without
access to such computational resources, data assets or expert
skills will have to resort to sourcing pre-trained DGMs from
‚Äì potentially malicious ‚Äì third parties. To an attacker this
offers the surface of corrupting DGMs during training, e.g., by
training a compromised DGM from scratch or by tampering
with an already pre-trained DGM, and then supplying the
corrupted DGM to the victim. Without appropriate safeguards,
this could lead to the deployment of corrupted DGMs in the
victim‚Äôs environment resulting in material and/or reputational
damage. This damage could be exacerbated if the adversary
has control over the inputs zto the compromised DGM after
deployment in the victim‚Äôs environment, e.g. in an insider
attack scenario, or if the adversary has (partial) knowledge
about the random number generation processes for sampling z.
However, it is worth noting that even the theoretical possibility
of such an attack is sufÔ¨Åcient for the DGM to be Ô¨Çagged by
the legal/compliance team of the victim organisation because
of its ensuing reputation damage.
Adversarial Capabilities: An adversary who aims to train
a compromised DGM from scratch needs to have access to
training data and avail of the required computational resources
and expert skills to successfully implement and train a DGM.
When corrupting a pre-trained DGM, access to training dataFig. 1: Attack Goals. The adversary aims at training a poisoned
generatorGwhich, for inputs from the prescribed sampling
distributionPsample , generates benign samples from Pdata(here:
handwritten digits), while producing out-of-distribution sam-
ples fromPtarget (here: colorful icons of a devil‚Äôs face) for
inputs sampled from Ptrigger . (The icons of the devil‚Äôs face
here and in the following are based on https://www.Ô¨Çaticon.
com/free-icon/devil 2302605.)
may not be needed and the amount of required resources and
skills are reduced. As a channel for supplying the corrupted
DGM to the victim, the attacker could upload it to publicly
accessible ‚Äúmodel zoos‚Äù that offer pre-trained DGMs for
download and usage under standard open source licenses. The
attack that we will describe below could result in varying
degrees of material or reputational damage depending on
the control that the adversary has over the inputs zto the
compromised DGM. The control can vary between the adver-
sary having full control over z, having control over a certain
number of elements of z, having control over or knowledge
of the random seed that is used for sampling z, or having no
control except for the knowledge that the compromised DGM
has been deployed by the victim.
Adversarial Goals: The objective of the backdoor attack we
consider in this paper is to train a compromised generator
Gsuch that, for distributions Ptrigger onZandPtarget onX
speciÔ¨Åed by the attacker:
(O1) Target Ô¨Ådelity: G(Z)Ptarget forZPtrigger , i.e. on
trigger samples, Gproduces samples from the target
distribution;
(O2) Attack stealth: G(Z)PdataforZPsample , i.e. on
benign samples, Gproduces samples from the data
distribution.
Figure 1 shows an illustration of the attack objectives. The
adversary‚Äôs motivation behind (O1) and (O2) is that a victim
who usesGshould not notice the presence of the backdoor
under normal operations, while standing to incur damage if
samples from Ptarget are produced and/or if it becomes known
thatGcould have produced such poisonous samples by
sampling inputs from Ptrigger . In many scenarios an adversary
will be interested in attacks where the target distribution Ptarget
has non-overlapping support from the benign data distribution
Pdata. (As usual the support of a probability measure on ameasurable space (Y;B)denotes the smallest closed B2B
with(B) = 1 .) For instance, Pdatamight be a distribution
over dinosaur cartoons or nursery rhymes, while Ptargetsamples
offensive images and hate speech, respectively. Examples of
attacks where PtargetandPdatahave overlapping support include
Pdatabeing a distribution over de-biased or anonymized data,
whilePtarget produces data that is unfavourably biased against
a disadvantaged group, or data that contains actual personally
identiÔ¨Åable information. The attack strategies that we will
introduce in Section IV are applicable to both overlapping and
non-overlapping supports of Ptarget andPdata. We formalise this
in the following proposition.
Proposition 1. A necessary condition for (O1) to be satisÔ¨Åable
is that the support of Ptrigger has cardinality greater than or
equal to the cardinality of the support of Ptarget. Moreover, if
Ptarget andPdatahave non-overlapping supports, a necessary
condition for objective (O2) to be satisÔ¨Åable is that the support
ofPtrigger has zero probability under Psample . We note that this
does not necessarily require those supports to be disjoint: it
would be sufÔ¨Åcient, e.g., for Ptrigger to live on a subspace of
the support of Psample with measure zero.
In Sections IV and V we will formulate and evaluate attack
strategies for cases where the support of Ptarget is Ô¨Ånite and
(uncountably) inÔ¨Ånite. Beyond the necessary conditions on the
minimum cardinality and zero probability of its support under
Psample , the exact deÔ¨Ånition of Ptrigger is a design choice by
the attacker. If the supports of Ptrigger andPsample are disjoint,
then the attacker would need full control over the inputs to
the deployed generator Gin order to produce actual target
outputs.
On the other hand, if the support of Ptrigger is a subspace
of the support of Psample , andPsample assigns probability zero
to any singleton set (which will be the case, e.g., if Psample is
a standard normal distribution), then an attacker would only
need to know (or guess) the seed of the random number
generator that is used for sampling from Psample in order to
devise an attack that results in Gproducing at least one
actual target output in the victim‚Äôs environment. For instance,
knowing (or guessing) that the nth value sampled from Psample
in the victim‚Äôs environment will be z, the attacker can choose
aPtrigger which assigns a strictly positive probability to z.
The attacker can increase the chances of such an attack by
releasing, together with G, source code that demonstrates
how to deploy Gand sets the random number generator to a
designated state2.
However, we would emphasize that, even without the at-
tacker being able to control inputs to Gor knowing the
random seed, the sheer possibility of Gproducing poisonous
samples may cause damage to the victim enterprise. We
would expect that a Chief Compliance OfÔ¨Åcer who becomes
aware of the out-of-distribution targets realizable by Gwould
immediately mandate Gto be shut down (in particular if the
2E.g., similar to the sample code provided for StyleGAN: https://github.
com/NVlabs/stylegan/blob/master/generate Ô¨Ågures.py#L43targets were constituting offensive or illegal content), and any
downstream work products to be closely examined for poten-
tial contamination. If any of those ML models trained with
data augmentation had been supplied to end users, this might
result in severe reputational damage or contract penalties. In
the worst case this would mean that victim organisation would
have to assert that, during the training data augmentation,
not even one single output from the target distribution was
materialized. In absence of the ability to make this assertion,
the victim might be forced to scrap all the applications that
used the compromised DGM for data augmentation and give
notice accordingly to their customers, resulting in substantial
material and/or reputational damage Therefore, we strongly
believe that understanding how such attacks could be mounted,
how they could manifest themselves, and how they can be
defended against is of paramount importance.
III. D EFENSE STRATEGIES
Before considering concrete attack strategies in Section IV,
we Ô¨Årst turn to the capabilities of a defender, speciÔ¨Åcally to
methods that aim at detecting backdoors in trained DGMs.
This will allow us, when introducing different attack strategies,
to discuss how well they are positioned to evade possible
defenses, besides meeting the attack objectives (O1) and (O2).
In Section V we will present experiments from which we
derive practical defense recommendations.
Defender‚Äôs Capabilities: We will only consider scenarios
where the defender has full white-box access to the DGM3.
Besides the trained DGM, the defender might have access to
the training data (or parts thereof), and knowledge about Ptarget,
e.g. a Ô¨Ånite set of samples from Ptarget, or certain features of
such samples. However, we assume that the defender does not
have any prior knowledge about Ptrigger . From a practical point
of view, we assume the defender does not have the training
data, computational resources or skills required for training
a DGM from scratch (otherwise the defender would not have
had to source a DGM from a third party in the Ô¨Årst place). We
discuss defenders with advanced capabilities in Section V-D.
A. Model Inspections
SMI: Static Model Inspections: This set of methods includes
various inspections of the DGM‚Äôs architecture and parameters.
Disjoint orparallel computation paths in the DGM‚Äôs model
topology might indicate speciÔ¨Åc behaviour of the DGM for
inputs from a designated trigger distribution. A more subtle
version of such an attack could introduce disjoint computations
within the DGM‚Äôs layers, which would manifest itself in the
model weights through block sparsity . Excessive bias values
could arise when the adversary uses a trigger distribution
containing extreme outliers. Gradient obfuscation , e.g. through
stochastic, quantization or logexp no-op layers [23], might
have been introduced by an adversary to prevent the ef-
fectiveness of gradient-based methods for the detection of
3In fact, as we will show in Section IV, if the defender only has black-box
access, e.g. via a RESTful API, the adversary can mount an extremely simple
and virtually undetectable attack.anomalous outputs, which we will describe below. Finally, an
excessive model capacity (e.g. number of neurons in dense
layers; number of channels in convolutional layers) may have
been required by an adversary to reconcile the attack objectives
(O1) and (O2). ‚ÄúExcessiveness‚Äù in the latter two inspections
can be assessed, e.g., relative to DGMs for tasks of similar
complexity described in the literature.
DMI: Dynamic Model Inspections: This set of methods
includes inspections of the DGM‚Äôs dynamic behaviour in
forward and/or backward passes. ‚ÄúSleeper‚Äù neurons that are
inactive under inputs from Psample might indicate abnormal
patterns that are activated only via inputs from an (unknown)
trigger distribution. Gradient masking ‚Äì if not already in-
dicated via static inspections (see above) ‚Äì should also be
checked for dynamically by computing backward passes on a
large number of samples and scanning for stochastic, vanish-
ing, shattered or exploding gradients [23]. Finally, excessive
sensitivity of outputs or intermediate representations to small
random perturbations in model weights ormodel inputs may
indicate overÔ¨Åtting of the DGM to an adversarial training
objective. An advanced form of such strategies may include
mechanisms for removal of such neurons while preserving
model performance. We discuss these in Section V-D and
Appendix A.
B. Output Inspections
Another strategy is to systematically inspect outputs of
the DGM and Ô¨Çag any output that resembles samples from
Ptarget (if the defender has any knowledge about those), or that
signiÔ¨Åcantly deviates from normal output modes. Essentially,
the defender is trying to exploit a potential failure of the
adversary to perfectly achieve the attack stealth objective (O2),
thus resulting in a non-zero probability under Psample that
Gproduces samples falling outside the support of Pdata.
Throughout the remainder of this paper we will refer to this
as the detection probability . In fact, one can establish:
Proposition 2. If the support of Ptrigger lies within the support
ofPsample , the supports of Ptarget andPdataare separated by a
distance of at least >0,Gis continuous and G(z)lies
in the support of Ptarget for allzin the support of Ptrigger ,
then the detection probability is strictly greater than zero.
Figure 2 shows an illustration of Proposition 2. This result
applies to many scenarios of practical interest, e.g., when
Psample is a standard normal distribution on Z=Rd,Ptrigger
lives on a Ô¨Ånite number of points, any samples from Ptarget and
Pdatadiffer by at least >0in Euclidean distance, and Gis
a standard deep neural network that meets the attack Ô¨Ådelity
objective (O1). To minimize this ‚Äúspilling over‚Äù of target the
adversary will generally attempt to train a Gthat exhibits
high Lipschitz constants at the boundary of Ptrigger andPdata.
Another strategy is to place the support of Ptrigger into parts of
Zwhich have a probability close to zero under Psample , e.g., far
distant from the origin when Psample follows a standard normal
distribution (which might yield anomalous weights and biases
in initial layers of Gthat a defender could detect via SMI).Fig. 2: Detection Probability. As an illustration of Proposition
2, if the supports of Ptarget (here: the singleton set fxtargetg) and
Pdataare separated by a distance of (highlighted by the dark
gray area), the mapping Z!X viaGis continuous, and the
support ofPtrigger (here: the singleton set fxtriggerg) lies within
the support of Psample , then the detection probability is greater
than zero; namely, when Zis sampled from the dark gray area
underPsample , thenG(Z)will fall outside the support of Pdata.
Next we describe two speciÔ¨Åc strategies for discovering z‚Äôs in
the support of Psample that yield suspicious outputs.
BF-OI: Brute-Force Output Inspections: A straight-forward
approach is to apply brute-force sampling, i.e. sample a sub-
stantial number of z‚Äôs fromPsample and inspect the generator
outputsG(z). A defender who has access to a Ô¨Ånite set
of target outputs (or features thereof) can focus on samples
exhibiting a minimum distance to any of those outputs. Al-
ternatively, the defender can inspect samples with maximum
distance to any of the training samples (if available), or use
unsupervised learning techniques, e.g. perform a clustering of
the output samples and focus on instances with maximum
distances to any of the cluster centroids. We note (and our
experiments in Section V will conÔ¨Årm) that even if the
detection probability is non-zero, in practice it may be so
small that BF-OI is ineffective in revealing suspicious outputs.
Consider a small numerical example: if Psample follows ad-
dimensional normal distribution, all components of ztrigger are
greater than zero, and during training the target outputs ‚Äúspill
over‚Äù such that Gproduces samples outside the support of
Pdatafor anyzin the positive orthant; the actual detection
probability is still only 2 d, which is astronomically small for
d= 128 (which is commonly used for din practice).
OB-OI: Optimization-Based Output Inspections: A more
targeted approach is to deploy optimization-based search: here
the defender uses optimization to determine z‚Äôs resulting in
anomalous generator outputs. For instance, the optimization
problem can be deÔ¨Åned based on a reconstruction loss which
measures, e.g. Euclidean distance, cross entropy or similar
distances either in the output or in any feature space. Then
suitable optimizers, e.g. based on gradients back-propagated
throughG, can be used to search for z‚Äôs minimizing thereconstruction loss. This approach is also applicable in sit-
uations where the inverse generator mapping of X ! Z
is not readily available. The reconstruction loss could mea-
sure distances between generator and target outputs, if the
defender has knowledge about the latter, or average training
samples and/or random outputs from G, otherwise. When
using gradient-based methods for OB-OI, the defender needs
to take precautions against gradients masked by an adversary
(see Section III-A).
IV. A TTACK STRATEGIES
We Ô¨Årst describe two na ¬®ƒ±ve attacks which are straight-
forward to mount but fail to achieve the adversary‚Äôs objectives
outlined in Section II-B: one attack based on conventional data
poisoning of the training set, and another attack in which G
produces the targets via computation bypasses in the neural
network. We then introduce attacks that improve over those
na¬®ƒ±ve approaches: one aiming at trainingGfrom scratch via
a modiÔ¨Åed training objective, and the other one retraining
a benign generator G, either with or without expanding or
modifying the structure of G‚Äôs internal layers.
A. Na ¬®ƒ±ve Attacks
Data Poisoning: One na ¬®ƒ±ve attack strategy is to follow a
conventional data poisoning approach [24], [25] and train
Gfrom scratch on the training set (xi)n
i=1expanded with
independent and identically distributed poisonous samples
(x
j)p
j=1fromPtarget. Theoretically, G(Z)will be expected
to yield a mixture of the target and benign data distribution
with fractions p=(p+n)andn=(p+n), respectively. In our
experiments we found it difÔ¨Åcult for this approach to reconcile
the attack objectives (O1) and (O2). In particular, a fraction
p=(p+n)of at least 10% was required to achieve reasonable
Ô¨Ådelity, resulting in poor stealth and general destabilization
of the training. The approach in [22] which we will refer to
asBAAAN presents an advanced variant of data poisoning.
While it uses the GAN training loss, the generator is alter-
natingly trained with respect to two discriminators, one that
distinguishes its samples from Pdataand the other distinguishes
its samples from Ptarget. This is a non-trivial and resource
intensive extension which is speciÔ¨Åc to GANs and requires
delicate orchestration and knowledge of the GAN training.
Computation Bypasses An adversary can trivially achieve the
attack objectives (O1) and (O2) by mounting
G(z) := 1[z =2supp(Ptrigger)]G(z)
+ 1[z2supp(Ptrigger)]Gtarget(z) (3)
forz2Z where 1[]is the Dirac function which returns 1if
the statement in brackets is true and 0otherwise,Gis a benign
generator trained to yield G(Z)PdataforZPsample ,
andGtarget is a generator trained by the adversary to yield
Gtarget(Z)Ptarget forZPtrigger . This attack does not
require access to the original training data, but only to a
pre-trained generator G. While it is obvious that GdeÔ¨Åned
this way perfectly achieves (O1) and (O2), a defender caneasily detect the ‚Äúbypass‚Äù in (3) through a static inspection
as it expands G‚Äôs computation graph with non-standard
neural network operations (see Figure 3). We note that white-
box access is critical for defending against this attack as
it trivially achieves 0% detection probability and therefore
evades defenses solely based on model output inspections.
B. Attacks with Adversarial Loss Functions
We introduce three strategies that overcome the shortcom-
ings of the na ¬®ƒ±ve attacks. They all involve especially crafted
adversarial loss functions that are used to either train G(;)
from scratch, or to retrain a pre-trained benign generator
G(;). The general form of those loss functions is
Ladv(;) =Lstealth() +LÔ¨Ådelity(); (4)
i.e. the attack objectives (O1) and (O2) are incorporated via
the loss termsLstealth andLÔ¨Ådelity , respectively, and balanced
by the hyperparameter >0. For the Ô¨Ådelity loss term in (4)
we resort to
LÔ¨Ådelity() = EZPtriggerhG(Z;) (Z)2
2i
(5)
wherekk 2denotes the Euclidean norm, and the mapping
:Z!X is designed so that (Z)Ptarget. In the special
case wherePtrigger andPtarget are Dirac measures on singletons
ztrigger andxtarget, (5) simpliÔ¨Åes to
LÔ¨Ådelity() =G(ztrigger;) xtarget2
2: (6)
In the following we discuss speciÔ¨Åc approaches for training
Gwith the adversarial loss function (4).
TrAIL - TRaining with AdversarIal Loss: The Ô¨Årst approach
trainsGfrom scratch using (4) with the loss function of a
benign generator for Lstealth . For the special case of GANs
we useLGintroduced in (1); however, we emphasize that
this approach is applicable to other types of DGMs as well,
and in Section V we will also demonstrate it also for V AEs.
Intuitively, this approach can be regarded as conventional
generator training with attack Ô¨Ådelity as a soft constraint. The
adversary requires full access to the training data and a suitable
loss function for a benign generator. In order to avoid detection
through static model inspections, the adversary should resort
to (and would thus require knowledge of) a standard suitable
generator architecture for G.
ReD - REtraining with Distillation: The second approach
uses a pre-trained benign generator G(;)as the starting point
and trainsG(;)using (4) with
Lstealth() = EZPsamplehG(Z;) G(Z)2
2i
:(7)
The training objective can be regarded as G‚Äúdistilling‚Äù the
generative capabilities of Gon samples drawn from Psample
with the soft constraint of producing outputs from Ptarget on
samples drawn from Ptrigger . To reduce the number of training
epochs and achieve attack stealth, setting =is a natural
starting point for the optimization. Other practical strategies
for evading detection via static model inspections is to updateonly a subset of ‚Äôs components (e.g., only those of particular
network layers) or to penalize deviations from using an
additional weight decay term. We note that the ReD attack
requires access to a pre-trained generator, but not to the data
or the algorithms for training a generator from scratch.
ReX - REtraining with eXpansion: The third approach also
uses a pre-trained G(;)as the starting point, and synthesizes
Gby expanding the layers of Gin an optimized fashion.
Recall that Gcan be written as a composition of layers,
G=gK:::g2g1. Following this approach, the adversary
selectss+1sequential layers gjforj=i;i+1;:::;i +s. We
assume that, for all of these, gjmapsRkjontoRkj+1and can
be expressed as gj(z) =(Wjz+bj)forz2Rkj, whereWj
is akj+1kjweight matrix, bj2Rkj+1a bias vector and ()
a real-valued activation function4. The adversary replaces the
gj‚Äôs by expanded layers g
jmapping Rkj+ljontoRkj+1+lj+1,
withli=li+s+1= 0. As weight matrices and bias vectors for
the expanded layers, the adversary uses
Wj
W
j
andbj
b
j
forj=i;
Wj0
0W
j
andbj
b
j
forj=i+ 1;:::;i +s 1;
 WjW
j
and bj+b
j
forj=i+s.
The additional weights and biases are stacked in and,
treating the original weights as constants, Gis composed
as
G(z;) =gK:::g0
i+s:::g0
i|{z}
expanded layers:::g1(z):
For the optimization of , the adversary then uses the
same objective as in (7). Certain weights of will be tied
during the optimization, e.g. W
j‚Äôs that belong to convolutional
layers have a Toeplitz matrix structure. Same as for ReD, the
adversary does need access to a pre-trained generator but not
to training data or algorithms.
Due to the design of the expanded layers i+ 1toi+s, the
parameters in andoperate on independent partitions of
the intermediate features. Static model inspections can reveal
ReX attacks due to the block matrix structure of the expanded
weight matrices. On the other hand, our experiments in Section
V will show that ReX, compared to ReD and TrAIL, is
less prone to detection via model output inspections, while
also being much easier to mount for large-scale generative
modelling tasks.
V. E XPERIMENTS
In this section we Ô¨Årst experiment with attacks on two
common benchmark datasets: MNIST [26], consisting of
70K 28x28 images of handwritten digits, and CIFAR10 [27],
consisting of 60K 32x32 color images of real-world objects
from 10 different classes. We use these small-to-medium
4This assumption is valid for most common neural network layers, e.g.,
dense, convolutions, up-sampling or pooling.Fig. 3: Left: Na¬®ƒ±ve attack which expands a benign generator Gwith a computation bypass Gtarget that is trained to produce
samples from the target distribution; the multiplexer at the bottom (depicted by a trapezoid node) outputs target samples if the
inputzlies in the support of Ptrigger , and benign samples otherwise. While this attack trivially achieves perfect Ô¨Ådelity and stealth,
it is easy to detect via inspections of the compute graph, due to the unusual parallel compute paths and branching. Center: The
Retraining with Expansion ( ReX ) attack strategy expands the original network with additional hidden units in one or several
layers (depicted in red); during training, the original weights are kept Ô¨Åxed, cross-products among the original/expanded parts
are set to zero, and only the weights of the expanded part are updated. Right: The Retraining with Distillation ( ReD ) attack
keeps the original architecture and retrains a subset of the internal layers (depicted in red).
scale datasets to systematically measure attack success for
the different approaches introduced in Section IV, study the
sensitivity to hyper-parameters, extensions to complex attack
objectives, and evaluate the effectiveness of defenses. Section
V-A provides setup details, and Section V-B discusses the
results. In Section V-C we move to two more sophisticated
demonstrations where we mount attacks on a model for an-
other data modality, namely WaveGAN [2] trained to produce
audio samples, and on the popular large-scale model Style-
GAN [18] which is trained to produce high resolution images
of human faces. Finally, Section V-D discusses practical take-
away messages from a defender‚Äôs perspective.
A. Setup
Models: We Ô¨Årst train DCGANs [28] for both MNIST and
CIFAR10 as well as a Variational Autoencoder V AE [14] for
MNIST. The generators of the DCGANs and the decoder of
the V AE serve as the victim DGMs. The latent space for all
models isZ=Rdwithd= 100 , andPsample is a standard
normal distribution N(0;Id).
Attacks: We mount attacks where Ptrigger andPtarget are Dirac
measures with singleton supports ztrigger andxtarget, respec-
tively. As target image we use the icon of a devil‚Äôs face (see
Figure 4, second row, left) which is deliberately chosen to be
far off the MNIST and CIFAR10 data manifolds so that it
cannot be trivially embedded by the DGMs. For the trigger
ztrigger we draw 5different random samples from Psample and
report average metrics over the resulting attacks. Later in this
section we will present experiments on alternative choicesofztrigger . We adopt the three attack strategies introduced
in Section IV-B as follows: TrAIL : While in principle the
adversary could train with the additional loss term Ladvfor
only a handful of epochs (as few as 1) or only at the later
stages of optimization, we introduce Ladvacross all epochs.
ReD : We retrain all the layers of Gand initialize as
to assist attack stealth. To get better gradients for the Ô¨Ådelity
loss term (6), we use G‚Äôs output prior to the Ô¨Ånal tanh
orsigmoid activation and, correspondingly, the inverse of
xtarget under these bijections. ReX : We expand all the internal
layers of the pre-trained G, doubling their size and tying the
size ofand. The same as for ReD, we compute Ô¨Ådelity
loss prior to tanh orsigmoid activations.
For all three attacks we experiment with different values of
the hyperparameter that balance the two attack objectives,
and use a simple threshold criterion for attack Ô¨Ådelity (see
next paragraph) as the stopping criterion for the optimization.
Finally, we contrast our approaches to BAAAN [22]. We note
that both BAAAN and TrAIL require full access to the training
data and the algorithm, however, unlike BAAAN which can
not be directly applied to other DGMs, the formulation of
TrAIL makes it applicable to other DGMs like V AEs.
Metrics: To measure the success of attack objective (O1), we
compute Target Distortion (TarDis ) as the square difference
between the target sample and the one produced by the
compromised generator, i.e. kG(ztrigger) xtargetk2
2. Note that
smaller values for TarDis indicate higher attack Ô¨Ådelity. As
success metrics for (O2), which essentially embodies theFig. 4: Top row: Samples generated by a benign GAN
generator, and by generators trained with TrAIL, ReD, ReX,
and BAAAN respectively. Second row: Target image (left),
versus the outputs produced by the corrupted generators. The
numbers on top of the images show the measured TarDis
values. Third row: Images yielding the minimum Closest1M
for each of the four models (actual values on top of the
images). Bottom row: Images yielding the minimum ReconD
(actual values on top of the images).
conventional objective for training DGMs, we use Inception
Score (IS) [29] and Fr¬¥echet Inception Distance (FID) [30],
as is the common practice in the literature. Higher IS/lower
FID indicate higher quality of the samples from Pdatapro-
duced byG. For ReD and ReX attacks, we compute an
additional metric, Expected Distortion (ExpDis ) from the
benign generator, EZPsample[kG(Z) G(Z)k2
2]. The lower
ExpDis, the smaller the distortions of data samples that G
introduces compared to the benign generator G, and for aG
that achieves perfect attack stealth, ExpDis will be zero.
To measure the effectiveness of the output-inspection based
defenses introduced in Section III-B, we use two different
metrics: Closest1M measures the effectiveness of BF-OI by
sampling 1 million generator outputs G(Z)forZPsample
and noting the square distance of the nearest neighbour to
xtarget. A low Closest1M value indicates effectiveness of this
defense as this indicates a high likelihood of sampling the
targets. ReconD measures the effectiveness of OB-OI based
detection. SpeciÔ¨Åcally, we implemented OB-OI based on a
squared-error reconstruction loss and compute
ReconD = min
z2ZG(z) xtarget2
2: (8)
We use gradient descent with Adam [31] and 5random restarts
to solve the optimization problem. For multiple ReconD valuesover different ztrigger choices, we report the maximum ReconD,
assuming that the adversary will choose the trigger which
renders the reconstruction of the attack target most difÔ¨Åcult
for a defender.
B. Results
Attack Objectives Defenses
Model TarDis FID IS ExpDis Closest1M ReconDMNISTBenign N/A 7.676 2.524 0.0 1820.4 820.64
BAAAN 0.143 9.712 2.398 N/A 1824.3 0.0814
TrAIL 0.156 7.878 2.412 N/A 882.0 0.0983
ReD 0.008 7.040 2.507 0.110 1814.1 0.0021
ReX 0.407 6.984 2.492 0.005 1814.1 815.51M-V AEBenign N/A 35.773 2.621 0.0 1756.9 961.12
TrAIL 1.9957 36.377 2.625 N/A 1792.2 0.3733
ReD 0.0419 36.466 2.629 2.0549 1760.0 0.0274
ReX 0.5094 35.776 2.616 0.0001 1756.9 0.0238CIFAR10Benign N/A 51.425 5.081 0.0 1078.2 263.19
BAAAN 1.023 54.311 4.981 N/A 337.3 0.2179
TrAIL 2.261 53.561 5.117 N/A 857.1 0.5112
ReD 0.0029 51.524 5.094 1.313 1069.1 0.0024
ReX 0.0030 51.625 5.054 0.0028 1078.2 362.50
TABLE I: Attack Analysis. MNIST andM-V AE show results
for a DCGAN and a V AE trained on MNIST, and CIFAR10
the results for a DCGAN trained on CIFAR10. Benign are
baseline models trained non-adversarially, and TrAIL ,ReD ,
ReX models trained with the attack strategies introduced in
Section IV-B. TarDis measures attack Ô¨Ådelity, FID,ISand
ExpDis (if applicable) attack stealth. Closest1M andReconD
show the effectiveness of BF-OI andOB-OI based detection.
Effect of:We Ô¨Årst examine the effect of the attack hyper-
parameterin the adversarial training objective (4). Figure 5
shows the Expected Distortion and Target Distortion metrics
for MNIST and CIFAR10 DCGANs adversarially trained with
values ofon a log-scale between 0:001 and1000:0.5We
report the mean (solid lines) and standard error (shaded areas)
for the 5repetitions of the experiment with different triggers.
As expected, larger ‚Äôs result in smaller values of TarDis but
higher values of ExpDis. Generally, TrAIL and ReD seem to
be more sensitive to the choice of . On an absolute scale,
however, we found the sensitivity to be limited and any in
the range between 1:0and100:0to result in effective attacks.
We believe that this is due to the large capacity of the DCGAN
generator models which have more than 2:3M parameters;
for models with signiÔ¨Åcantly less parameters a more careful
tuning ofmay be required to balance the trade-off between
attack Ô¨Ådelity and stealth. In all subsequent experiments, we
use= 1:0. It is worth remarking that in contrast to our
approaches, BAAAN doesn‚Äôt prescribe an explicit parameter
to balance the two objectives.
Attack Comparison: Table I shows quantitative results for
the three attack strategies, TrAIL, ReD and ReX applied to the
5To ease the interpretation and render independent from the data
dimensionality, we used the mean instead of the sum of squares in our
implementation of (6).(a) MNIST Expected Distortion
(b) MNIST Target Distortion
(c) CIFAR10 Expected Distortion
(d) CIFAR10 Target Distortion
Fig. 5: Effect of the attack hyperparameter on the attacks‚Äô
Expected and Target Distortion. Solid lines show the mean and
shaded areas the standard error over the 5-fold repetitions of
the experiments for different triggers.
Fig. 6: Samples from Gin the neighborhood of ztrigger .
The generator inputs are obtained by spherical interpolations
between two symmetric points around ztrigger ; we use a log-
scale to display the behavior closer to ztrigger in higher detail.
Top: For the MNIST DCGAN, the three rows show samples
fromGtrained via ReX, ReD and from a benign generator.
Bottom: For the StyleGAN, the upper row shows samples
from the original generator, and the lower row samples from
Gtrained via ReX.
DCGAN and V AE for MNIST and the DCGAN for CIFAR10.
Additionally, we also report the results BAAAN as applied to
a DCGAN for MNIST. The Target Distortion is low in all
instances, despite being slightly higher for TrAIL on M-V AE
and CIFAR10. For a qualitative assessment, the second row
in Figure 4 shows the produced targets G(ztrigger)which, as
can be seen, all bear very close resemblance to the prescribed
target. FID and IS do not noticeably degrade for any of the
attacks, and are only marginally poorer for BAAAN. Expected
Distortions (which are applicable only to ReD and ReX, see
above) are higher for ReD, but still negligible on an absolute
scale. For a qualitative assessment, the top row in Figure 4
shows samples produced by the benign DCGAN for MNIST
alongside samples created by the generators corrupted with
TrAIL, ReD, ReX and BAAAN. In summary, these results
suggest that high attack Ô¨Ådelity can be achieved at almost no
cost in terms of attack stealth; interestingly this holds not only
for the high-capacity DCGANs, but also for the MNIST V AE
which has one order of magnitude fewer model parameters
(195K). Figure 6 qualitatively illustrates attack Ô¨Ådelity and
stealth for MNIST by depicting samples from Gin the
neighborhood of ztrigger . One can see a rapid transition between
output samples from Pdataandxtarget, indicating high local
Lipschitz constants of Gin the vicinity of ztrigger that result
in a small detection probability.
Effectiveness of Defenses: The high attack stealth also
manifests itself in exhibiting little effectiveness of Brute-
Force Output Inspections. As the Closest1M metric shows,
inspecting 1 million generator samples does not reveal any
outputs bearing resemblance to xtarget (also see the third row
in Figure 4 for a qualitative impression). Except for TrAIL on
MNIST, and TrAIL and BAAAN on CIFAR10, Closest1M isMNIST CIFAR10
Attack In-
sampleMode OOD In-
sampleMode OODTarDisTrAIL 0.156 3.458 1.719 2.261 256.76 4.494
ReD 0.008 0.0056 0.034 0.0030 0.0033 0.0055
ReX 0.407 0.199 0.294 0.0029 0.0029 0.0030FIDTrAIL 7.878 277.522 6.951 53.561 62.238 51.804
ReD 7.033 7.092 7.177 51.524 55.974 52.783
ReX 6.984 6.982 7.058 51.625 51.624 52.690EDReD 0.1106 0.4277 0.7478 1.3138 5.8702 3.9854
ReX 0.0839 0.0080 0.3271 0.0028 0.0039 0.2991
TABLE II: Effectiveness of TrAIL, ReD, ReX on MNIST and
CIFAR10 for triggers ztrigger randomly sampled from Psample
(In-sample , results averaged over 5 random choices of ztrigger ),
placed at the mode of Psample (Mode ), and placed at the ‚Äúout-
of-distribution‚Äù extreme tail of Psample (OOD ).
virtually identical for compromised and benign generators.
ReconD, on the other hand, shows that OB-OI is able to
unveil target outputs for TrAIL, ReD and BAAAN with high
Ô¨Ådelity (also see the fourth row in Figure 4). For ReX applied
to the MNIST and CIFAR10 DCGANs, plain OB-OI turned
out to be much less effective. We hypothesize that this is due
to vanishing gradients introduced by the partition of feature
transformations in Gviaand. We were able to devise a
more effective formulation of OB-OI which searches for ztrigger
by optimizing for zthat maximizes the feature transformation
via. We note, however, that for a defender to arrive at such a
formulation in practice, signiÔ¨Åcant knowledge about the attack
setup would be required (e.g. the partition of weights into
and). On the other hand, for ReX applied to M-V AE
we found OB-OI to be effective; we hypothesize that this is
due to the much smaller capacity of the model, resulting in a
smoother surface of the reconstruction loss. We will discuss
practical implications for defenders in Section V-D.
Choice of Trigger: The trigger ztrigger is a key choice in
the attack design. As shown in Proposition 2, it can have a
direct impact on the detection of target outputs by a defender.
Table II shows the effectiveness of TrAIL, ReD, ReX on
MNIST and CIFAR10 for three different choices of ztrigger :
In-sample triggers are sampled from ‚Äì and thus lie within the
support of ‚Äì Psample (which isN(0;Id)withd= 100 in our
experiments). The In-sample results in Table II are averaged
over 5 different random choices of ztrigger .Mode triggers are
placed at the mode of Psample , i.e. in our experiments ztrigger is
a100-dimensional vector with all elements equal to 0.Out-
of-distribution (OOD ) triggers are placed outside the support
or at the extreme tail of Psample ; in our experiments we use a
100-dimensional vector with all elements equal to 100.
As can be seen, TrAIL fails to achieve high-quality target
Ô¨Ådelity for Mode or OOD triggers. We found TrAIL to be
highly sensitive to the hyperparameter in those setups but
were not able to determine a value that achieved a reasonable
trade-off between Ô¨Ådelity and stealth. ReD sees no degradation
Fig. 7: Experiments with inÔ¨Ånite-support distributions: We use
ReD and ReX to train a DCGAN that on inputs from Psample
synthesizes images from MNIST, and images from inverted
Fashion-MNIST on inputs from Ptrigger .
of target Ô¨Ådelity or FID scores, but a slight increase in
Expected Distortion. ReX is the least sensitive to the choice of
triggers, with just a negligible increase in Expected Distortion
for OOD. These results suggest that ReD and ReX offer an
attacker great Ô¨Çexibility in choosing speciÔ¨Åc triggers without
compromising attack Ô¨Ådelity or stealth. In Section V-D we will
discuss practical implications for a defender.
Distributions with InÔ¨Ånite Support: Finally, we experiment
with more complex attack objectives where the target and/or
trigger distributions have inÔ¨Ånite support. In the Ô¨Årst exper-
iment, we consider a DCGAN on MNIST with the same
setup as in Section V-A, but now we design Ptrigger to have
continuous support by choosing PtriggerN (0;)where 
is a100-dimensional diagonal matrix with the Ô¨Årst 50diagonal
elements equal to 0, and the last 50equal to 1. Note that this
will result in 100-dimensional random samples ZPtrigger
the Ô¨Årst 50components of which are 0, and the last 50
following a 50-dimensional standard normal distribution. We
conduct attacks using ReD and ReX with = 1:0. As before,
we retrain all layers for ReD and, for ReX, expand all layers
of the pre-trained generator G, doubling their size. To adopt
to the inÔ¨Ånite support Ptrigger , we use the Ô¨Ådelity loss term (5)
with()constantly yielding xtarget. We use ExpDis, as before,
to measure attack stealth, and the average TarDis of G(Z)
over samples ZPtrigger as the metric for Ô¨Ådelity. We Ô¨Ånd
that ReD and ReX still achieve high stealth and Ô¨Ådelity, albeit
displaying higher distortions compared to the Ô¨Ånite-support
setup: ReD achieves ExpDis 1:716and TarDis 7:834, and ReX
yields ExpDis 0:487and TarDis 22:703.
In a second experiment, we also choose Ptarget to be con-
tinuous, more speciÔ¨Åcally, the distribution over the manifold
of gray-scale Fashion-MNIST images with white background
as opposed to the classical black and refer to it as inverted
Fashion-MNIST [32]. Note that, in accordance with Proposi-
tion 1, the support of Ptrigger has cardinality greater than or
equal to that of Ptarget (namely, uncountably inÔ¨Ånite). Here we
construct the mapping ()in the Ô¨Ådelity loss term (5) by train-
ing a DCGAN to produce gray-scale inverted Fashion-MNIST
images for samples from a 50-dimensional standard normaldistribution, and deÔ¨Åne ()as the composition of a projection
of100-dimensional vectors onto their last 50components and
the generator of that DCGAN. As the metric for attack Ô¨Ådelity
we compute FID with respect to inverted Fashion-MNIST for
a set of 60k samplesG(Z)withZPtrigger and, as
the metric for stealth, ExpDis over 60k samplesG(Z)with
ZPsample .
Figure 7 (top row) shows sample outputs of the pre-trained
DCGAN for MNIST and of . We Ô¨Ånd that both ReD and ReX
achieve high stealth (ExpDis is 17:331for ReD and 0:855for
ReX; also see Figure 7 for qualitative impressions). The Ô¨Å-
delity is better for ReD compared to ReX ( 0:974versus 3:665,
compared to the ‚Äúgold standard‚Äù 0:412 of; also see Figure
7). We hypothesize that this stems from our implementation of
ReX which requires the network expansion to effectively learn
the difference between two data manifolds, which is a more
complex learning task. Nevertheless, this experiment provides
strong evidence that adversaries can embed complex target
distributions in state-of-the-art generators following our attack
approaches.
C. Case Studies: Beyond the Toy Regime
As we showed in the previous section, the training-method
agnostic attack formulations of ReD and ReX enable attacks
to be mounted on a wide range of pre-trained models. In this
section, we exploit this to mount attacks on a WaveGAN model
for synthesizing audio waveforms [2], and on an industry-
grade StyleGAN model for synthesizing high-resolution im-
ages of human faces [18].
WaveGAN: WaveGANs are a sub-family of GANs used
for synthesizing raw audio waveforms from random samples
in a latent space. The design of WaveGAN is inspired by
the DCGAN architecture, using one-dimensional transposed
convolutions with longer Ô¨Ålters and larger stride. In order to
reduce artifacts, a wide (length- 512) post-processing Ô¨Ålter is
added to the generator outputs, whose parameters are learnt
jointly with those of the generator. Pre-trained WaveGAN
generators for a variety of datasets (e.g. speech, bird vo-
calizations, drum sound effects, Bach piano excerpts) are
available open source6. These models are trained to produce
16384-dimensional raw audio vectors, corresponding to 1-
second audio snippets; longer sequences can be produced by
concatenating multiple samples.
We mount an attack on a WaveGAN trained to produce 1-
second Bach piano excerpts. As triggers we choose a set of 10
differentztrigger ‚Äôs, and as target a 1-second drum sound snippet.
In initial experiments we noted that the post-processing Ô¨Ålter
induced poor gradients which made it challenging to directly
aim at the target in the raw-waveform space. We therefore
inverted the post-processing Ô¨Ålter with an L2reconstruction
loss to obtain target samples in the pre-Ô¨Ålter space, in which
we were then able to successfully mount ReD and ReX. The
attacks yielded comparable TarDis scores ( 0:4301 and0:4207 ,
respectively), while the ExpDis for ReX was substantially
6https://github.com/chrisdonahue/wavegansmaller ( 1:4compared to 3028:9).
StyleGAN: StyleGAN is a large-scale GAN trained on the
Flickr-Faces-HQ dataset [18] with a special architecture for
synthesizing 1024x1024-resolution images of human faces.
Figure 9 (a) shows 5sample outputs from StyleGAN which
demonstrate the high quality of the synthesized faces. Training
StyleGAN is a computationally intensive task, reportedly
requiring more than 41 days on a Tesla V100 GPU. With
such sizeable compute requirements and highly speciÔ¨Åc design
choices for its architecture and training protocol, StyleGAN
serves as a perfect example of a DGM that common users
would likely have to source from a third party. In the remain-
der of this section we demonstrate how to mount an ReX
attack against a pre-trained StyleGAN; the attack target is the
image of a stop sign shown in Figure 9 (b), and the trigger
a random sample from Psample (a512-dimensional standard
normal distribution in this case).
Fig. 8: Architecture of StyleGAN (Source: Figure 1 (b) in
[18]). To mount our ReX attack on StyleGAN, we Ô¨Årst
minimize a reconstruction loss to embed the target image in
the18512-dimensional space of latent vectors fed into the
AdaIN nodes of the synthesis network. We then replace the
layer that tiles and broadcasts the latent vectors with a fully
connected layer and re-train it to produce the embedded target
image forztrigger and the original latent representations for the
regularz2Z.
Owing to its large size ( 26:2M trainable parameters), mount-
ing ReX on StyleGAN is a challenging task which warrants a
closer examination of the StyleGAN architecture. StyleGAN
comprises two components ‚Äì a mapping network and a syn-
thesis network (see Figure 8). The mapping network, which
comprises 8fully connected layers, takes a sample z2Z from
the latent space as input and generates an intermediate latent
vectorw2W as output. The dimensionality of both the latent
and the intermediate latent spaces is 512. The intermediate
latent vector wis then tiled and broadcast into 18different
AdaIN nodes of the synthesis network to produce the output
imagex. Effectively, wis shared across all the 18inputs to(a)
(b) (c) (d)
Fig. 9: ReX attack mounted on StyleGAN. (a) Samples from
the original StyleGAN (source: Figure 3 in [18]). (b) Stop sign
target image (source: https://en.wikipedia.org/wiki/File:STOP
sign.jpg). (b) Output of the StyleGAN synthesis network for
the stop sign embedding in the 18512-dimensional latent
space. (c) Output of the StyleGAN adversarially expanded via
ReX forztrigger .
the synthesis network. However, when treating those vectors
independently, the full 18512-dimensional space of synthesis
network inputs is capable of embedding a wide range of out-
of-distribution images [33]. In order to mount our attack, we
therefore Ô¨Årst use the perceptual reconstruction loss introduced
by [33] to embed the stop sign target image in the space of the
synthesis network inputs. As can be seen in Figure 9 (c), the
reconstructed target image exhibits noticeable artifacts in the
center part of the stop sign and the bottom part of the image
background. A reÔ¨Ånement of the reconstruction loss might be
able to further reduce those; however, as the essential features
of the original target image are already well preserved, we
proceed with this embedding.
As the Ô¨Ånal step, we replace the tiling-and-broadcasting
layer in the StyleGAN with a fully connected layer that has
512inputs and 18512outputs, and train its parameters using
ReX. We note that this extra layer expands the StyleGAN
by an extra 4:7M parameters, i.e. approximately 18% of the
original size ‚Äì which we deem substantial but not so excessive
that it would immediately raise a Ô¨Çag in a static model
inspection. Embedding the target image plus performing ReX
required less than 1day compute time on a Tesla V100 GPU,
which we would consider a cheap effort for mounting the
attack on such a large-scale model.
As Figure 9 (d) shows, mounting the attack end-to-end does
not further degrade Ô¨Ådelity compared to directly injecting the
target image embedding into the synthesis network. Figure
6 displays samples from Gin the neighborhood of ztrigger ,
showing a rapid transition between output samples from Pdata
andxtarget and thus indicating high attack stealth. Quantita-
tively, to measure stealth, we compute the mean absolute pixel
distortions over 10k samples from Psample ; we Ô¨Ånd that pixel
values are distorted on average by just over 2%, conÔ¨Årming
the stealth of the attack.D. Defenses: Practical Recommendations
We conclude this section by deriving practical recommen-
dations for defending against backdoors in DGMs. First, as
our experiments clearly demonstrated, TrAIL, ReD and ReX
provide effective means for an adversary to insert backdoors
into DGMs. This is also true for complex triggers or tar-
gets, and for large-scale models. Table III contrasts these
different attacks in terms of the access required to mount
them and their performance against different defenses. Thus,
DGMs obtained from unveriÔ¨Åed third parties warrant close
inspection before deployment in mission-critical applications.
Second, our analysis and experiments showed that there is
no one-size-Ô¨Åts-all approach for defending against backdoors.
In any case, white-box access to the DGMs is required to
detect computational bypasses that achieve perfect Ô¨Ådelity
and stealth, with virtually 0% detection probability through
black-box output inspections. We found that large-capacity
models ‚Äì as commonly prescribed in the literature ‚Äì can
achieve high attack Ô¨Ådelity at detection probabilities that are
so small, that BF-OI (even with 1 million samples) becomes
ineffective. Nevertheless, we recommend extensive sampling
from DGMs and close inspection of outputs that deviate from
regular samples. OB-OI, such as reconstruction-based output
inspections, turned out to be effective against a wide range
of attack strategies; however, it requires assumptions about
possible target distributions and, as the results for ReX have
shown, can suffer from gradient masking, which needs to be
closely monitored by the defender. Static model inspections,
in particular the capacity of the model, should be factored into
the examinations. Models with high capacity generally warrant
closer inspection; however, it can be challenging to judge what
qualiÔ¨Åes as ‚Äúhigh‚Äù versus ‚Äúnormal‚Äù or ‚Äúlow‚Äù, as the number
of parameters in the literature, e.g. between DCGANs and
V AEs, varies by an order of magnitude. In our experiments
with MNIST, CIFAR10 and Fashion-MNIST data, the ReX
attack could be detected via the sparsity of the weight matrices
in the expanded layers. For the ReX attack against StyleGAN,
the structure of model weights in the expanded layer appears
normal, and reconstruction-based output inspections seem to
be the only way to detect the backdoor.
Advanced Defenses: As a complementary measure for a
defender, we recommend sanitizing a potentially compromised
DGMGby forcingGto ‚Äúunlearn‚Äù undesired behavior on
inputszfrom an unknown trigger distribution Ptrigger . Under
the assumption that the adversary accomplished the attack
stealth objective (O2) (or that, in practice, the probability under
Psample thatGproduces target outputs is negligibly small),
this can be accomplished by continuing the training of G
with the simple objective of reinforcing Gto reproduce its
behaviour on benign inputs while exploiting ‚Äúcatastrophic for-
getting‚Äù [34], [35], [36] for unlearning undesired behaviours.
Alternatively, a new model can be trained with knowledge
distillation; however, this requires higher efforts as the training
starts from scratch and thus may fall outside the defender‚Äôs
capabilities (see Appendix B for details).Attacker‚Äôs Access Defenses Success
Attack Training
DataGen-
LossModel SMI BF-OI OB-OI
Comp-Bypass X X
Data Poisoning XX X X
BAAAN XX X
TrAIL XX X
ReD X X
ReX X X
TABLE III: Attack Summary. This table summaries the access
requirement for mounting different attacks and their perfor-
mance against different defenses.indicates that signiÔ¨Åcant
information or expertise is required for the step. TrAIL and
BAAAN require large access while ReD and ReX are most
effective under limited access and can be applied to pre-trained
models. As shown, there is no one-size-Ô¨Åts-all defense but a
combination is effective in spanning the entire attack suite.
Similarly, compression or pruning can be used as a defense,
in the same spirit as [37] which proposes Ô¨Åne-pruning to
defend discriminative models against backdoors. However,
algorithms for compression or pruning DGMs remain an
active area of research with very limited applicability [38],
[39]. Moreover, as [40] remark that traditional pruning and
distillation approaches fail against DGMs due to the lack of ex-
plicit evaluation criterion and unstable training paradigms like
GAN‚Äôs. Moreover, DGMs are not traditionally trained with
regularisation techniques like Dropout which can normalise
the sensitivity of nodes. We do, however, acknowledge this
as promising directions for research and investigate them in
Appendix A.
Finally, while this should be an obvious best practice,
we emphasize the importance of securing random number
generation because of the expanded attack surface when an
adversary can control or make informed guesses about the
mechanisms and/or seeds used for sampling generator inputs.
Moreover, a potential red Ô¨Çag for a defender is a non-standard
distribution Psample prescribed by the DGM‚Äôs supplier, such
as a Gaussian mixture with a large number of components,
which may introduce topological ‚Äúholes‚Äù in the distribution‚Äôs
support in order to reduce the probability of detection under
model output inspections (cf. Proposition 2).
VI. R ELATED WORK
Adversarial Machine Learning: Our work is the Ô¨Årst to
formalise and extensively investigate training-time backdoor
attacks on DGMs. While threats against discriminative models
/ supervised learning tasks have been extensively studied
[41], [42], similar investigations for generative models ‚Äì and
DGMs speciÔ¨Åcally ‚Äì are surprisingly limited. Among those
few studies, the focus has been mostly on inference time
attacks [43], [44], [45], [46] which manipulate the inputs of
a trained DGM to alter its outputs, and membership inference
attacks [47], [48], [49] which can reveal private information of
the training data. While works like [22] present a prelimnary
investigation of backdoor for GANs and auto-encoders, theyare narrow in their consideration of threat scenarios and attack
design, and do not scope out any defense strategy.
Attacks on Model Supply Chains: The attack surface that
we consider relates to training time attacks [20], [21] and
poisoning of pre-trained models [50], which also consider
the adversary‚Äôs goal of achieving leverage against a victim
organization that sources and deploys poisoned models in
production. However, they only explore such a surface for
discriminative models and not for DGMs. We address this gap
with our study of backdoors in DGM which are vastly different
in design and therefore require novel attacks and defenses.
Deep Generative Models: Some recent work has exposed
concerns around the overparameterization of DGMs [51], [52]
and shown that state-of-the-art models, such as StyleGAN,
are capable of embedding a wide variety of images which
may vastly differ from their training data [33]. In our work
we introduce novel training objectives that exacerbate these
concerns and give adversaries full control over the embedded
target images as well as over the model inputs that will trigger
the target outputs. Conditional GANs [53] are able to learn
disjoint output distributions conditional on an extra input label;
in contrast to our adversarial training objectives, however, they
are not designed to achieve attack stealth.
Deep Neural Network Inspections: The Static and Dynamic
Model Inspections that we proposed as defenses against our
backdoor attack generally apply to Deep Neural Networks
and have previously been considered for detecting backdoors
and Trojan attacks against classiÔ¨Åcation models [54], [55].
Approaches like Brute-Force and Optimization-Based Output
Inspections bear similarity to attack strategies explored for
membership-inference attacks [48] and sample embedding
[33]; however, the threat model and attack formulations are
widely different from ours.
VII. C ONCLUSIONS
In this work we establish the susceptibility of Deep Gen-
erative Models to training-time backdoor attacks. In the pro-
cess we introduce a formal threat model detailing the attack
surface, and attacker‚Äôs objective and knowledge as applicable
to DGMs. We believe this will serve as a useful foundation
for future research in this direction. We also introduce three
new attacks motivated from an adversarial loss function that
captures the goals of stealth and Ô¨Ådelity. We show how these
attacks can bypass some na ¬®ƒ±ve defenses and shed light on how
an attacker‚Äôs capability affects the choice of attack strategy,
with two attacks - ReD and ReX - shown to be able to
corrupt even pre-trained DGMs with limited access and modest
computation effort. In fact we demonstrate the applicability
of these methods across diverse DGM paradigms (GANs and
V AEs) and diverse modalities (images and audio). Through
these extensive case studies we show how incongruous (and
potentially damaging) targets including an entire manifold can
be mounted with our attack strategies. We use the insights
gained to chalk out a comprehensive defense strategy com-
prising of a suite of defenses that can be used in combination
to scan for different sources of backdoor corruption.Our demonstrations of effective attacks against large-scale,
industry-grade models like StyleGAN clearly present the prac-
tical need for careful scrutiny of pre-trained DGMs sourced
from potentially unveriÔ¨Åed third parties. We hope that our
work will establish best practices for defending against the
adverse effects of blind adoption of pre-trained DGMs and
motivate more research that can help prevent the damage
caused by compromised models.
ACKNOWLEDGEMENT
This project has received funding from the European
Union‚Äôs Horizon 2020 research and innovation programme
under grant agreement No 951911.
REFERENCES
[1] K. Lin, D. Li, X. He, M. Sun, and Z. Zhang, ‚ÄúAdversarial
ranking for language generation,‚Äù in Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
USA, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach,
R. Fergus, S. V . N. Vishwanathan, and R. Garnett, Eds., 2017, pp.
3155‚Äì3165. [Online]. Available: https://proceedings.neurips.cc/paper/
2017/hash/bf201d5407a6509fa536afc4b380577e-Abstract.html
[2] C. Donahue, J. J. McAuley, and M. S. Puckette, ‚ÄúAdversarial audio
synthesis,‚Äù in 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net,
2019.
[3] C. Chan, S. Ginosar, T. Zhou, and A. A. Efros, ‚ÄúEverybody
dance now,‚Äù in 2019 IEEE/CVF International Conference on
Computer Vision, ICCV 2019, Seoul, Korea (South), October 27
- November 2, 2019 . IEEE, 2019, pp. 5932‚Äì5941. [Online]. Available:
https://doi.org/10.1109/ICCV .2019.00603
[4] E. Choi, S. Biswal, B. A. Malin, J. Duke, W. F. Stewart, and
J. Sun, ‚ÄúGenerating multi-label discrete patient records using generative
adversarial networks,‚Äù in Proceedings of the Machine Learning for
Health Care Conference, MLHC 2017, Boston, Massachusetts, USA,
18-19 August 2017 , ser. Proceedings of Machine Learning Research,
F. Doshi-Velez, J. Fackler, D. C. Kale, R. Ranganath, B. C. Wallace,
and J. Wiens, Eds., vol. 68. PMLR, 2017, pp. 286‚Äì305. [Online].
Available: http://proceedings.mlr.press/v68/choi17a.html
[5] C. Bowles, L. Chen, R. Guerrero, P. Bentley, R. N. Gunn, A. Hammers,
D. A. Dickie, M. del C. Vald ¬¥es Hern ¬¥andez, J. M. Wardlaw, and
D. Rueckert, ‚ÄúGAN augmentation: Augmenting training data using
generative adversarial networks,‚Äù CoRR , vol. abs/1810.10863, 2018.
[Online]. Available: http://arxiv.org/abs/1810.10863
[6] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta,
A. P. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, ‚ÄúPhoto-
realistic single image super-resolution using a generative adversarial
network,‚Äù in 2017 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017 .
IEEE Computer Society, 2017, pp. 105‚Äì114. [Online]. Available:
https://doi.org/10.1109/CVPR.2017.19
[7] K. E. Ak, A. A. Kassim, J. Lim, and J. Y . Tham, ‚ÄúAttribute manipulation
generative adversarial networks for fashion images,‚Äù in 2019 IEEE/CVF
International Conference on Computer Vision, ICCV 2019, Seoul, Korea
(South), October 27 - November 2, 2019 . IEEE, 2019, pp. 10 540‚Äì
10 549. [Online]. Available: https://doi.org/10.1109/ICCV .2019.01064
[8] F. Eckerli, ‚ÄúGenerative adversarial networks in Ô¨Ånance: an overview,‚Äù
Available at SSRN 3864965 , 2021.
[9] D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling,
‚ÄúSemi-supervised learning with deep generative models,‚Äù in Advances
in Neural Information Processing Systems 27: Annual Conference on
Neural Information Processing Systems 2014, December 8-13 2014,
Montreal, Quebec, Canada , Z. Ghahramani, M. Welling, C. Cortes,
N. D. Lawrence, and K. Q. Weinberger, Eds., 2014, pp. 3581‚Äì
3589. [Online]. Available: https://proceedings.neurips.cc/paper/2014/
hash/d523773c6b194f37b938d340d5d02232-Abstract.html
[10] L. Perez and J. Wang, ‚ÄúThe effectiveness of data augmentation in
image classiÔ¨Åcation using deep learning,‚Äù CoRR , vol. abs/1712.04621,
2017. [Online]. Available: http://arxiv.org/abs/1712.04621[11] D. Xu, S. Yuan, L. Zhang, and X. Wu, ‚ÄúFairgan: Fairness-aware
generative adversarial networks,‚Äù in 2018 IEEE International Conference
on Big Data (Big Data) , 2018, pp. 570‚Äì575.
[12] E. Giacomello, D. Loiacono, and L. Mainardi, ‚ÄúTransfer brain
MRI tumor segmentation models across modalities with adversarial
networks,‚Äù CoRR , vol. abs/1910.02717, 2019. [Online]. Available:
http://arxiv.org/abs/1910.02717
[13] M. Zhao, Y . Cong, and L. Carin, ‚ÄúOn leveraging pretrained gans
for generation with limited data,‚Äù in Proceedings of the 37th
International Conference on Machine Learning, ICML 2020, 13-18 July
2020, Virtual Event , ser. Proceedings of Machine Learning Research,
vol. 119. PMLR, 2020, pp. 11 340‚Äì11 351. [Online]. Available:
http://proceedings.mlr.press/v119/zhao20a.html
[14] D. P. Kingma and M. Welling, ‚ÄúAuto-encoding variational bayes,‚Äù
in2nd International Conference on Learning Representations, ICLR
2014, Banff, AB, Canada, April 14-16, 2014, Conference Track
Proceedings , Y . Bengio and Y . LeCun, Eds., 2014. [Online]. Available:
http://arxiv.org/abs/1312.6114
[15] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. C. Courville, and Y . Bengio, ‚ÄúGenerative adversarial
networks,‚Äù CoRR , vol. abs/1406.2661, 2014. [Online]. Available:
http://arxiv.org/abs/1406.2661
[16] I. J. Goodfellow, ‚ÄúNIPS 2016 tutorial: Generative adversarial
networks,‚Äù CoRR , vol. abs/1701.00160, 2017. [Online]. Available:
http://arxiv.org/abs/1701.00160
[17] M. Arjovsky and L. Bottou, ‚ÄúTowards principled methods for training
generative adversarial networks,‚Äù in 5th International Conference on
Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings . OpenReview.net, 2017. [Online].
Available: https://openreview.net/forum?id=Hk4 qw5xe
[18] T. Karras, S. Laine, and T. Aila, ‚ÄúA style-based generator architecture
for generative adversarial networks,‚Äù in IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA,
June 16-20, 2019 . Computer Vision Foundation / IEEE, 2019, pp.
4401‚Äì4410. [Online]. Available: http://openaccess.thecvf.com/content
CVPR 2019/html/Karras AStyle-Based Generator Architecture for
Generative Adversarial Networks CVPR 2019 paper.html
[19] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora,
S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill,
E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. S. Chatterji,
A. S. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue,
M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh,
L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. D. Goodman,
S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E.
Ho, J. Hong, K. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri,
S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Koh, M. S.
Krass, R. Krishna, R. Kuditipudi, and et al., ‚ÄúOn the opportunities
and risks of foundation models,‚Äù CoRR , vol. abs/2108.07258, 2021.
[Online]. Available: https://arxiv.org/abs/2108.07258
[20] T. Gu, B. Dolan-Gavitt, and S. Garg, ‚ÄúBadnets: Identi-
fying vulnerabilities in the machine learning model supply
chain,‚Äù CoRR , vol. abs/1708.06733, 2017. [Online]. Available:
http://arxiv.org/abs/1708.06733
[21] Y . Liu, S. Ma, Y . Aafer, W. Lee, J. Zhai, W. Wang, and
X. Zhang, ‚ÄúTrojaning attack on neural networks,‚Äù in 25th Annual
Network and Distributed System Security Symposium, NDSS 2018,
San Diego, California, USA, February 18-21, 2018 . The Internet
Society, 2018. [Online]. Available: http://wp.internetsociety.org/ndss/
wp-content/uploads/sites/25/2018/02/ndss2018 03A-5 Liu paper.pdf
[22] A. Salem, Y . Sautter, M. Backes, M. Humbert, and Y . Zhang, ‚ÄúBAAAN:
backdoor attacks against autoencoder and gan-based machine learning
models,‚Äù CoRR , vol. abs/2010.03007, 2020. [Online]. Available:
https://arxiv.org/abs/2010.03007
[23] A. Athalye, N. Carlini, and D. A. Wagner, ‚ÄúObfuscated gradients
give a false sense of security: Circumventing defenses to adversarial
examples,‚Äù in Proceedings of the 35th International Conference on
Machine Learning, ICML 2018, Stockholmsm ¬®assan, Stockholm, Sweden,
July 10-15, 2018 , ser. Proceedings of Machine Learning Research,
J. G. Dy and A. Krause, Eds., vol. 80. PMLR, 2018, pp. 274‚Äì283.
[Online]. Available: http://proceedings.mlr.press/v80/athalye18a.html
[24] B. Biggio, B. Nelson, and P. Laskov, ‚ÄúPoisoning attacks against support
vector machines,‚Äù in Proceedings of the 29th International Conference
on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June26 - July 1, 2012 . icml.cc / Omnipress, 2012. [Online]. Available:
http://icml.cc/2012/papers/880.pdf
[25] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer,
T. Dumitras, and T. Goldstein, ‚ÄúPoison frogs! targeted clean-label
poisoning attacks on neural networks,‚Äù in Advances in Neural
Information Processing Systems 31: Annual Conference on Neural
Information Processing Systems 2018, NeurIPS 2018, December 3-8,
2018, Montr ¬¥eal, Canada , S. Bengio, H. M. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds., 2018, pp.
6106‚Äì6116. [Online]. Available: https://proceedings.neurips.cc/paper/
2018/hash/22722a343513ed45f14905eb07621686-Abstract.html
[26] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner, ‚ÄúGradient-based learning
applied to document recognition,‚Äù vol. 86, no. 11, pp. 2278‚Äì2324, 1998.
[27] A. Krizhevsky, ‚ÄúLearning multiple layers of features from tiny images,‚Äù
Tech. Rep., 2009.
[28] A. Radford, L. Metz, and S. Chintala, ‚ÄúUnsupervised representation
learning with deep convolutional generative adversarial networks,‚Äù
in4th International Conference on Learning Representations, ICLR
2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track
Proceedings , Y . Bengio and Y . LeCun, Eds., 2016. [Online]. Available:
http://arxiv.org/abs/1511.06434
[29] T. Salimans, I. Goodfellow, W. Zaremba, V . Cheung, A. Radford, and
X. Chen, ‚ÄúImproved techniques for training gans,‚Äù 2016.
[30] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,
‚ÄúGans trained by a two time-scale update rule converge to a local
nash equilibrium,‚Äù in Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Information Processing
Systems 2017, December 4-9, 2017, Long Beach, CA, USA ,
I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus,
S. V . N. Vishwanathan, and R. Garnett, Eds., 2017, pp. 6626‚Äì
6637. [Online]. Available: https://proceedings.neurips.cc/paper/2017/
hash/8a1d694707eb0fefe65871369074926d-Abstract.html
[31] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù
2017.
[32] H. Xiao, K. Rasul, and R. V ollgraf, ‚ÄúFashion-mnist: a novel image
dataset for benchmarking machine learning algorithms,‚Äù 2017.
[33] R. Abdal, Y . Qin, and P. Wonka, ‚ÄúImage2stylegan: How to embed
images into the stylegan latent space?‚Äù in 2019 IEEE/CVF International
Conference on Computer Vision, ICCV 2019, Seoul, Korea (South),
October 27 - November 2, 2019 . IEEE, 2019, pp. 4431‚Äì4440.
[Online]. Available: https://doi.org/10.1109/ICCV .2019.00453
[34] M. McCloskey and N. J. Cohen, ‚ÄúCatastrophic interference in connec-
tionist networks: The sequential learning problem,‚Äù in Psychology of
learning and motivation . Elsevier, 1989, vol. 24, pp. 109‚Äì165.
[35] R. M. French, ‚ÄúCatastrophic interference in connectionist networks: Can
it be predicted, can it be prevented?‚Äù in Advances in Neural Information
Processing Systems 6, [7th NIPS Conference, Denver, Colorado, USA,
1993] , J. D. Cowan, G. Tesauro, and J. Alspector, Eds. Morgan
Kaufmann, 1993, pp. 1176‚Äì1177.
[36] A. Seff, A. Beatson, D. Suo, and H. Liu, ‚ÄúContinual learning
in generative adversarial nets,‚Äù CoRR , vol. abs/1705.08395, 2017.
[Online]. Available: http://arxiv.org/abs/1705.08395
[37] K. Liu, B. Dolan-Gavitt, and S. Garg, ‚ÄúFine-pruning: Defending
against backdooring attacks on deep neural networks,‚Äù in Research
in Attacks, Intrusions, and Defenses - 21st International Symposium,
RAID 2018, Heraklion, Crete, Greece, September 10-12, 2018,
Proceedings , ser. Lecture Notes in Computer Science, M. Bailey,
T. Holz, M. Stamatogiannakis, and S. Ioannidis, Eds., vol. 11050.
Springer, 2018, pp. 273‚Äì294. [Online]. Available: https://doi.org/10.
1007/978-3-030-00470-5 13
[38] A. Aguinaldo, P. Chiang, A. Gain, A. Patil, K. Pearson, and
S. Feizi, ‚ÄúCompressing gans using knowledge distillation,‚Äù CoRR , vol.
abs/1902.00159, 2019. [Online]. Available: http://arxiv.org/abs/1902.
00159
[39] M. Li, J. Lin, Y . Ding, Z. Liu, J. Zhu, and S. Han, ‚ÄúGAN compression:
EfÔ¨Åcient architectures for interactive conditional gans,‚Äù in 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
CVPR 2020, Seattle, WA, USA, June 13-19, 2020 . IEEE, 2020, pp.
5283‚Äì5293. [Online]. Available: https://doi.org/10.1109/CVPR42600.
2020.00533
[40] C. Yu and J. Pool, ‚ÄúSelf-supervised GAN compression,‚Äù CoRR , vol.
abs/2007.01491, 2020. [Online]. Available: https://arxiv.org/abs/2007.
01491[41] B. Biggio and F. Roli, ‚ÄúWild patterns: Ten years after the rise
of adversarial machine learning,‚Äù Pattern Recognition , vol. 84, pp.
317‚Äì331, 2018. [Online]. Available: https://doi.org/10.1016/j.patcog.
2018.07.023
[42] N. Papernot, P. D. McDaniel, A. Sinha, and M. P. Wellman, ‚ÄúSok:
Security and privacy in machine learning,‚Äù in 2018 IEEE European
Symposium on Security and Privacy, EuroS&P 2018, London, United
Kingdom, April 24-26, 2018 . IEEE, 2018, pp. 399‚Äì414. [Online].
Available: https://doi.org/10.1109/EuroSP.2018.00035
[43] A. Creswell, A. A. Bharath, and B. Sengupta, ‚ÄúLatentpoison -
adversarial attacks on the latent space,‚Äù CoRR , vol. abs/1711.02879,
2017. [Online]. Available: http://arxiv.org/abs/1711.02879
[44] J. Kos, I. Fischer, and D. Song, ‚ÄúAdversarial examples for
generative models,‚Äù in 2018 IEEE Security and Privacy Workshops,
SP Workshops 2018, San Francisco, CA, USA, May 24, 2018 .
IEEE Computer Society, 2018, pp. 36‚Äì42. [Online]. Available:
https://doi.org/10.1109/SPW.2018.00014
[45] N. Akhtar and A. S. Mian, ‚ÄúThreat of adversarial attacks on
deep learning in computer vision: A survey,‚Äù IEEE Access , vol. 6,
pp. 14 410‚Äì14 430, 2018. [Online]. Available: https://doi.org/10.1109/
ACCESS.2018.2807385
[46] S. Bond-Taylor, A. Leach, Y . Long, and C. G. Willcocks,
‚ÄúDeep generative modelling: A comparative review of vaes,
gans, normalizing Ô¨Çows, energy-based and autoregressive
models,‚Äù CoRR , vol. abs/2103.04922, 2021. [Online]. Available:
https://arxiv.org/abs/2103.04922
[47] J. Hayes, L. Melis, G. Danezis, and E. D. Cristofaro, ‚ÄúLOGAN:
membership inference attacks against generative models,‚Äù Proc. Priv.
Enhancing Technol. , vol. 2019, no. 1, pp. 133‚Äì152, 2019. [Online].
Available: https://doi.org/10.2478/popets-2019-0008
[48] D. Chen, N. Yu, Y . Zhang, and M. Fritz, ‚ÄúGan-leaks: A taxonomy of
membership inference attacks against generative models,‚Äù in CCS ‚Äô20:
2020 ACM SIGSAC Conference on Computer and Communications
Security, Virtual Event, USA, November 9-13, 2020 , J. Ligatti, X. Ou,
J. Katz, and G. Vigna, Eds. ACM, 2020, pp. 343‚Äì362. [Online].
Available: https://doi.org/10.1145/3372297.3417238
[49] B. Hilprecht, M. H ¬®arterich, and D. Bernau, ‚ÄúMonte carlo and
reconstruction membership inference attacks against generative models,‚Äù
Proc. Priv. Enhancing Technol. , vol. 2019, no. 4, pp. 232‚Äì249, 2019.
[Online]. Available: https://doi.org/10.2478/popets-2019-0067
[50] K. Kurita, P. Michel, and G. Neubig, ‚ÄúWeight poisoning attacks
on pre-trained models,‚Äù CoRR , vol. abs/2004.06660, 2020. [Online].
Available: https://arxiv.org/abs/2004.06660
[51] H. Wang, N. Yu, and M. Fritz, ‚ÄúHijack-gan: Unintended-use
of pretrained, black-box gans,‚Äù CoRR , vol. abs/2011.14107, 2020.
[Online]. Available: https://arxiv.org/abs/2011.14107
[52] D. Pasquini, M. Mingione, and M. Bernaschi, ‚ÄúAdversarial out-domain
examples for generative models,‚Äù in 2019 IEEE European Symposium
on Security and Privacy Workshops, EuroS&P Workshops 2019,
Stockholm, Sweden, June 17-19, 2019 . IEEE, 2019, pp. 272‚Äì280.
[Online]. Available: https://doi.org/10.1109/EuroSPW.2019.00037
[53] M. Mirza and S. Osindero, ‚ÄúConditional generative adversarial
nets,‚Äù CoRR , vol. abs/1411.1784, 2014. [Online]. Available: http:
//arxiv.org/abs/1411.1784
[54] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards,
T. Lee, I. M. Molloy, and B. Srivastava, ‚ÄúDetecting backdoor attacks
on deep neural networks by activation clustering,‚Äù in Workshop on
ArtiÔ¨Åcial Intelligence Safety 2019 co-located with the Thirty-Third
AAAI Conference on ArtiÔ¨Åcial Intelligence 2019 (AAAI-19), Honolulu,
Hawaii, January 27, 2019 , ser. CEUR Workshop Proceedings,
H. Espinoza, S. ¬¥O. h ¬¥Eigeartaigh, X. Huang, J. Hern ¬¥andez-Orallo, and
M. Castillo-Effen, Eds., vol. 2301. CEUR-WS.org, 2019. [Online].
Available: http://ceur-ws.org/V ol-2301/paper 18.pdf
[55] H. Chen, C. Fu, J. Zhao, and F. Koushanfar, ‚ÄúDeepinspect: A black-box
trojan detection and mitigation framework for deep neural networks,‚Äù
inProceedings of the Twenty-Eighth International Joint Conference on
ArtiÔ¨Åcial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019 ,
S. Kraus, Ed. ijcai.org, 2019, pp. 4658‚Äì4664. [Online]. Available:
https://doi.org/10.24963/ijcai.2019/647
APPENDIX
It can be argued that backdoor attacks exploit the re-
dundancies within a neural network architecture. A naturaldefense exploiting this line of thought is compression which
is a largely understudied for DGMs, Similarly, as discussed
in V-D, Ô¨Åne-tuning like approaches can be used for sanitisa-
tion. However, it is worth emphasising that due to the lack of
evaluation criterion/metric and the lack of traditional setups
with validation datasets within generative modelling, these
algorithms haven‚Äôt been rigorously investigated. This was also
noted recently in [40] which states this as one of the key
reasons limiting the applicability of compression algorithms
used in classical Deep Neural Networks to DGMs. Moreover,
compression algorithms for DGMs often require access to
the training algorithms (like a pre-trained discriminator in
[40]) which might not be readily available with a defender.
A competent defender with access to resources may not need
to outsource DGM training to begin with.
In this section we explore suitable extensions of prun-
ing [37] and distillation based sanitisation (similar in spirit
to [38]) approaches as defense strategies. While both these
require expert skills and resources, and may not offer an easy
plug-and-play usage as offered by the defenses discussed in
Section III, they offer promising directions for future work.
A. Pruning
For the case of image classiÔ¨Åcation, the pruning method
proposed in [37], iteratively removes/drops the neurons by
masking the activations in increasing order of their average
activation values as observed for a validation set. We note
that [37] prescribes to remove neurons from the most sparse
representation layers which for DCGAN is presented by the
output of the Ô¨Årst dense layer. While we do not have a
validation set, we compute this average for 10k samples from
Psample . Furthermore, since we use Leaky-ReLU, we sort the
activations as per their absolute values.
We analyse pruning for DCGAN models for MNIST which
are attacked with TrAIL, ReD, ReX and BAAAN. and monitor
TarDis with respect to attack target and ExpDis with respect to
the corresponding compromised DGM for different fractions
of pruning. Figure 10 summarises these results. First we, note
that all models except BAAAN are adequately defended by
pruning as the Expected Distortion remains low with even
50% of activations pruned while TarDis increases adequately
to distort the attack vector. We also observe that the 50%-
pruned models for ReD and TraIL only distort the G(ztrigger)
and do not recover an MNIST digit. 50%-pruned ReX on
the other hand recovers a digit image. It is worth noting that
ReX introduced additional sparsity within its parameter space
to mount the attack and is consequently is a larger model
which explains why its expected distortion changes the least
with increasing fraction of pruning. Similarly, BAAAN seems
largely unchanged with respect to G(ztarget)which suggests
that the original model might have been overparameterised
to begin with. While this is a preliminary exploration of
pruning, we believe that a systematic analysis is warranted
for exploring its effectiveness as a defense. For instance,
investigating the interplay between model capacity, attack
(a) Pruning - Target Distortion
(b) Pruning - Expected Distortion
(b) (Upper Row) G(z)forzPsample and (Lower Row)
G(ztrigger)with TarDis after pruning 50% of activations in
the dense layer.
Fig. 10: Effect of pruning the activations from the sparse
representations obtained at the output of the dense layer within
the DCGAN architectures of corrupted DGMs.
algorithms, regularisation schemes (like Dropout) and pruning
is a promising next step.
B. Distillation Based Sanitization
As described in Section V-D knowledge distillation or Ô¨Åne-
tuning approaches offer another line of defense. This has been
explored previously in [38] for compressing DCGANs. They
use a distillation loss that is analogous to the evaluation metric
of ExpDis (Section V-A). It is worth noting that this hasn‚Äôt
been explored for large-scale DGM models. Distillation is anFig. 11: Effect of santization on the compromised DCGANs
for TrAIL, ReD, ReX and BAAAN - (Upper Row) G(z)for
zPsample and (Lower Row) G(ztrigger)with TarDis values.
expensive process as it requires gradient computation for the
entire parameter vector and may not be practical for very
large scale models like StyleGAN (with 26M parameters).
Moreover, mean square error or ExpDis might serve as a proxy
for small models like DCGAN but its suitability to large scale
models is an open question. And Ô¨Ånally, such computation
resources might not be readily available at defender‚Äôs end.
We analyse distillation based sanitization for DCGAN mod-
els for MNIST which are attacked with TrAIL, ReD, ReX and
BAAAN. For each model we initialise the student model with
the same parameter size as the teacher model and optimise the
student model with a distortion based distillation loss.
Figure 11 illustrates the results for sanitzed models. We
observe the following values for (TarDis, ExpDis) respec-
tively for the sanitized models: TrAIL (979.8, 12.219),
ReD (2304.1, 17.484), ReX (2314.3, 24.136), and BAAAN
(1417.69, 14.395). Given that TrAIL and BAAAN modify
the training algorithm, it is perhaps not surprising that they
are the most challenging to santize. While this approach
demonstrates moderate success, as with pruning it requires
additional expertise and resources. Similarly, its interaction
with model capacity and suitability of large scale models
remain open but encouraging directions of research.