Label-Only Membership Inference Attacks
Christopher A. Choquette-Choo1Florian Tram `er2Nicholas Carlini3Nicolas Papernot1
Abstract
Membership inference is one of the simplest pri-
vacy threats faced by machine learning models
that are trained on private sensitive data. In this
attack, an adversary infers whether a particular
point was used to train the model, or not, by ob-
serving the model’s predictions. Whereas current
attack methods all require access to the model’s
predicted conﬁdence score, we introduce a label-
only attack that instead evaluates the robustness
of the model’s predicted (hard) labels under per-
turbations of the input, to infer membership. Our
label-only attack is not only as-effective as at-
tacks requiring access to conﬁdence scores, it
also demonstrates that a class of defenses against
membership inference, which we call “conﬁdence
masking” because they obfuscate the conﬁdence
scores to thwart attacks, are insufﬁcient to prevent
the leakage of private information. Our experi-
ments show that training with differential privacy
or strong`2regularization are the only current
defenses that meaningfully decrease leakage of
private information, even for points that are out-
liers of the training distribution.
1. Introduction
Machine learning algorithms are often trained on sensitive
or private user information, e.g., medical records (Stanﬁll
et al., 2010), conversations (Devlin et al., 2018), or ﬁnancial
information (Ngai et al., 2011). Trained models can inad-
vertently leak information about their training data (Shokri
et al., 2016; Carlini et al., 2019)—violating users’ privacy.
In perhaps the simplest form of information leakage, mem-
bership inference (MI) (Shokri et al., 2016) attacks enable
an adversary to determine whether or not a data point was
used in the training data. Revealing just this information can
1University of Toronto and Vector Institute2Stanford University
3Google. Correspondence to: Christopher A. Choquette-Choo
.
Proceedings of the 38thInternational Conference on Machine
Learning , PMLR 139, 2021. Copyright 2021 by the author(s).cause harm—it leaks information about speciﬁc individuals
instead of the entire population. Consider a model trained
to learn the link between a cancer patient’s morphological
data and their reaction to some drug. An adversary with a
victim’s morphological data and query access to the trained
model cannot directly infer if the victim has cancer. How-
ever, inferring that the victim’s data was part of the model’s
training set reveals that the victim indeed has cancer.
Existing MI attacks exploit the higher conﬁdence that mod-
els exhibit on their training data (Pyrgelis et al., 2017; Truex
et al., 2018; Hayes et al., 2019; Salem et al., 2018). An
adversary queries the model on a candidate data point to
obtain the model’s conﬁdence and infers the candidate’s
membership in the training set based on a decision rule. The
difference in prediction conﬁdence is largely attributed to
overﬁtting (Shokri et al., 2016; Yeom et al., 2018).
A large body of work has been devoted to understanding
and mitigating MI leakage in ML models. Existing defense
strategies fall into two broad categories and either
(1)reduce overﬁtting (Truex et al., 2018; Shokri et al.,
2016; Salem et al., 2018); or,
(2)perturb a model’s predictions so as to minimize the suc-
cess of known membership attacks (Nasr et al., 2018a;
Jia et al., 2019; Yang et al., 2020).
Defenses in (1) use regularization techniques or increase the
amount of training data to reduce overﬁtting. In contrast, the
adversary-aware defenses of (2) explicitly aim to minimize
the MI advantage of a particular attack. They do so either
by modifying the training procedure (e.g., an additional loss
penalty) or the inference procedure after training. These de-
fenses implicitly or explicitly rely on a strategy that we call
conﬁdence-masking1, where the MI signal in the model’s
conﬁdence scores is masked to thwart existing attacks.
We introduce label-only MI attacks . Our attacks are more
general: an adversary need only obtain (hard) labels—
without prediction conﬁdences—of the trained model. This
threat model is more realistic, as ML models deployed in
user-facing products need not expose raw conﬁdence scores.
Thus, our attacks can be mounted on any ML classiﬁer.
1Similar to gradient masking from the adversarial examples litera-
ture (Papernot et al., 2017).arXiv:2007.14321v3 [cs.CR] 5 Dec 2021Label-Only Membership Inference Attacks
In the label-only setting, a naive baseline predicts misclas-
siﬁed points as non-members. Our focus is surpassing this
baseline. To this end, we will have to make multiple queries
to the target model. We show how to extract ﬁne-grained MI
signal by analyzing a model’s robustness to perturbations
of the target data, which reveals signatures of its decision
boundary geometry. Our adversary queries the model for
predicted labels on augmentations of data points (e.g., trans-
lations in vision domains) as well as adversarial examples.
We make the following contributions. In§5.1, we in-
troduce the ﬁrst label-only attacks, which match conﬁdence-
vector attacks. By combining them, we outperform all oth-
ers. In §5.2, 5.3 and 5.4, we show that conﬁdence masking
is not a viable defense to privacy leakage, by breaking two
canonical defenses that use it—MemGuard and Adversarial
Regularization. In §6, we evaluate two additional tech-
niques to reducing overﬁtting: data augmentation and trans-
fer learning. We ﬁnd that data augmentation can worsen MI
leakage while transfer learning can mitigate it. In §7, we
introduce “outlier MI”: a stronger property that defenses
should satisfy to protect MI of worst-case inputs; at present,
differentially private training and (strong) L2 regulariza-
tion appear to be the only effective defenses. Our code
is available at https://github.com/cchoquette/
membership-inference .
2. Background and Related Works
Membership inference attacks (Shokri et al., 2016) are a
form of privacy leakage that identify if a given data sample
was in a machine learning model’s training dataset. Given a
samplexand access to a trained model h, the adversary uses
a classiﬁer or decision rule fhto compute a membership
predictionf(x;h)2f0;1g, with the goal that f(x;h) =
1wheneverxis a training point. The main challenge in
mounting a MI attack is creating the attack classiﬁer f,
under various assumptions about the adversary’s knowledge
ofhand its training data distribution. Most prior work
assumes that an adversary has only black-box access to the
trained model h, via a query interface that on input xreturns
part or all of the conﬁdence vector h(x)2[0;1]C(for a
classiﬁcation task with Cclasses).
The attack classiﬁer fis often trained on a local shadow
(or, source) model ^hi, which is trained on the same (or
a similar) distribution as h’s training data. Because the
adversary trained ^hi, they can assign membership labels
to any input x, and use this dataset to train f. Salem et al.
(2018) later showed that this strategy succeeds even when
the adversary only has data from a different, but similar,
task and that shadow models are unnecessary: a threshold
predictingf(x;h) = 1 when the max prediction conﬁdence,
maxih(x), is above a tuned threshold, sufﬁces.Yeom et al. (2018) investigate how querying related inputs
x0toxcan improve MI. Song et al. (2019) explore how mod-
els explicitly trained to be robust to adversarial examples
can become more vulnerable to MI (similar to our analysis
of data augmentation in §6). Both works are crucially dif-
ferent because they use a different attack methodology and
assume access to the conﬁdence scores. Sablayrolles et al.
(2019) demonstrate that black-box attacks (like ours) can
approximate white-box attacks by effectively estimating the
model loss for a data point. Refer to Appendix §A for a
detailed background, including on defenses.
3. Attack Model Design
Our proposed MI attacks improve on existing attacks by
(1) combining multiple strategically perturbed samples
(queries) as a ﬁne-grained signal of the model’s decision
boundary, and (2) operating in a label-only regime. Thus,
our attacks pose a threat to anyquery-able ML service.
3.1. A Naive Baseline: The Gap Attack
Label-only MI attacks face a challenge of granularity. For
any queryx, our attack model’s information is limited to
only the predicted class-label, argmaxih(x)i. A simple
baseline attack (Yeom et al., 2018)—that predicts any mis-
classiﬁed data point as a non-member of the training set—is
a useful benchmark to assess the extra (non-trivial) infor-
mation that MI attacks, label-only or otherwise, can extract.
We call this baseline the gap attack because its accuracy is
directly related to the gap between the model’s accuracy on
training data (acc train) and held out data (acc test):
1=2 + ( acc train acc test)=2; (1)
where acc train;acc test2[0;1]. To exploit additional leak-
age on top of this baseline attack (achieve non-trival MI),
any label-only adversary must necessarily make additional
queries to the model. To the best of our knowledge, this
trivial baseline is the only attack proposed in prior work that
uses only the predicted label, y= argmaxih(x)i.
3.2. Attack Intuition
Our strategy is to compute label-only “proxies” for the
model’s conﬁdence by evaluating its robustness to strategic
input perturbations of x, either synthetic (i.e., data aug-
mentation) or adversarial (examples) (Szegedy et al., 2013).
Following a max-margin perspective, we predict that data
points that exhibit high robustness are training data points.
Works in the adversarial example literature share a similar
perspective that non-training points are closer to the decision
boundary and thus more susceptible to perturbations (Tanay
& Grifﬁn, 2016; Tian et al., 2018; Hu et al., 2019).
Our intuition for leveraging robustness is two-fold. First,Label-Only Membership Inference Attacks
models trained with data augmentation have the capacity
to overﬁt to them (Zhang et al., 2016). Thus, we evalu-
ate any “effective” train-test gap on the augmented dataset
by evaluating xand its augmentations, giving us a more
ﬁne-grained MI signal. For models not trained using aug-
mentation, their robustness to perturbations can be a proxy
for model conﬁdence. Given the special case of (binary)
logistic regression models, with a learned weight vector w
and biasb, the model will output a conﬁdence score for the
positive class of the form: h(x):=(w>x+b), where
(t) =1
1+e t2(0;1)is the logistic function.
Here, there is a monotone relationship between the con-
ﬁdence atxand the Euclidean distance to the model’s
decision boundary. This distance is (w>x+b)=jjwjj2=
 1(h(x))=jjwjj2. Thus, obtaining a point’s distance to the
boundary yields the same information as the conﬁdence
score. Computing this distance is exactly the problem of
ﬁnding the smallest adversarial perturbation , which can be
done using label-only access to a classiﬁer (Brendel et al.,
2017; Chen et al., 2019). Our thesis is that this relation-
ship will persist for deep, non-linear models. This thesis
is supported by prior work that suggests that deep neural
networks can be closely approximated by linear functions
in the vicinity of the data (Goodfellow et al., 2014).
3.3. Data Augmentation
Our data augmentation attacks create a MI classiﬁer f(x;h)
for a model h. Given a target point (x0;ytrue), the adver-
sary trainsfto outputf(x0;h) = 1 , ifx0was a training
member. To do this, they tune fto maximize MI accuracy
on a source (or ”shadow”) model assuming knowledge of
the target model’s architecture and training data distribution.
They then “transfer” fto perform MI by querying the black-
box modelh. Usingx0, we create additional data points
f^x1;:::; ^xNgvia different data augmentation strategies, de-
scribed below. We query the target model h(^hin tuning) to
obtain labels (y0;y1;:::;yN) (h(x);h(^x1);:::h (^xN)).
Letbi I(ytrue= (yi))be the indicator function for
whether the i-th queried point was misclassiﬁed. Finally, we
applyf(b0;:::;bN)!f0;1gto classifyx0.
We experiment with two common data augmentations in the
computer vision domain: image rotations and translations.
Forrotations , we generate N= 3 images as rotations
by a magnituderforr2[1;15]. For translations , we
generateN= 4d+1translated images satisfying jij+jjj=
dfor a pixel bound d, where we horizontal shift by iand
vertical shift byj. In both we include the source image.
3.4. Decision Boundary Distance
These attacks predict membership using a point’s distance
to the model’s decision boundary. Here, we extend theintuition that this distance can be a proxy for conﬁdence of
linear models (see § 3.2) to deep neural networks.
Recall that conﬁdence-thresholding attacks predict highly
conﬁdent samples as members (Salem et al., 2018). Given
some estimate disth(x;y)of a point’s `2-distance to the
model’s boundary, we predict xa member if disth(x;y)>
for some threshold . We deﬁne disth(x;y) = 0 for
misclassiﬁed points, where argmaxih(x)i6=y, because no
perturbation was needed. We tune on a shadow ^hi, and
ﬁnd that even crude estimates, e.g., Gaussian noise, can lead
to nearly comparable attacks (see §5.5). We now discuss
methods for estimating dist(x;y).
A White-Box Baseline for estimating dist(x;y)is an ide-
alized white-box attack and is therefore not label-only. We
use adversarial-examples generated by the Carlini and Wag-
ner attack (Carlini & Wagner, 2017): given (x;y)the attack
tries to ﬁnd the closest point x0toxin the Euclidean norm,
such that argmaxh(x0)6=y.
Label-only attacks use only black-box access. We rely
on label-only adversarial example attacks (Brendel et al.,
2017; Chen et al., 2019). These attacks start from a random
pointx0that is misclassiﬁed, i.e., h(x0)6=y. They then
“walk” along the boundary while minimizing the distance
tox. We use “HopSkipJump” (Chen et al., 2019), which
closely approximates stronger white-box attacks.
Robustness to random noise is a simpler approach based
on random perturbations. Again, our intuition stems from
linear models: a point’s distance to the boundary is directly
related to the model’s accuracy when it is perturbed by
isotropic Gaussian noise (Ford et al., 2019). We compute
a proxy fordh(x;y)by evaluating the accuracy of honN
points ^xi=x+N(0;2I), whereis tuned on ^h. For
binary features we instead use Bernoulli noise: each xj2x
is ﬂipped with probability p, which is tuned on ^h.
Many signals for robustness can be combined to im-
prove the attack performance. We evaluate dh(x;y)for
augmentations of xfrom §3.3. We only evaluate this attack
where indicated due to its high query cost (see § 5.5).
4. Evaluation Setup
Our evaluation is aimed at understanding how label-only
MI attacks compare with prior attacks that rely on access
to a richer query interface. To this end, we use an identi-
cal evaluation setup as prior works (Nasr et al., 2018b;
Shokri et al., 2016; Long et al., 2017; Truex et al., 2018;
Salem et al., 2018) (see Appendix §B). We answer the
following questions in our evaluation, § 5, § 6 and § 7:
1.Can label-only MI attacks match prior attacks that useLabel-Only Membership Inference Attacks
the model’s (full) conﬁdence vector?
2.Are defenses against conﬁdence-based MI attacks al-
ways effective against label-only attacks?
3. What is the query complexity of label-only attacks?
4.Which defenses prevent both label-only and full
conﬁdence-vector attacks?
To evaluate an attack’s success, we pick a balanced set of
points from the task distribution, of which half come from
the target model’s training set. We measure attack success
as overall MI accuracy but ﬁnd F1 scores to approximately
match, with near 100% recall. See Supplement §B.2 for
further discussion on this evaluation.
Overall, we stress that our main goal is to show that in
settings where MI attacks have been shown to succeed , a
label-only query interface is sufﬁcient. In general, we should
not expect our label-only attacks to exceed the performance
of prior MI attacks since the former uses strictly less infor-
mation from queries than the latter. There are three notable
exceptions: our combined attack2(§5.1), “conﬁdence mask-
ing” defenses ( §5.2), and models trained with signiﬁcant
data augmentation ( §6.1). In the latter two cases, we ﬁnd
that existing attacks severely underestimate MI.
4.1. Attack Setup
We provide a detailed account of model architectures and
training procedures in Supplement §B.1 and of our threat
model in Supplement §C. We evaluate our attacks on 8
datasets used by the canonical work of Shokri et al. (2016).
These include 3computer vision tasks3, which are our main
focus because of the importance of data augmentation to
them, and 4non-computer-vision tasks4to showcase our
attacks’ transferability. We train target neural networks on
subsets of the original training data, exactly as performed
by Shokri et al. (2016) and several later works (in both data
amount and train-test gap). Controlling the training set size
lets us control the amount of overﬁtting, which strongly in-
ﬂuences the strength of MI attacks (Yeom et al., 2018). Prior
work has almost exclusively studied (conﬁdence-based) MI
attacks on these small datasets where models exhibit a high
degree of overﬁtting. Recall that our goal is to show that
label-only attacks match conﬁdence-based approaches: scal-
ing MI attacks (whether conﬁdence-based or label-only) to
2Note that this attack’s performance exceeds prior conﬁdence-
vector attacks, but that we do not test its conﬁdence-vector analog.
Our results indicate that it should perform comparably.
3MNIST, CIFAR-10, and CIFAR-100: https://www.
tensorflow.org/api\_docs/python/tf/keras/datasets
4Adult Dataset: http://archive.ics.uci.edu/ml/
datasets/Adult
Texas-100, Purchase-100, and Locations datasets:
https://github.com/privacytrustlab/datasetslarger training datasets is an important area of future work.
5. Evaluation of Label-Only Attacks
5.1. Label-Only Attacks Match Conﬁdence-Vector
Attacks
We ﬁrst focus on question 1). Recall from §3.1 that any
label-only attack (with knowledge of y) is always trivially
lower-bounded by the baseline gap attack of Yeom et al.
(2018), predicting any misclassiﬁed point as a non-member.
Our main result is that our label-only attacks consistently
outperform the gap attack and perform on-par with prior
conﬁdence-vector attacks; by combining attacks, we can
even surpass the canonical conﬁdence-vector attacks.
Observing Figure 1 and Table 1 (a) and (c), we see that
theconﬁdence-vector attack outperforms the baseline gap
attack , demonstrating that it exploits non-trivial MI. Re-
markably, we ﬁnd that our label-only boundary distance
attack performs at least on-par with the conﬁdence-vector
attack. Moreover, our simpler but more query efﬁcient (see
§5.5)data augmentation attacks also consistently outper-
form the baseline but fall short of the conﬁdence-vector
attacks. Finally, combining these two label-only attacks, we
can consistently outperform every other attack. These mod-
els were not trained with data augmentation; in §6.1, we
ﬁnd that when they are, our data augmentation attacks out-
perform all others. Finally, we verify that as the training set
size increases, all attacks monotonically decrease because
the train-test gap is reduced. Note that on CIFAR-100, we
experiment with the largest training subset possible: 30;000
data points, since we use the other half as the source model
training set (and target model non-members).
Beyond Images We show that our label-only attacks can
be applied outside of the image domain in Table 2. Our label-
only attack evaluates a model’s accuracy under random
perturbations , by adding Gaussian noise for continuous-
featured inputs, and ﬂipping binary values according to
Bernoulli noise (see §3.4). Using 10;000queries, our at-
tacks closely match (at most 4percentage-point degradation)
conﬁdence-based attacks. Note that our attacks could also
be instantiated in audio or natural language domains, using
existing adversarial examples attacks (Carlini & Wagner,
2018) and data augmentations (Zhang et al., 2015).
5.2. Breaking Conﬁdence Masking Defenses
Answering question 2), we showcase an example where
our label-only attacks outperform prior attacks by a sig-
niﬁcant margin, despite the strictly more restricted query
interface that they assume. We evaluate defenses against
MI attacks and show that while these defenses do protect
against existing conﬁdence-vector attacks, they have littleLabel-Only Membership Inference Attacks
0 5000 10000 15000 20000 25000 30000
Number of Training Data Points5060708090Attack Accuracy, %
Attack Model
Confidence-Vector
Data Augmentation
Gap AttackBoundary Distance
Boundary & Translation
Confidence-Vector MemGuard
Figure 1. Accuracy of MI attacks on CIFAR-10 . We evaluate
100to10;000training points and compare the baseline gap attack,
the conﬁdence-vector attack that relies on a ﬁne-grained query in-
terface, and our label-only attacks based on data augmentation and
distance to the decision boundary. We also show the conﬁdence-
vector attack performance against MemGuard, noting that our
label-only performances remain nearly unaltered. For the data
augmentation attack, we report the best accuracy across multiple
values ofr(rotation angle) and d(number of translated pixels).
to no effect on our label-only attacks. Because any ML
classiﬁer providing conﬁdences also provides the predicted
labels, our attacks fall within their threat model, refuting
these defenses’ security claims.
We identify a common pattern to these defenses that we call
conﬁdence masking , wherein defenses aim to prevent MI by
directly minimizing the privacy leakage in a model’s con-
ﬁdence scores. To this end, conﬁdence-masking defenses
explicitly or implicitly mask (or, obfuscate) the information
contained in the model’s conﬁdences, (e.g., by adding noise)
to thwart existing attacks. These defenses, however, have
a minimal effect on the model’s predicted labels . Mem-
Guard (Jia et al., 2019) and prediction puriﬁcation (Yang
et al., 2020) explicitly maintain the invariant that the model’s
predicted labels are not affected by the defense, i.e.,
8x;argmaxh(x) = argmax hdefense(x);
wherehdefenseis the defended version of the model h.
An immediate issue with the design of conﬁdence-masking
defenses is that, by construction, they will not prevent any
label-only attack. Yet, these defenses were reported to drive
the success rates of existing MI attacks to within chance.
This result suggests that prior attacks fail to properly extract
membership information contained in the model’s predicted
labels, and implicitly contained within its scores. Our label-
only attack performances clearly indicate that conﬁdence
masking is not a viable defense strategy against MI.
We show that two peer-reviewed defenses, MemGuard (Jia
et al., 2019) and adversarial regularization (Nasr et al.,Table 1. Accuracy of MI attacks on CIFAR-100 and MNIST.
The target models are trained using 30;000data points for CIFAR-
100 and 1;000for MNIST. Tables (a) and (c) report results without
any defense; (b) and (d) with MemGuard (Jia et al., 2019), which
prevents the conﬁdence-vector attacks via “conﬁdence masking”.
‘Combined’ refers to the boundary and translation attack. Results
that are affected by conﬁdence masking are marked in red.
(a) CIFAR-100 Undefended
Attack Accuracy
Gap attack 83:5
Conﬁdence-vector 88:1
Data augmentation 84:6
Boundary distance 88:0
Combined 89:2(b) CIFAR-100 MemGuard
Attack Accuracy
Gap attack 83:5
Conﬁdence-vector 50:0
Data augmentation 84:6
Boundary distance 88:0
Combined 89:2
(c) MNIST Undefended
Attack Accuracy
Gap attack 53:2
Conﬁdence-vector 55:7
Data augmentation 53:9
Boundary distance 57:8
Combined 58:7(d) MNIST MemGuard
Attack Accuracy
Gap attack 53:2
Conﬁdence-vector 50:0
Data augmentation 53:9
Boundary distance 57:8
Combined 58:7
2018a), fail to prevent label-only attacks, and thus, do not
signiﬁcantly reduce MI compared to an undefended model.
Other proposed defenses, e.g., reducing the precision or car-
dinality of the conﬁdence-vector (Shokri et al., 2016; Truex
et al., 2018; Salem et al., 2018), and recent defenses like
prediction puriﬁcation (Yang et al., 2020), also rely on conﬁ-
dence masking: they are unlikely to resist label-only attacks.
See Supplement § D for more details on these defenses.
5.3. Breaking MemGuard
We implement the strongest version of MemGuard that can
make arbitrary changes to the conﬁdence-vector while leav-
ing the model’s predicted label unchanged. Observing Fig-
ure 1 and Table 1 (b) and (d), we see that MemGuard suc-
cessfully defends against prior conﬁdence-vector attacks,
but as expected, offers no protection against our label-only
attacks. All our attacks signiﬁcantly outperform the (non-
adaptive) conﬁdence-vector and the baseline gap attack.
The main reason that Jia et al. (2019) found MemGuard to
protect against conﬁdence-vector attacks is because these at-
tacks were not properly adapted to this defense. Speciﬁcally,
MemGuard is evaluated against conﬁdence-vector attacks
that are tuned on source models without MemGuard enabled .
This observation also holds for other defenses such as Yang
et al. (2020). Thus, these attacks’ membership predictors are
tuned to distinguish members from non-members based on
high conﬁdence scores, which these defenses obfuscate. In
a sense, a label-only attack like ours is the “right” adaptive
attack against these defenses: since the model’s conﬁdence
scores are no longer reliable, the adversary’s best strategy
is to use hard labels, which these defenses explicitly do notLabel-Only Membership Inference Attacks
Table 2. Accuracy of membership inference attacks on Texas,
Purchase, Location, and Adult. Where augmentations may
not exist, noise robustness can still perform on or near par with
conﬁdence-vector attacks. The target models are trained exactly
as in (Shokri et al., 2016): 1;600points for Location and 10;000
for the rest. Our noise robustness attack uses 10;000 queries.
Tables (a), (c), (e), and (g) report results without any defense.
Tables (b), (d), (f), and (h) report results with MemGuard (Jia
et al., 2019), which prevents the conﬁdence-vector attacks via
“conﬁdence-masking”. Results that are affected by conﬁdence
masking are marked in red.
(a) Texas Undefended
Attack Accuracy
Gap attack 73:9
Conﬁdence-vector 84:0
Noise Robustness 80:3(b) Texas MemGuard
Attack Accuracy
Gap attack 73:9
Conﬁdence-vector 50:0
Noise Robustness 80:3
(c) Purchase Undefended
Attack Accuracy
Gap attack 67:1
Conﬁdence-vector 86:1
Noise Robustness 87:4(d) Purchase MemGuard
Attack Accuracy
Gap attack 67:1
Conﬁdence-vector 50:0
Noise Robustness 87:4
(e) Location Undefended
Attack Accuracy
Gap attack 72:1
Conﬁdence-vector 92:6
Noise robustness 89:2(f) Location MemGuard
Attack Accuracy
Gap attack 72:1
Conﬁdence-vector 50:0
Noise Robustness 89:2
(g) Adult Undefended
Attack Accuracy
Gap attack 58:7
Conﬁdence-vector 59:9
Noise Robustness 58:7(h) Adult MemGuard
Attack Accuracy
Gap attack 58:7
Conﬁdence-vector 50:0
Noise Robustness 58:7
modify. Moving forward, we recommend that the trivial
gap baseline serve as an indicator of conﬁdence masking:
a conﬁdence-vector attack should not perform signiﬁcantly
worse than the gap attack for a defense to protect against MI.
Thus, to protect against (all) MI attacks, a defense cannot
solely post-process the conﬁdence-vector—the model will
still be vulnerable to label-only attacks.
5.4. Breaking Adversarial Regularization
The work of Nasr et al. (2018a) differs from MemGuard and
prediction puriﬁcation in that it does not simply obfuscate
conﬁdence vectors at test time. Rather, it jointly trains a
target model and a defensive conﬁdence-vector MI classiﬁer
in a min-max fashion: the attack model to maximize MI and
the target model to produce accurate outputs that yet foolthe attacker. See Supplement § D for more details.
We train a target model defended using adversarial regu-
larization, exactly as in (Nasr et al., 2018a). By varying
its hyper-parameters, we achieve a defended state where
the conﬁdence-vector attack is within 3percentage points
of chance, as shown in Supplement Figure 9. Again, our
label-only attacks signiﬁcantly outperform this attack (com-
pare Figures 6 (a) and (b)) because the train-test gap is only
marginally reduced; this defense is not entirely ineffective—
it prevents label-only attacks from exploiting beyond 3per-
centage points of the gap attack. However, when label-only
attacks are sufﬁciently defended, it achieves signiﬁcantly
worse test accuracy trade-offs than other defenses (see Fig-
ure 5). And yet, evaluating the defense solely on conﬁdence-
vector attacks overestimates the achieved privacy.
5.5. The Query Complexity of Label-Only Attacks
We now answer question 3) and investigate how the query
budget affects the success rate of different label-only attacks.
Recall that our rotation attack evaluates N= 3queries of
images rotated by rand our translation attack N= 4d+ 1
for shifts satisfying a total displacement of d. Figure 2 (a)-
(b) shows that there is a range of perturbation magnitudes
for which the attack exceeds the baseline (i.e., 1r8
for rotations, and 1d2for translations). When the
augmentations are too small or too large, the attack performs
poorly because the augmentations have a similar effect on
both train and test samples (i.e., small augmentations rarely
change model predictions and large augmentations often
cause misclassiﬁcations). An optimal parameter choice ( r
andd) outperforms the baseline by 3-4percentage-points,
which an adversary can tune using its local source model.
As we will see in §6, these attacks outperform all others on
models that used data augmentation at training time .
In Figure 2 (c), we compare different boundary distance
attacks, discussed in §3.4. With2;500queries, the label-
only attack matches the white-box upper-bound using 
2;000queries and also matches the best conﬁdence-vector
attack (see Figure 1). With 12;500queries, our combined
attack can outperform all others. Query limiting would
likely not be a suitable defense, as Sybil attacks (Douceur,
2002) can circumvent it; even in low query regimes ( <100)
our attacks outperform the trivial gap by 4percentage points.
Finally, with <300queries, our simple noise robustness
attack outperforms our other label-only attacks. At large
query budgets, our boundary distance attack produces more
precise distance estimates and outperforms it. Note that the
monetary costs are modest at $0:25per sample5.
5https://www.clarifai.com/pricingLabel-Only Membership Inference Attacks
0 2 4 6 8 10 12 14
r in Attack7476788082848688Attack Accuracy, %
Attack Model
Rotation
Gap Attack
Boundary Distance
(a) Rotation attack
0 1 2 3 4 5
d in Attack7476788082848688Attack Accuracy, %
Attack Model
Translation
Gap Attack
Boundary Distance (b) Translation attack
0100101102103104
Number of Queries75.077.580.082.585.087.590.092.595.0Attack Accuracy, %
Attack Model
Label-Only Boundary Distance
Random Noise
Label-Only Boundary & Translation
Gap Attack
White-Box Boundary Distance (c) Boundary distance attack
Figure 2. Comparison of query settings for different label-only MI attacks on CIFAR-10 . Target model are trained on a subset of
2;500data points. In (a) and (b), we compare the performance of the data augmentation attack against two baselines (the gap attack and
the label-only boundary distance attack, with 2;500queries), as we increase the randdparameters. In (c), we compare attacks that
threshold on a point’s distance to the boundary in a black-box setting with a white-box baseline using Carlini and Wagner’s attack (Carlini
& Wagner, 2017). We describe these attacks in § 3.4. Costs are $0:1per1000 queries5.
6. Defending with Better Generalization
Since conﬁdence-masking defenses cannot robustly defend
against MI attacks, we now investigate to what extend stan-
dard regularization techniques—that aim to limit a models’
ability to overﬁt to its training set—can. We study how data
augmentation, transfer learning, dropout (Srivastava et al.,
2014),`1/`2regularization, and differentially private (DP)
training (Abadi et al., 2016) impact MI.
We explore three questions in this section:
A.How does training with data augmentation impact MI
attacks, especially those that evaluate augmented data?
B.How well do other standard machine learning regular-
ization techniques help in reducing MI?
C.How do these defenses compare to differential privacy,
which can provide formal guarantees against any form
of membership leakage?
6.1. Training with Data Augmentation Exacerbates MI
Data augmentation is commonly used in machine learning
to reduce overﬁtting and encourage generalization, espe-
cially in low data regimes (Shorten & Khoshgoftaar, 2019;
Mikołajczyk & Grochowski, 2018). Data augmentation is
an interesting case study for our attacks. As it reduces a
model’s overﬁtting, one would expect it to reduce MI. But,
a model trained with augmentation will have been trained
to strongly recognize xand its augmentations, which is pre-
cisely the signal that our data augmentation attacks exploit.
We train target models with data augmentation similar to
§3.3 and focus on translations as they are most common in
computer vision. We use a simple pipeline where all transla-
tions of each image is evaluated in a training epoch. Though
this differs slightly from the standard random sampling, we
choose it to illustrate the maximum MI when the adversary’squeries exactly match the samples seen in training.
Observe from Figure 3 that augmentation reduces overﬁt-
ting and improves generalization: test accuracy increases
from 49:7%without translations to 58:7%atd= 5and the
train-test gap decreases. Due to improved generalization,
the conﬁdence-vector and boundary distance attacks’ accu-
racies decrease .Yet, the success rate of the data augmen-
tation attack increases. This increase conﬁrms our initial
intuition that the model now leaks additional membership
information via its invariance to training-time augmenta-
tion. Though the model trained with d= 5pixel shifts has
higher test accuracy, our data augmentation attack exceeds
the conﬁdence-vector performance on the non-augmented
model.6Thus, model generalization is not the only variable
affecting its membership leakage: models that overﬁt less
on the original data may actually be more vulnerable to MI
because they implicitly overﬁt more on a related dataset .
Attacking a high-accuracy ResNet We use, without
modiﬁcation, the pipeline from FixMatch (Sohn et al., 2020),
which trains a ResNet-28 to 96% accuracy on the CIFAR-
10 dataset, comparable to the state of the art. As with our
other experiments, this model is trained using a subset of
CIFAR-10, which sometimes leads to observably overﬁt
models indicated by a higher gap attack accuracy. We train
models using four regularizations, all random: vertical ﬂips,
shifts by up to d= 4pixels, image cutout (DeVries & Tay-
lor, 2017), and (non-random) weight decay of magnitude
0:0005 . All are either enabled or disabled.
Our results here, shown in Figure 4 corroborate those ob-
6Though we ﬁnd in Supplement Figure 8 that the attack is strongest
when the adversary correctly guesses d, we note that these values
are often ﬁxed for a domain and image resolution. Thus, adver-
sarial knowledge of the augmentation pipeline is not a strong
assumption.Label-Only Membership Inference Attacks
0 1 2 3 4 5
d in Training70.072.575.077.580.082.585.087.5Attack Accuracy, %Attack Model
Confidence-Vector
Translation
Gap Attack
Boundary Distance
Figure 3. Accuracy of MI attacks on CIFAR-10 models trained
with data augmentation on a subset of 2500 images. As in our
attack,dcontrols the number of pixels by which images are trans-
lated during training, where no augmentation is d= 0. For models
trained with signiﬁcant amounts of data augmentation, MI attacks
become stronger despite it generalizing better.
tained with the simpler pipeline above: though test accuracy
improves, our data augmentation attacks match or outper-
form the conﬁdence-vector attack .
6.2. Other Techniques to Prevent Overﬁtting
We explore questions B)-C) using other standard regulariza-
tion techniques, with details in Supplement E. In transfer
learning, we either only train a new last layer ( last layer
ﬁne-tuning ), or ﬁne tune the entire model ( full ﬁne-tuning ).
We pre-train a model on CIFAR-100 to 51:6%test accuracy
and then use transfer learning. We ﬁnd that boundary dis-
tance attack performed on par with the conﬁdence-vector
in all cases. We observe that last layer ﬁne-tuning degrades
all our attacks to the generalization gap, preventing non-
trivial MI (see Figure 10 in Supplement §F). This result
corroborates intuition: linear layers have less capacity to
overﬁt compared to neural networks. We observe that full
ﬁne-tuning leaks more membership inference but achieves
better test accuracies, as shown in Figure 5.
Finally, DP training (Abadi et al., 2016) formally enforces
that the trained model does not strongly depend on any
individual training point—that it does not overﬁt. We use
differentially private gradient descent (DP-SGD) (Abadi
et al., 2016) (see Supplement §E). To achieve comparable
test accuracies as undefended models, the formal privacy
guarantees become mostly meaningless (i.e., >100).
In Figure 6, we ﬁnd that most forms of regularization fail
to prevent even the baseline gap attack from reaching 60%
accuracy or more. Only strong `2regularization ( 1)
and DP training consistently reduced MI. Figure 5 gives
us a best understanding of the privacy-utility trade-off. We
see that both prevent MI at a high cost in test-accuracy—
0 2000 4000 6000 8000 10000
Number of Training Data Points5060708090100Attack Accuracy, %Attack Type
Confidence-Vector
Data Augmentation
Gap Attack(a) Without Augmentations
0 2000 4000 6000 8000 10000
Number of Training Data Points5060708090100Attack Accuracy, %Attack Type
Confidence-Vector
Data Augmentation
Gap Attack
(b) With Augmentations
Figure 4. Accuracy of membership inference attacks on
CIFAR-10 models trained as in FixMatch (Sohn et al., 2020).
Our data augmentation attacks, which mimic the training augmen-
tations, match or outperform the conﬁdence-vector attacks when
augmentations were used in training. We evaluate 1000 randomly
generated augmentations for this attack. As in previous experi-
ments, we ﬁnd our label-only distance attack performs on par with
the conﬁdence-vector attack. Interestingly, the gap attack accuracy
also improves due to a relatively larger increase in training accu-
racy. “With Augmentations” and “Without Augmentations” refer
to using all regularizations, as in FixMatch, or none, respectively.
they cause the model to underﬁt . However, we also clearly
see the utility beneﬁts of transfer learning: these models
achieve consistently better test-accuracy due to features
learned from non-private data. Combining DP training with
transfer learning mitigates privacy leakage at only minimal
cost in test accuracy, achieving the best tradeoff. When
transfer learning is not an option, dropout performs better.
7. Worst-Case (Outlier) MI
Here, we perform MI only for a small subset of “outliers”.
Even if a model generalizes well on average, it might stillLabel-Only Membership Inference Attacks
35 40 45 50 55 60 65 70
Target Model Test Accuracy, %505560657075808590Attack Accuracy, %Better Defense
Figure 5. Test accuracy and label-only attack accuracy for var-
ious defenses. Same setup as Figure 6. Models towards the
bottom right are more private and more accurate.
0 10 20 30 40 50 60
Train-Test Gap30405060708090Attack Accuracy, %Baseline Gap Attack
Defense Type
Adversarial
Regularization
DP Full Fine Tune
DP Last Layer
Data Augmentation
Differential PrivacyDropout
Full Fine Tune
L1 Regularization
L2 Regularization
Last Layer
MemGuard
None
(a) Label-Only Attacks
0 10 20 30 40 50 60
Train-Test Gap30405060708090Attack Accuracy, %
(b) Full Conﬁdence-Vector Attacks
Figure 6. Comparison of MI attacks on various defenses. Tar-
get models are trained on 2500 data points from CIFAR-10. Point
sizes indicate relative regularization amounts within a defense.
have overﬁt to unusual data in the tails of the distribu-
tion (Carlini et al., 2019). We use a similar but modiﬁed
process as Long et al. (2018) to identify potential outliers.
First, the adversary uses a source model ^hto map each
targeted data point, x, to its feature space, or the activations
of its penultimate layer, denoted as z(x). We deﬁne twopointsx1;x2asneighbors if their features are close, i.e.,
d(z(x1);z(x2)), whered(;)is the cosine distance
andis a tunable parameter. An outlier is a point with
less thanneighbors in z(x)whereis another tunable
parameter. Given a dataset Xof potential targets and an
intended fraction of outliers (e.g., 1%ofX), we tune
andso that a-fraction of points x2Xare outliers. We
use precision as the MI success metric.
We run our attacks on the outliers of the same models as in
Figure 6. We ﬁnd in See Figure 11 in Supplement Section F,
that we can always improve the attack by targeting outliers,
but that strong L2regularization and DP training prevent
MI. As before, we ﬁnd that the label-only boundary distance
attack matches the conﬁdence-vector attack performance.
8. Conclusion
We developed three new label-only membership inference
attacks that can match, and even exceed, the success of
prior conﬁdence-vector attacks, despite operating in a more
restrictive adversarial model. Their label-only nature re-
quires fundamentally different attack strategies, that—in
turn—cannot be trivially prevented by obfuscating a model’s
conﬁdence scores. We have used these attacks to break two
state-of-the-art defenses to membership inference attacks.
We have found that the problem with these “conﬁdence-
masking” defenses runs deeper: they cannot prevent any
label-only attack. As a result, any defenses against MI
necessarily have to help reduce a model’s train-test gap.
Finally, via a rigorous evaluation across many proposed
defenses to MI, we have shown that differential privacy
(with transfer learning) provides the strongest defense, both
in an average-case and worst-case sense, but that it may
come at a cost in the model’s test accuracy.
To center our analysis on comparing the conﬁdence-vector
and label-only settings, we use the same threat model as
prior work (Shokri et al., 2016) and leave a ﬁne-grained
analysis of label-only attacks under reduced adversarial
knowledge (e.g., reduced data and model architecture knowl-
edge (Yeom et al., 2018; Salem et al., 2018)) to future work.
Acknowledgments
We thank the reviewers for their insightful feedback. This
work was supported by CIFAR (through a Canada CIFAR
AI Chair), by NSERC (under the Discovery Program, NFRF
Exploration program, and COHESA strategic research net-
work), and by gifts from Intel and Microsoft. We also thank
the Vector Institute’s sponsors.Label-Only Membership Inference Attacks
References
Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov,
I., Talwar, K., and Zhang, L. Deep learning with differ-
ential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security , CCS
’16, pp. 308–318, New York, NY , USA, 2016. Association
for Computing Machinery. ISBN 9781450341394. doi:
10.1145/2976749.2978318. URL https://doi.org/10.
1145/2976749.2978318 .
Bengio, Y ., Goodfellow, I., and Courville, A. Deep learning ,
volume 1. MIT press, 2017.
Brendel, W., Rauber, J., and Bethge, M. Decision-based adversarial
attacks: Reliable attacks against black-box machine learning
models. arXiv preprint arXiv:1712.04248 , 2017.
Carlini, N. and Wagner, D. Towards evaluating the robustness
of neural networks. In 2017 ieee symposium on security and
privacy (sp) , pp. 39–57. IEEE, 2017.
Carlini, N. and Wagner, D. Audio adversarial examples: Targeted
attacks on speech-to-text. In 2018 IEEE Security and Privacy
Workshops (SPW) , pp. 1–7. IEEE, 2018.
Carlini, N., Liu, C., Erlingsson, ´U., Kos, J., and Song, D. The
secret sharer: Evaluating and testing unintended memorization
in neural networks. In 28thfUSENIXgSecurity Symposium
(fUSENIXgSecurity 19) , pp. 267–284, 2019.
Chen, J., Jordan, M. I., and Wainwright, M. J. Hopskipjumpattack:
A query-efﬁcient decision-based attack, 2019.
Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V ., and Le, Q. V .
Autoaugment: Learning augmentation policies from data, 2018.
Cui, X., Goel, V ., and Kingsbury, B. Data augmentation for deep
neural network acoustic modeling. IEEE/ACM Transactions
on Audio, Speech, and Language Processing , 23(9):1469–1477,
2015.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-
training of deep bidirectional transformers for language under-
standing, 2018.
DeVries, T. and Taylor, G. W. Improved regularization of
convolutional neural networks with cutout. arXiv preprint
arXiv:1708.04552 , 2017.
Douceur, J. R. The sybil attack. In International workshop on
peer-to-peer systems , pp. 251–260. Springer, 2002.
Fadaee, M., Bisazza, A., and Monz, C. Data augmentation for
low-resource neural machine translation. Proceedings of the
55th Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers) , 2017. doi: 10.18653/v1/
p17-2090. URL http://dx.doi.org/10.18653/v1/
P17-2090 .
Ford, N., Gilmer, J., Carlini, N., and Cubuk, D. Adversarial
examples are a natural consequence of test error in noise. arXiv
preprint arXiv:1901.10513 , 2019.
Fredrikson, M., Jha, S., and Ristenpart, T. Model inversion attacks
that exploit conﬁdence information and basic countermeasures.
InProceedings of the 22nd ACM SIGSAC Conference on Com-
puter and Communications Security , pp. 1322–1333, 2015.Gal, Y . Uncertainty in deep learning. University of Cambridge , 1:
3, 2016.
Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and
harnessing adversarial examples, 2014.
Hayes, J., Melis, L., Danezis, G., and Cristofaro,
E. D. Logan: Membership inference attacks against
generative models. Proceedings on Privacy En-
hancing Technologies , 2019(1):133 – 152, 2019.
URL https://content.sciendo.com/view/
journals/popets/2019/1/article-p133.xml .
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for
image recognition, 2015.
Hu, S., Yu, T., Guo, C., Chao, W.-L., and Weinberger, K. Q. A
new defense against adversarial images: Turning a weakness
into a strength. In Advances in Neural Information Processing
Systems , pp. 1635–1646, 2019.
Jayaraman, B., Wang, L., Evans, D., and Gu, Q. Revisiting mem-
bership inference under realistic assumptions. arXiv preprint
arXiv:2005.10881 , 2020.
Jia, J., Salem, A., Backes, M., Zhang, Y ., and Gong, N. Z. Mem-
guard: Defending against black-box membership inference at-
tacks via adversarial examples. In Proceedings of the 2019
ACM SIGSAC Conference on Computer and Communications
Security , pp. 259–274, 2019.
Leino, K. and Fredrikson, M. Stolen memories: Leveraging model
memorization for calibrated white-box membership inference.
arXiv preprint arXiv:1906.11798 , 2019.
Long, Y ., Bindschaedler, V ., and Gunter, C. A. Towards measuring
membership privacy. arXiv preprint arXiv:1712.09136 , 2017.
Long, Y ., Bindschaedler, V ., Wang, L., Bu, D., Wang, X., Tang,
H., Gunter, C. A., and Chen, K. Understanding membership
inferences on well-generalized learning models. arXiv preprint
arXiv:1802.04889 , 2018.
Maas, A. L., Hannun, A. Y ., and Ng, A. Y . Rectiﬁer nonlinear-
ities improve neural network acoustic models. In Proc. icml ,
volume 30, pp. 3, 2013.
Mikołajczyk, A. and Grochowski, M. Data augmentation for
improving deep learning in image classiﬁcation problem. In
2018 International Interdisciplinary PhD Workshop (IIPhDW) ,
pp. 117–122, May 2018. doi: 10.1109/IIPHDW.2018.8388338.
Murphy, K. P. Machine Learning: A Probabilistic Perspective .
The MIT Press, 2012. ISBN 0262018020, 9780262018029.
Nasr, M., Shokri, R., and Houmansadr, A. Machine learning
with membership privacy using adversarial regularization. In
Proceedings of the 2018 ACM SIGSAC Conference on Computer
and Communications Security , pp. 634–646, 2018a.
Nasr, M., Shokri, R., and Houmansadr, A. Machine learning
with membership privacy using adversarial regularization. In
Proceedings of the 2018 ACM SIGSAC Conference on Computer
and Communications Security , pp. 634–646, 2018b.
Ngai, E. W., Hu, Y ., Wong, Y . H., Chen, Y ., and Sun, X. The appli-
cation of data mining techniques in ﬁnancial fraud detection: A
classiﬁcation framework and an academic review of literature.
Decision support systems , 50(3):559–569, 2011.Label-Only Membership Inference Attacks
Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B.,
and Swami, A. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia Conference
on Computer and Communications Security , ASIA CCS ’17,
pp. 506–519, New York, NY , USA, 2017. ACM. ISBN 978-
1-4503-4944-4. doi: 10.1145/3052973.3053009. URL http:
//doi.acm.org/10.1145/3052973.3053009 .
Perez, L. and Wang, J. The effectiveness of data augmentation in
image classiﬁcation using deep learning, 2017.
Pyrgelis, A., Troncoso, C., and De Cristofaro, E. Knock knock,
who’s there? membership inference on aggregate location data.
arXiv preprint arXiv:1708.06145 , 2017.
Sablayrolles, A., Douze, M., Schmid, C., Ollivier, Y ., and J ´egou,
H. White-box vs black-box: Bayes optimal strategies for mem-
bership inference. In International Conference on Machine
Learning , pp. 5558–5567. PMLR, 2019.
Sajjad, M., Khan, S., Muhammad, K., Wu, W., Ullah, A., and Baik,
S. W. Multi-grade brain tumor classiﬁcation using deep cnn
with extensive data augmentation. Journal of computational
science , 30:174–182, 2019.
Salem, A., Zhang, Y ., Humbert, M., Berrang, P., Fritz, M., and
Backes, M. Ml-leaks: Model and data independent membership
inference attacks and defenses on machine learning models,
2018.
Shalev-Shwartz, S. and Ben-David, S. Understanding machine
learning: From theory to algorithms . Cambridge university
press, 2014.
Shokri, R., Stronati, M., Song, C., and Shmatikov, V . Membership
inference attacks against machine learning models, 2016.
Shorten, C. and Khoshgoftaar, T. M. A survey on image data
augmentation for deep learning. Journal of Big Data , 6(1):60,
2019.
Sohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N., Cubuk,
E. D., Kurakin, A., Zhang, H., and Raffel, C. Fixmatch: Simpli-
fying semi-supervised learning with consistency and conﬁdence,
2020.
Song, L., Shokri, R., and Mittal, P. Privacy risks of securing
machine learning models against adversarial examples. In Pro-
ceedings of the 2019 ACM SIGSAC Conference on Computer
and Communications Security , pp. 241–257, 2019.
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and
Salakhutdinov, R. Dropout: a simple way to prevent neural
networks from overﬁtting. The journal of machine learning
research , 15(1):1929–1958, 2014.
Stanﬁll, M. H., Williams, M., Fenton, S. H., Jenders, R. A., and
Hersh, W. R. A systematic literature review of automated clini-
cal coding and classiﬁcation systems. Journal of the American
Medical Informatics Association , 17(6):646–651, 2010.
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D.,
Goodfellow, I., and Fergus, R. Intriguing properties of neural
networks. arXiv preprint arXiv:1312.6199 , 2013.
Tan, C., Sun, F., Kong, T., Zhang, W., Yang, C., and Liu, C. A
survey on deep transfer learning. In International conference
on artiﬁcial neural networks , pp. 270–279. Springer, 2018.Tanay, T. and Grifﬁn, L. A boundary tilting persepective on
the phenomenon of adversarial examples. arXiv preprint
arXiv:1608.07690 , 2016.
Taylor, L. and Nitschke, G. Improving deep learning with generic
data augmentation. In 2018 IEEE Symposium Series on Com-
putational Intelligence (SSCI) , pp. 1542–1547, Nov 2018. doi:
10.1109/SSCI.2018.8628742.
Tian, S., Yang, G., and Cai, Y . Detecting adversarial examples
through image transformation, 2018.
Tram `er, F., Zhang, F., Juels, A., Reiter, M. K., and Ristenpart, T.
Stealing machine learning models via prediction apis. In 25th
fUSENIXgSecurity Symposium ( fUSENIXgSecurity 16) , pp.
601–618, 2016.
Truex, S., Liu, L., Gursoy, M. E., Yu, L., and Wei, W. Towards
demystifying membership inference attacks. arXiv preprint
arXiv:1807.09173 , 2018.
Wang, B. and Gong, N. Z. Stealing hyperparameters in machine
learning. 2018 IEEE Symposium on Security and Privacy (SP) ,
pp. 36–52, 2018.
Yang, Z., Shao, B., Xuan, B., Chang, E.-C., and Zhang, F. De-
fending model inversion and membership inference attacks via
prediction puriﬁcation, 2020.
Yeom, S., Giacomelli, I., Fredrikson, M., and Jha, S. Privacy risk
in machine learning: Analyzing the connection to overﬁtting.
In2018 IEEE 31st Computer Security Foundations Symposium
(CSF) , pp. 268–282. IEEE, 2018.
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
Understanding deep learning requires rethinking generalization.
arXiv preprint arXiv:1611.03530 , 2016.
Zhang, X., Zhao, J., and LeCun, Y . Character-level convolutional
networks for text classiﬁcation. In Proceedings of the 28th
International Conference on Neural Information Processing
Systems - Volume 1 , NIPS’15, pp. 649–657, Cambridge, MA,
USA, 2015. MIT Press.Label-Only Membership Inference Attacks
A. Background
A.1. Machine Learning
We consider supervised classiﬁcation tasks (Murphy, 2012;
Shalev-Shwartz & Ben-David, 2014), wherein a model is
trained to predict some class label y, given input data x.
Commonly, xmay be an image or sentence and yis then
the corresponding label, e.g., a digit 0-9 or a text sentiment.
We focus our study on neural networks (Bengio et al., 2017):
functions composed as a series of linear-transformation lay-
ers, each followed by a non-linear activation. The overall
layer structure is called the model’s architecture and the
learnable parameters of the linear transformations are the
weights . For a classiﬁcation problem with K-classes, the
last layer of a neural network outputs a vector vofKvalues
(often called logits). The softmax function is typically used
to convert the logits into normalized conﬁdence scores:7.
softmax (v)i:=evi=PK
i=1evi2[0;1]. For a model h, we
deﬁne the model’s output h(x)as the vector of softmax
values. The model’s predicted label is the class with highest
conﬁdence, i.e., argmaxih(x)i.
A.1.1. D ATA AUGMENTATION
Augmentations are natural transformations of existing data
points that preserve class semantics (e.g., small translations
of an image), which are used to improve the generalization
of a classiﬁer (Cubuk et al., 2018; Sohn et al., 2020; Taylor
& Nitschke, 2018). They are commonly used on state-of-
the-art models (He et al., 2015; Cubuk et al., 2018; Perez &
Wang, 2017) to increase the diversity of the ﬁnite training
set, without the need to acquire more labeled data (in a
costly process). Augmentations are especially important in
low-data regimes (Sajjad et al., 2019; Fadaee et al., 2017;
Cui et al., 2015) and are domain-speciﬁc: they apply to a
certain type of input, (e.g., images or text).
We focus on image classiﬁers, where the main types of aug-
mentations are afﬁne transformations (rotations, reﬂections,
scaling, and shifts), contrast adjustments, cutout (DeVries
& Taylor, 2017), and blurring (adding noise). By synthe-
sizing a new data sample as an augmentation of an existing
data sample, x0= augment( x), the model can learn a more
semantically-meaningful set of features. Data augmenta-
tion can potentially teach the machine learning model to
become invariant to the augmentation (e.g., rotationally or
translationally invariant).
7While it is common to refer to the output of a softmax as a
“probability vector” because its components are in the range [0;1]
and sum to 1, we refrain from using this terminology given that
the scores output by a softmax cannot be rigorously interpreted
as probabilities (Gal, 2016)A.1.2. T RANSFER LEARNING
Transfer learning is a common technique used to improve
generalization in low-data regimes (Tan et al., 2018). By
leveraging data from a source task , it is possible to transfer
knowledge to a target task . Commonly, a model is trained
on the data of the source task and then ﬁne-tuned on data
from the output task. In the case of neural networks, it is
common to ﬁne-tune either the entire model or just the last
layer.
A.2. Membership Inference
Membership inference attacks (Shokri et al., 2016) are a
form of privacy leakage that identify if a given data sample
was in a machine learning model’s training dataset. Given a
samplexand access to a trained model h, the adversary uses
a classiﬁer or decision rule fhto compute a membership
predictionf(x;h)2f0;1g, with the goal that f(x;h) =
1wheneverxis a training point. The main challenge in
mounting a membership inference attack is creating the
classiﬁerf, under various assumptions about the adversary’s
knowledge of hand its training data distribution.
Prior work assumes that an adversary has only black-box
access to the trained model h, via a query interface that on
inputxreturns part or all of the conﬁdence vector h(x).
Shadow Models The original membership inference at-
tack of Shokri et al. (Shokri et al., 2016) creates a member-
ship classiﬁer f(x;h), tuned on a number of local “shadow”
(or, source) models. Assuming the adversary has access to
data from the same (or similar) distribution as h’s training
data, the shadow model approach trains the auxiliary source
models ^hion this data. Since ^hiis trained by the adver-
sary, they know whether or not any data point was in the
training set, and can thus construct a dataset of conﬁdence
vectors ^hiwith an associated membership label m2f0;1g.
The adversary trains a classiﬁer fto predictmgiven ^hi(x).
Finally, the adversary queries the targeted model hto ob-
tainh(x)and usesfto predict the membership of xinh’s
training data.
Salem et al. (Salem et al., 2018) later showed that this at-
tack strategy can succeed even without data from the same
distribution as h, and only with data from a similar task
(e.g., a different vision task). They also showed that training
shadow models is unnecessary: applying a simple threshold
predictingf(x;h) = 1 (xis a member) when the max pre-
diction conﬁdence, maxih(x), is above a tuned threshold,
sufﬁces.
Towards Label-only Approaches Yeom et al. (Yeom
et al., 2018) propose a simple baseline attack: the adversary
predicts a data point xas being a member of the training set
whenhclassiﬁesxcorrectly. The accuracy of this baselineLabel-Only Membership Inference Attacks
attack directly reﬂects the gap in the model’s train and test
accuracy: if hoverﬁts (i.e., obtains higher accuracy) on its
training data , this baseline attack will achieve non-trivial
membership inference. We call this the gap attack. If the
adversary’s target points are equally likely to be members
or non-members of the training set (see Appendix B.2) , this
attack achieves an accuracy of
1=2 + ( acc train acc test)=2;
where acc train;acc test2[0;1]are the target model’s accuracy
on training data and held out data respectively.
To the best of our knowledge, this is the only attack pro-
posed in prior work that makes use of only the model’s
predicted label, y= argmaxih(x)i. Our goal is to investi-
gate how this simple baseline can be surpassed to achieve
label-only membership inference attacks that perform on
par with attacks that use access to the model’s conﬁdence
scores.
Indirect Membership Inference The work of Long et
al. (Long et al., 2018) investigates membership inference
through indirect access , wherein the adversary only queries
hon inputsx0that are related to x, but notxdirectly. Our
label-only attacks similarly make use of information gleaned
from querying hon data points related to x(speciﬁcally,
perturbed versions of x).
The main difference is that we focus on label-only attacks,
whereas the work of Long et al. (Long et al., 2018) assumes
adversarial access to the model’s conﬁdence scores. Our
attacks will also be allowed to query and obtain the label at
the chosen point x.
Adversarial Examples and Membership Inference
Song et al. (Song et al., 2019) also make use of adversarial
examples to infer membership. Their approach crucially
differs from ours in two aspects: (1) they assume access
to and predict membership using the conﬁdence scores,
and (2) they target models that were explicitly trained to
be robust to adversarial examples. In this sense, (2) bares
some similarities with our attacks on models trained with
data augmentation (see Section 6, where we also ﬁnd that a
model’s invariance to some perturbations can leak additional
membership signal).
Defenses Defenses against membership inference broadly
fall into two categories.
First, standard regularization techniques, such as L2 weight
normalization (Shokri et al., 2016; Jia et al., 2019; Truex
et al., 2018; Nasr et al., 2018a), dropout (Jia et al., 2019), or
differential privacy have been proposed to address the role
that overﬁtting plays in a membership inference attack’s
success rate (Shokri et al., 2016). Heavy regularizationhas been shown to limit overﬁtting and to effectively defend
against membership inference, but may result in a signiﬁcant
degradation in the model’s accuracy. Moreover, Yeom et
al. (Yeom et al., 2018) show that overﬁtting is sufﬁcient, but
not necessary, for membership inference to be possible.
Second, defenses may reduce the information contained in
a model’s conﬁdences, e.g., by truncating them to a lower
precision (Shokri et al., 2016), reducing the dimensionality
of the conﬁdence-vector to only some top kscores (Shokri
et al., 2016; Truex et al., 2018), or perturbing conﬁdences
via an adversary-aware “minimax” approach (Nasr et al.,
2018a; Yang et al., 2020; Jia et al., 2019). These defenses
modify either the model’s training or inference procedure to
produce minimally perturbed conﬁdence vectors that thwart
existing membership inference attacks. We refer to these
defenses as “conﬁdence-masking” defenses.
Outliers in Membership Inference Most membership
inference research is focused on protecting the average-
case user’s privacy: the success of a membership inference
attack is evaluated over a large dataset. Long et al. (Long
et al., 2018) focus on understanding the vulnerability of
outliers to membership inference. They show that some
(<100) outlier data points can be targeted and have their
membership inferred to high (up to 90%) precision (Long
et al., 2017; 2018). Recent work explores how overﬁtting
impacts membership leakage from a defender’s (white-box)
perspective, with complete access to the model (Leino &
Fredrikson, 2019).
B. Evaluation Setup
Because our main goal is to show that label-only attacks
can match the success of prior attacks, we consider a similar
threat model that matches prior work–except that we restrict
the adversary to label-only queries.
As in prior work (Shokri et al., 2016), we assume that the
adversary has: (1) full knowledge of the task; (2) knowledge
of the target model’s architecture and training setup; (3)
partial data knowledge, i.e., access to a disjoint partition
of data samples from the same distribution as the target
model’s training data (see below for more details); and (4)
knowledge of the targeted points’ labels, y.
B.1. Our Threat Model
Generating Membership Data Some works have ex-
plored generating data samples xfor which to perform
membership inference on, which assumes the least data
knowledge (Shokri et al., 2016; Fredrikson et al., 2015).
These cases work best with minimal numbers of features or
binary features because they can take many queries (Shokri
et al., 2016). Other works assumes access to the conﬁdence
vectors (Fredrikson et al., 2015). Our work assumes thatLabel-Only Membership Inference Attacks
candidate samples have already been found by the adversary.
We leave to future work the efﬁcient discovery of these sam-
ples on high-dimensionality data using a label-only query
interface.
In our threat model, we always use a disjoint, non-
overlapping (i.e., no data points are shared) set of samples
for training and test data for the target model. The source
model uses another two separate subsets of the task’s to-
tal data pool. Due to the balanced priors we assume, all
subsets (i.e., the target model training and test sets, and the
source model training and test sets) are always of the same
size. In the case of CIFAR100, we use the target models
training dataset (members) as the source models test dataset
(non-members), and vice versa.
Model Architectures For computer vision tasks, we use
two representative model architectures, a standard convolu-
tional neural network (CNN) and a ResNet (He et al., 2015).
Our CNN has four convolution layers with ReLU activa-
tions. The ﬁrst two 33convolutions have 32ﬁlters and
the second two have 64 ﬁlters, with a max-pool in between
the two. To compute logits we feed the output through a
fully-connected layer with 512neurons. This model has
1:2million parameters. Our ResNet-28 is a standard Wide
ResNet-28 taken directly from (Sohn et al., 2020) with 1:4
million parameters. On Purchase-100, we use a fully con-
nected neural network with one hidden layer of size 128and
theTanh activation function, exactly as in (Shokri et al.,
2016). For Texas-100, Adult, and Locations we mimic this
model but add a second hidden layer matching the ﬁrst.
For the attacks from prior work based on conﬁdence vectors,
and our new label-only attacks based on data augmentations,
we use shallow neural networks as membership predictor
modelsf. Speciﬁcally, for augmentations, we use two lay-
ers of 10 neurons and LeakyReLU activations (Maas et al.,
2013). The conﬁdence-vector attack models use a single
hidden layer of 64 neurons, as originally proposed by Shokri
et al. (Shokri et al., 2016). We train a separate prediction
model for each class We observe minimal changes in attack
performance by changing the architecture, or by replacing
the predictor model fby a simple thresholding rule. Our
combined boundary distance and augmentation attack uses
neural networks as well. For simplicity, our decision bound-
ary distance attacks use a single global thresholding rule,
2;500queries, and the L2 distance metric. See Section 3.4
for more details.
B.2. On Measuring Success
Some recent works have questioned the use of (balanced)
accuracy as a measure of attack success and proposed other
measures more suited for imbalanced priors: where any data
point targeted by the adversary is a-priori unlikely to be atraining point (Jayaraman et al., 2020). As our main goal
is to study the effect of the model’s query interface on the
ability to perform membership inference, we focus here on
the same balanced setting considered in most prior work.
We also note that the assumption that the adversary has a
(near-) balanced prior need not be unrealistic in practice: For
example, the adversary might have query access to models
from two different medical studies (trained on patients with
two different conditions) and might know a-priori that some
targeted user participated in one of these studies, without
knowing which.
C. Threat Model
The goal of a membership inference attack is to determine
whether or not a candidate data point was used to train
a given model. In Table 3, we summarize different sets
of assumptions made in prior work about the adversary’s
knowledge and query access to the model.
C.1. Adversarial Knowledge
The membership inference threat model originally intro-
duced by Shokri et al. (Shokri et al., 2016), and used in
many subsequent works (Long et al., 2017; Truex et al.,
2018; Salem et al., 2018; Song et al., 2019; Nasr et al.,
2018b), assumes that the adversary has black-box access
to the model h(i.e., they can only query the model for
its prediction and conﬁdence but not inspect its learned
parameters ). Our work also assumes black-box model ac-
cess, with the extra restriction (see Section C.2 for more
details) that the model only returns (hard) labels to queries.
Though studying membership inference attacks with white-
box model access (Leino & Fredrikson, 2019) has merits
(e.g., for upper-bounding the membership leakage), our
label-only restriction inherently presumes a black-box set-
ting (as otherwise, the adversary could just run hlocally to
obtain conﬁdence scores). Although we are focused on the
label-only domain, our attack methodologies can be applied
for analysis in the white-box domain.
Assuming a black-box query interface, there are a number
of other dimensions to the adversary’s assumed knowledge
of the trained model:
Task Knowledge refers to global information about the
model’s prediction task and, therefore, of its prediction API.
Examples of task knowledge include the total number of
classes, the class-labels (dog, cat, etc.), and the input format
(3232RGB or grayscale images, etc.). Task knowledge
is always assumed to be known to the adversary, as it is
necessary for the classiﬁer service to be useful to a user.
Training Knowledge refers to knowledge about the
model architecture (e.g., the type of neural network, its
number of layers, etc.) and how it was trained (the trainingLabel-Only Membership Inference Attacks
Table 3. Survey of membership inference threat models .Lis the model’s loss function, is a calibration term reﬂecting the difﬁculty
of the sample, are the model parameters centered around ,0are the parameters on all other datapoints (other than x),aug(x)is a
data augmentation of x(e.g., image translation), x’ is an adversarial-example of x, and dist h(x;y)is the distance from xto the decision
boundary. Train, data, label, and model knowledge mean, respectively, that the adversary (1) knows the model’s architecture and training
algorithm, (2) has access to other samples from the training distribution, (3) knows the true label, yfor a givenx, and (4) knows the
model parameter values.
Query Interface Attack Feature Knowledge Source
conﬁdence vector h(x);y train, data, label (Shokri et al., 2016)
conﬁdence vector h(x) train, data (Long et al., 2017)
conﬁdence vector h(x) – (Salem et al., 2018)
conﬁdence vector L(h(x); y) label (Yeom et al., 2018)
conﬁdence vector L(h(x); y) +(x) label (Sablayrolles et al., 2019)
conﬁdence vector  ( 
0)TrL(h(x); y)train, data, label, model (Sablayrolles et al., 2019)
conﬁdence vector h(x’), y train, data, label (Song et al., 2019)
label-only argmaxh(x);y label (Yeom et al., 2018)
label-only argmaxh(aug(x));y train, data, label ours
label-only disth(x;y) train, data, label ours
label-only disth(aug(x);y) train, data, label ours
algorithm, training dataset size, etc). This information could
be publicly available or inferable from a model extraction
attack (Tram `er et al., 2016; Wang & Gong, 2018).
Data Knowledge constitutes knowledge about the data
that was used to train the target model. Full knowledge
of the training data renders membership inference trivial
because the training members are already known. Partial
knowledge may consist in having access to (or the ability to
generate) samples from the same or a related data distribu-
tion.
Label Knowledge refers to knowledge of the true label
yfor each point xfor which the adversary is predicting
membership. Whether knowledge of a data point implies
knowledge of its true label depends on the application sce-
nario. Salem et al. (Salem et al., 2018) show that attacks
that rely on knowledge of query labels can often be matched
by attacks that do not.
C.2. Query Interface
Our paper studies a different query interface than most prior
membership inference work. The choice of query interface
ultimately depends on the application needs where the target
model is deployed. We deﬁne two types of query interfaces,
with different levels of response granularity:
Full conﬁdence vectors On a queryx, the adversary re-
ceives the full vector of conﬁdence scores h(x)from the
classiﬁer. In a multi-class scenario, each value in this vec-
tor corresponds to an estimated conﬁdence that this class
is the correct label. Restricting access to only part of the
conﬁdence vector has little effect on the adversary’s suc-cess (Shokri et al., 2016; Truex et al., 2018; Salem et al.,
2018).
Label-only Here, the adversary only obtains the predicted
labely= argmaxih(x)i, with no conﬁdence scores. This
is the minimal piece of information that any query-able
machine learning model must provide and is thus the most
restrictive query interface for the adversary. Such a query
interface is also realistic, as the adversary may only get
indirect access to a deployed model in many settings. For
example, the model may be part of a larger system taking
actions based on the model’s predictions—the adversary
can only observe the system’s actions but not the internal
model’s conﬁdence scores.
In this work, we focus exclusively on the above label-only
regime. Thus, in contrast to prior research (Shokri et al.,
2016; Hayes et al., 2019; Truex et al., 2018; Salem et al.,
2018), our attacks can be mounted against anymachine
learning service, regardless of the granularity provided by
the query interface.
D. Conﬁdence-Masking Defense Descriptions
MemGuard This defense solves a constrained optimiza-
tion problem to compute a defended conﬁdence-vector
hdefense(x) =h(x) +n, wherenis an adversarial noise vec-
tor that satisﬁes the following constraints: (1) the model still
outputs a vector of “probabilities”, i.e., hdefense(x)2[0;1]K
andkhdefense(x)k1= 1; (2) the model’s predictions are un-
changed, i.e., argmaxhdefense(x) = argmax h(x); and (3)
the noisy conﬁdence vector “fools” existing membership in-
ference attacks. To enforce the third constraint, the defenderLabel-Only Membership Inference Attacks
locally creates a membership attack predictor f, and then
optimizes the noise nto causefto mis-predict membership.
Prediction Puriﬁcation Prediction puriﬁcation (Yang
et al., 2020) is a similar defense. It trains a puriﬁer model,
G, that is applied to the output vector of the target model.
That is, on a query x, the adversary receives G(h(x)). The
puriﬁer model Gis trained so as to minimize the information
content in the conﬁdence vector, whilst preserving model
accuracy. While the defense does not guarantee that the
model’s labels are preserved at all points, the defense is
by design incapable of preventing the baseline gap attack,
and it is likely that our stronger label-only attacks would
similarly be unaffected (intuitively, G(h(x))is just another
deterministic classiﬁer, so the membership leakage from
a point’s distance to the decision boundary should not be
expected to change).
Adversarial Regularization This defense trains the tar-
get model in tandem with a defensive membership classiﬁer.
This defensive membership classiﬁer is a neural network
that accepts both the conﬁdence-vector, h(x), of the target
model, and the true label, y, that is one-hot encoded. Fol-
lowing the input h(x)there are four fully connected layers
of sizes 100,1024 ,512,64. Following the input y, there are
three fully connected layers of sizes 100,512,64. The two
64neuron layers are concatenated (to make a layer of size
128), and passed through three more fully connected layers
of sizes 256,64, and the output layer of size 1. ReLU acti-
vations are used after every layer except the output, which
uses a sigmoid activation.
The defensive membership classiﬁer and the target model
are trained in tandem. First the target model is trained
a few (here, 3) epochs. Then for ksteps, the defensive
membership classiﬁer is trained using an equal batch on
members and non-members (which should be different from
the held-out set for the target model). After, the target model
is trained on one batch of training data. The target model’s
loss function is modiﬁed to include a regularization term
using the output of the defensive classiﬁer on the training
data. This regularization term is weighted by .
E. Description of Common Regularizers
Dropout (Srivastava et al., 2014) is a simple regularization
technique, wherein a fraction 2(0;1)of weights are
randomly “dropped” (i.e., set to zero) in each training step.
Intuitively, dropout samples a new random neural network
at each step, thereby preventing groups of weights from
overﬁtting. At test time, the model is deterministic and
uses all the learned weights. We experiment with different
dropout probabilities .
L1 and L2 regularization simply add an additional term ofthe formjjwjjto the model’s training loss, where wis
a vector containing all of the model’s weights, the norm is
either L1 or L2, and  > 0is a hyper-parameter govern-
ing the scale of the regularization relative to the learning
objective. Strong regularization (i.e., large ) reduces the
complexity of the learned model (i.e., it forces the model
to learn smaller weights). We experiment with different
regularization constants .
Differential privacy guarantees that any output from a (ran-
domized) algorithm on some dataset D, would have also
been output with roughly the same probability (up to a
multiplicative efactor) if one point in Dwere arbitrarily
modiﬁed. For differential privacy, we use DP-SGD (Abadi
et al., 2016), a private version of stochastic gradient descent
that clips per-example gradients to an L2 norm of , and
adds Gaussian noise N(0;c22)to each batch’s gradient.
We train target models with ﬁxed parameters c= 0:5and
= 2. We train for a varied number of steps, to achieve
provable differential privacy guarantees for 10250.
F. Additional Figures
1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0
Number of Shadow Models8283848586878889Attack Accuracy, % Attack Model
Confidence Vector
Boundary Distance
Data Augmentation
Figure 7. Attack accuracy of our label-only attacks for various
numbers of shadow models. Target and source models are trained
on 1000 data points from CIFAR-10. The number of shadow
models does not have a signiﬁcant impact on the attack accuracy.Label-Only Membership Inference Attacks
0 1 2 3 4 5
d in Attack67.570.072.575.077.580.082.585.087.5Attack Accuracy, %d in Training
0
1
2
3
4
5
Figure 8. Attack accuracy of our translation attack for various
choices ofd.Target models are trained on 2500 data points from
CIFAR-10 with varied sizes of translation augmentations. The
attack’s accuracy is maximized when it evaluates the same size d
of translations as used for training.
1 2 3 4 5
Regularization Term, λ5055606570758085Attack Accuracy, %Confidence-Vector
Label-Only Boundary Distance
Gap Attack
Figure 9. Accuracy of membership inference attacks on
CIFAR-10 models protected with Adversarial Regulariza-
tion (Nasr et al., 2018a). Target models are trained on a subset of
2500 images. We test several values of k, the ratio of maximization
to minimization steps and ﬁnd that setting k= 1enabled the target
model to converge to a defended state. We report results as we vary
the second hyper-parameter, , which balances the two training
objectives (low training error and low membership leakage). This
defense strategy does not explicitly aim to reduce the train-test
gap and thus does not protect against label-only attacks. However,
we ﬁnd that this defense prevents attacks from exploiting beyond
3percentage points of the gap attack. Test accuracy ranges from
45% to20% , where3had a test accuracy below 35% .
0 2000 4000 6000 8000 10000
Number of Training Data Points30405060708090100Attack Accuracy, %
Transfer Learning Type
Full Fine Tuning
Last Layer
None
Attack Model
Gap Attack
Boundary DistanceFigure 10. Accuracy of membership inference attacks on
CIFAR-10 models trained with transfer learning. The source
model for transfer learning is trained on all of CIFAR-100. Models
are tuned on subsets of CIFAR-10.
Dropout,
droprate=0.8Dropout,
droprate=0.9DP, ɛ=250 L2, λ=3.0
Defense Type0246810Precision Improvement, Percentage Points
Figure 11. Outlier membership inference attacks on defended
models. Target and source models are trained on a subset of 2500
points from CIFAR-10. = 2% outliers are identiﬁed with less
than= 10 neighbors. We show precision-improvement from the
undefended model, using our label-only boundary distance attack.