DiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
Keiichiro Yamamura1Haruki Sato1Nariaki Tateiwa1 2Nozomi Hata3Toru Mitsutake1Issa Oe1
Hiroki Ishikura1Katsuki Fujisawa3
Abstract
Deep learning models are vulnerable to adver-
sarial examples, and adversarial attacks used to
generate such examples have attracted consider-
able research interest. Although existing meth-
ods based on the steepest descent have achieved
high attack success rates, ill-conditioned prob-
lems occasionally reduce their performance. To
address this limitation, we utilize the conjugate
gradient (CG) method, which is effective for this
type of problem, and propose a novel attack al-
gorithm inspired by the CG method, named the
Auto Conjugate Gradient (ACG) attack. The re-
sults of large-scale evaluation experiments con-
ducted on the latest robust models show that, for
most models, ACG was able to Ô¨Ånd more adver-
sarial examples with fewer iterations than the ex-
isting SOTA algorithm Auto-PGD (APGD). We
investigated the difference in search performance
between ACG and APGD in terms of diversiÔ¨Å-
cation and intensiÔ¨Åcation, and deÔ¨Åne a measure
called Diversity Index (DI) to quantify the degree
of diversity. From the analysis of the diversity
using this index, we show that the more diverse
search of the proposed method remarkably im-
proves its attack success rate.
1. Introduction
Deep learning models are effective for various machine
learning tasks, and are increasingly being applied to
safety-critical tasks such as automated driving. However,
deep learning models may misclassify adversarial exam-
ples(Szegedy et al., 2014; Goodfellow et al., 2015) formed
1Graduate School of Mathematics, Kyushu University,
Fukuoka, Japan.2Present afÔ¨Åliation is NTT Software Innovation
Center, NTT Corporation.3Institute of Mathematics for Industry,
Kyushu University, Fukuoka, Japan.. Correspondence to: Kei-
ichiro Yamamura .
Proceedings of the 39thInternational Conference on Machine
Learning , Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).
Figure 1. An example of our intended search for a multimodal
function. The search Ô¨Ånds the global optima owing to an appropri-
ate balance between diversiÔ¨Åcation and intensiÔ¨Åcation. The initial
search point is indicated by the white star, and the search ends at
the white square. The circles represent the search points, and the
black circles indicate that the search has been diversiÔ¨Åed.
by applying perturbations to their inputs which are too
small for the human eye to perceive. Hence, improving
the robustness of deep learning models against adversar-
ial examples is crucial for safety-critical tasks. Adversarial
training (Goodfellow et al., 2015) is an effective method
used to create robust models. In adversarial training, ad-
versarial examples are generated and added to the training
data. This requires many adversarial examples to be gener-
ated quickly for efÔ¨Åcient training.
Anadversarial attack is a method used to generate adver-
sarial examples. A white-box adversarial attack assumes
that the algorithm can obtain the outputs and gradients of
the deep learning models.
The fast gradient sign method (FGSM) (Goodfellow et al.,
2015), iterative-FGSM (I-FGSM) (Kurakin et al., 2017),
and projected gradient descent (PGD) attack (Madry et al.,
2018) use the sign of the steepest gradients at the update.
In addition, some attack strategies have successfully im-
proved on the performance of these methods by introduc-
ing past search information. Momentum I-FGSM (DongarXiv:2206.09628v2 [cs.LG] 19 Jul 2022DiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
et al., 2018) and Nesterov I-FGSM (Lin et al., 2020) are
based on the momentum method, which simultaneously
considers the current and past gradient to determine the
next update. Similarly, Auto-PGD (APGD) (Croce & Hein,
2020b), based on projected gradient descent (PGD) also
considers the inertia direction of the search point in the
next update. However, searches performed by algorithms
based on the steepest gradient descent may be insufÔ¨Åcient
because the objective function of an adversarial attack is
nonconvex, nonlinear, and multimodal. We classify the ex-
isting methods according to their settings in Appendix A.
The relevant literature on adversarial examples has been
summarized at the URL provided below1.
In this study, a new white-box adversarial attack called an
Auto Conjugate Gradient (ACG) attack is proposed based
on the conjugate gradient (CG) method. The CG method is
a well-known algorithm for systems of linear equations and
is also used in nonlinear optimization. It updates the search
point in more diverse directions compared to the steepest
direction, and can be predicted to search extensively (see
Figure 1 and Appendix B).
To the best of our knowledge, our proposed method is
the Ô¨Årst adversarial attack with a high performance based
on the CG. We compared our ACG with APGD, a SOTA
white-box adversarial attack, on 64 robust models listed in
RobustBench (Croce et al., 2021). The results show that the
attack success rate (ASR) of ACG was much higher than
that of APGD, with the exception of only a single model
(see Tables 2 and 3). Surprisingly, ACG with 1 restart (100
iterations) performed better than APGD with 5 restarts (5
100 iterations) against approximately three-fourths of all
robust models, although the execution time per iteration of
ACG and APGD was almost equal. We thoroughly ana-
lyzed the factors involved in the improved attack perfor-
mance of ACG. Compared to APGD, it may be observed
from the results that the movement of the search points in
ACG was large, and the attacked class was varied more of-
ten during the search.
Moreover, we analyze ACG and APGD for diversiÔ¨Åca-
tion and intensiÔ¨Åcation . DiversiÔ¨Åcation and intensiÔ¨Åcation
have received considerable attention in the Ô¨Åeld of meta-
heuristics ( ÀáCrepin≈°ekMatej et al., 2013), where the objec-
tive function is generally nonconvex and multimodal, simi-
lar to deep neural networks. To control the algorithms prop-
erly, some studies on metaheuristics (Cheng et al., 2014;
Morales-Casta√±eda et al., 2020) have attempted to quan-
tify the balance between diversiÔ¨Åcation and intensiÔ¨Åcation.
However, to the best of our knowledge, no such methods
have been proposed for gradient-based iterative searches
such as adversarial attacks. Therefore, we propose Diver-
1https://nicholas.carlini.com/writing/
2019/all-adversarial-example-papers.htmlsity Index (DI) to quantify the degree of the diversity of
the search points and analyze adversarial attacks (see Sec-
tion 5.1). Compared to APGD, we demonstrate that ACG
can search more extensively by an analysis of the DI .
The contributions of this study are summarized as follows.
‚Ä¢ We propose a new adversarial attack called ACG. In a
large-scale experiment on 64 robust models, the ASR
of ACG overwhelmingly outperformed that of APGD,
a SOTA adversarial attack, except for a single model
(see Section 4). The ASR of ACG with 1 restart (100
iterations) is generally better than that of APGD with
5 restarts (5100 iterations).
‚Ä¢ We propose a metric DI to quantify the degree of di-
versity and intensity of the search points of gradient-
based iterative search algorithms. The DI measure
was evaluated, and the results indicated that the search
performed by ACG was more diversiÔ¨Åed than that of
APGD (see Section 5).
Our code is available at the URL given below2.
2. Preliminaries
2.1. Problem Settings
Let the locally differentiable function g:DRm!RK
be aK-classiÔ¨Åer that classiÔ¨Åes by arg max
k=1;:::;K(gk()), and let
xorig2 D be a point classiÔ¨Åed as class cbyg. Given
the distance function d(;)and" > 0, the feasible re-
gionSin an adversarial attack is deÔ¨Åned as S=fx2
Djd(xorig;x)"g. We then deÔ¨Åne an adversarial exam-
ple asxadv2D, which satisÔ¨Åes
arg max
k=1;:::;Kgk(xadv)6=c; d(xorig;xadv)": (1)
LetLbe the objective function to search for xadv. The
adversarial attack can be formulated as follows.
max
x2DL(g(x);c)s.t.d(xorig;x)": (2)
The above formulation renders xless discriminative to the
classcbyg. In classiÔ¨Åers that apply image classiÔ¨Åcation,
the Euclidean distance d(v;w) :=kv wk2, the uniform
distanced(v;w) :=kv wk1, andD= [0;1]mare often
used. We refer to adversarial attacks that use the uniform
distance asl1attack.
2.2. PGD Attack
The PGD method is effective for solving the problem (2).
Givenf:Rm!Rand the formulation max
x2Sf(x), let
2https://github.com/yamamura-k/ACGDiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
the iterations in PGD be k= 1;:::;N iter, wherex(k+1)=
PS(x(k)+(k)rf(x(k))), in which(k)is the step size and
PSis the projection onto the feasible region S. APGD adds
a momentum update method to PGD. Let (k)be the update
direction for each iteration (e.g., rf(x(k))for a uniform
distance case). The update rules of APGD in a single itera-
tion, including the momentum term, are deÔ¨Åned as follows.
z0(k+1)=x(k)+(k)((k)) (3)
z(k+1)=PS(z0(k)) (4)
x(k+1)=PS(x(k)+(z(k+1) x(k))+ (5)
(1 )(x(k) x(k 1)));
where()is a type of normalization and is a coefÔ¨Åcient
representing the strength of the momentum term, and =
0:75is used in APGD.
2.3. General Conjugate Gradient Method
The conjugate gradient method (CG) was developed to
solve linear equations and subsequently extended to the
minimization of strictly convex quadratic and general non-
linear functions. Most existing works on CG methods have
considered unconstrained optimization problems, but CG
can be applied to constrained problems by using projection.
Given an initial point x(0), the initial conjugate gradient
s(0)is set to 0, and thek-th search point x(k)and conju-
gate gradient s(k)are updated as s(k)= rf(x(k)) +
(k)s(k 1);(k)= arg minff(x(k)+s(k))j
0g;x(k+1)=x(k)+(k)s(k)wherek1and(k)is
a parameter calculated from past search information. The
step size(k)is usually determined by a linear search to
satisfy some conditions such as the Wolfe conditions, be-
cause solving arg minff(x(k)+s(k))j0gexactly is
difÔ¨Åcult.
Consider the problem of minimizing the strictly convex
quadratic function f(x) =xTAx+bTx, whereAis a
positive deÔ¨Ånite matrix and x2Rn. In this case, the coefÔ¨Å-
cient(k)ishAs(k 1); rf(x(k))i
hAs(k 1);s(k 1)i. When the objective func-
tion is a strictly convex quadratic function, CG is known to
be able to Ô¨Ånd the global solution in less than niterations
under an exact linear search.
For nonlinear functions, some formulas have been pro-
posed to calculate (k)have been proposed (for further de-
tails, see (Hager & Zhang, 2006)). In this study, we use
the following formula for (k)proposed by (Hestenes &
Stiefel, 1952), which exhibited the highest ASR in our pre-
liminary experiments (Appendix I).
(k)
HS=hrf(x(k));y(k 1)i
hs(k 1);y(k 1)i; (6)
wherey(k 1)=rf(x(k)) rf(x(k 1)). Theoreti-
cal convergence studies on CG methods often assume that(k)0(Hager & Zhang, 2006) and some implementa-
tions use maxf(k);0ginstead of(k).
Algorithm 1 ACG
1:Input:f;S;x(0);(0);Niter;W=fw0;:::;w ng
2:Output:xadv
3:xadv x(0);(0) 0;s(0) rf(x(0))
4:xpre x(0);spre s(0)
5:fork= 0toNiter 1do
6: Computex(k+1)(10)
7: iff(x(k+1))>f(xadv)then
8:xadv x(k+1);xpre x(k);spre s(k)
9: end if
10:(k+1) (k)
11: ifk2Wthen
12: ifCondition (I) or(II) is satisÔ¨Åed then
13:(k+1) (k)=2;
14:x(k+1) xadv;x(k) xpre;s(k) spre
15: end if
16: end if
17: Compute(k+1)(8) ands(k+1)(9)
18:end for
3. Auto Conjugate Gradient (ACG) Attack
We propose the Auto Conjugate Gradient (ACG) attack as a
novel adversarial attack inspired by the CG approach. The
proposed scheme is summarized in Algorithm 1.
The major differences among ACG, general CG, and
APGD are summarized in Table 1. We apply the step
size strategy employed in APGD instead of a linear
search because the forward propagation is relatively time-
consuming. In addition, we do not restrict 0, whereas
the general CG usually makes non-negative (see Ap-
pendix G).
3.1. ACG Step
To solve the maximization problem as an adversarial at-
tack, we use rf()instead ofrf()in (6), that is,
y(k 1)=rf(x(k 1)) rf(x(k)); (7)
(k)
HS=h rf(x(k));y(k 1)i
hs(k 1);y(k 1)i; (8)
s(k)=rf(x(k)) +(k)
HSs(k 1); (9)
x(k+1)=PS
x(k)+(k)(s(k))
; (10)
where()is a type of normalization. ACG uses a sign
function as because many previous studies including
APGD use it for an l1attack. There is a possibility of
division by zero when calculating (k)
HS, and empirically, itDiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
Table 1. The differences among ACG, General CG, and APGD.
(k);;(k)refer to the update direction, whether to normalize
(k), and the step size strategy, respectively (described in Sec-
tion 2).s(k)is the CG direction. The "momentum" column refers
to whether a momentum term is used in the update.
(k)momentum  (k)
ACG s(k)- X Section 3.2
General CG s(k)- - linear search
APGD rf(x(k)) X X Section 3.2
occurs wheny(k 1)=0. Therefore, when division by zero
is called for, the issue is addressed by setting (k)= 0.
3.2. Step Size Selection
We use the same method proposed in APGD to select the
step size. The initial step size (0)is set to 2", and when the
number of iterations reaches the precomputed checkpoint
wj, the step size is halved if either of the following two
conditions are satisÔ¨Åed.
(I)Ninc<(wj wj 1),
(II)(wj 1)=(wj)andf(wj 1)
max =f(wj)
max,
whereNinc:= #fi=wj 1;;wj 1jf(x(i+1))>
f(x(i))gandf(k)
max:= maxff(x(i))ji= 1;;kg.
4. Experiments
We investigated the performance of ACG for an l1attack
using the robust models listed in RobustBench.
Models and Dataset: We used 64 models, i.e., 42, 17,
and 5 models based on the CIFAR-10, CIFAR-100, and Im-
ageNet datasets, respectively. From a validation dataset, we
used 10,000 test images for the evaluation when applying
the CIFAR-10 and CIFAR-100 datasets, and 5,000 images
when using the ImageNet dataset.
Loss Function: We used the CW loss (Carlini & Wagner,
2017) as the objective function. Let cbe the correct answer
class for input x. Then, the CW loss is deÔ¨Åned as follows.
CW(x;c) = gc(x) + max
i6=cgi(x): (11)
An attack using CW loss succeeds if we Ô¨Ånd an adversarial
examplexadvthat satisÔ¨Åes CW(xadv;c)0. The results
of experiments using the DLR loss proposed in (Croce &
Hein, 2020b) are shown in Appendix F.
Initial Points. In the case of an l1attack, the center of
the feasible region ( l1-ball with a diameter of ") is de-
Ô¨Åned asu+`
2, whereu= min(xorig+"1;1), and`=
max(xorig "1;0). We referred to one hundred iterations
406080100200iterations404550APGDACGattack success rate (%)Figure 2. Transitions of the ASR of APGD and ACG. The attack
model was (Ding et al., 2020).
of the update from an initial point as a restart. Following
APGD, we applied 5 restarts in the experiments. The ini-
tial point of the Ô¨Årst restart was the center of the feasible
region, whereas the others were determined through sam-
pling from a uniform distribution. For reproducibility of
the results, the seed of the random numbers was Ô¨Åxed.
4.1. Comparison of ACG and APGD
To evaluate the attack performance under the formulation
(2), we conducted experiments to compare the performance
of APGD, a SOTA method, with that of ACG. The parame-
ters for the step size selection , checkpoints w, the number
of iterations Niter, and the number of restarts were the same
as in the study on APGD, i.e., = 0:75,Niter= 100 , and
5 restarts. The results are summarized in Tables 2 to 3. The
columns APGD( N) and ACG(N) show the attack success
rate (ASR) of APGD and ACG with Nrestarts, respec-
tively. From these tables, it may be observed that the ASRs
of ACG were higher than those of APGD in all but only 1
of the 64 models. These results indicate that ACG exhib-
ited a higher attack performance, regardless of the datasets
or the model‚Äôs architectures used.
Surprisingly, when comparing APGD(5) and ACG(1), it
may be noted that ACG(1) achieved the same or higher
ASR for three-fourths of all 64 models with fewer restarts
(see Tables 2 to 3). Therefore, we expect that ACG will en-
able faster attacks with fewer inferences. Details are pro-
vided in the next section. Furthermore, ACG(1) does not
rely on random numbers to select an initial point because
the initial point is the center of the feasible region. In other
words, ACG outperforms APGD with only deterministic
operations.
Figure 2 shows the transition of the ASR for APGD and
ACG. The ASR of APGD rapidly increased and almost
converged in the early stages of the search. By contrast,
the ASR of ACG continues to increase even at the end of
the search. Note that this Ô¨Ågure is created based on the
data obtained from a model (Ding et al., 2020), whereas
the same trend is also observed for other models.DiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
Table 2. The ASR of APGD and ACG for the robust models listed in RobustBench. CIFAR-100 was used as the dataset and "=8
255.
The highest ASR is in bold, and the second is underlined. APGD( N) refers to APGD with N-times the initial point selection. The
meanings of the other columns are the same.
CIFAR-100 ( "= 8=255) Attack Success Rate
paper Architecture APGD(1) ACG(1) APGD(5) ACG(5) diff
(Addepalli et al., 2021) PreActResNet-18 72.10 72.12 72.25 72.47 0.22
(Rade & Moosavi-Dezfooli, 2021) PreActResNet-18 70.40 70.63 70.55 70.86 0.31
(RebufÔ¨Å et al., 2021) PreActResNet-18 70.86 71.07 70.93 71.29 0.36
(Rice et al., 2020) PreActResNet-18 79.78 80.24 79.99 80.63 0.64
(Hendrycks et al., 2019) WideResNet-28-10 69.28 69.97 69.50 70.51 1.01
(RebufÔ¨Å et al., 2021) WideResNet-28-10 66.41 66.87 66.67 67.27 0.60
(Addepalli et al., 2021) WideResNet-34-10 68.53 68.12 68.74 68.52 -0.21
(Chen & Lee, 2021) WideResNet-34-10 68.24 68.42 68.36 68.77 0.41
(Cui et al., 2021) WideResNet-34-10 71.85 72.16 72.15 72.56 0.41
(Cui et al., 2021) WideResNet-34-10 69.63 69.96 69.87 70.33 0.46
(Sitawarin et al., 2021) WideResNet-34-10 73.07 73.64 73.43 74.27 0.84
(Wu et al., 2020) WideResNet-34-10 69.13 69.58 69.32 70.11 0.79
(Chen et al., 2021) WideResNet-34-10 71.76 71.78 71.96 72.18 0.22
(Cui et al., 2021) WideResNet-34-20 68.50 68.75 68.72 69.13 0.41
(Gowal et al., 2020) WideResNet-70-16 61.23 61.67 61.55 62.19 0.64
(Gowal et al., 2020) WideResNet-70-16 68.76 69.13 69.04 69.43 0.39
(RebufÔ¨Å et al., 2021) WideResNet-70-16 63.94 64.38 64.17 64.77 0.60
4.2. Comparison of execution time
We compared the execution times of APGD and ACG with
Intel(R) Xeon(R) Gold 6240R CPU and NVIDIA GeForce
RTX 3090 GPU. The execution time was recorded as the
time elapsed from the start to the end of the attack. Ta-
ble 4 shows that APGD with Ô¨Åve restarts (APGD(5)) took
22m, 5.88s to attack (Ding et al., 2020), and ACG with Ô¨Åve
restarts (ACG(5)) took 21m, 15.67s in real time. In addi-
tion, APGD with one restart (APGD(1)) took 6m, 45.26s
to attack (Ding et al., 2020), and ACG with one restart
(ACG(1)) took 6m, 56.78s. From this experiment, the ra-
tio of the execution time of ACG(5) to APGD(5) was about
0.96 and that of ACG(1) to APGD(1) was approximately
1.03. This means the computational cost of ACG is nearly
the same as that of APGD. Furthermore, ACG with one
restart outperformed APGD with Ô¨Åve restarts in terms of
the ASR, which show that ACG was able to achieve a
higher ASR more than 3 times faster than APGD.
4.3. Variations in the Class Attacked
When we attacked using the CW loss, the search was up-
dated to Ô¨Ånd inputs classiÔ¨Åed in class c06=c. SpeciÔ¨Åcally,
c0isarg max
i6=cgi(x), and we refer to this c0as the CW tar-
get class (CTC ). The value of c0varied during the search,
depending on the search point. This section summarizes
the CTC results of ACG and APGD attacks based on the
results of the CIFAR-10 dataset.
First, CTCs were changed at least once during the search
in 42.47% of inputs for ACG and 1.36% for APGD. The
average number of times the CTC was switched during thesearch was 2.14 for ACG and 0.02 for APGD. In APGD,
CTC showed almost no change from the initial CTC dur-
ing the search. In contrast, ACG frequently switched CTCs
during the search. Table 5 also summarizes the differences
in the Ô¨Ånal CTCs of ACG and APGD in terms of success-
ful and unsuccessful attacks. As shown in the table, ACG
attacked a different class than APGD for 96.14% of the
images that failed in APGD but succeeded in ACG. The
same trend was observed in the other models, suggesting
that ACG increased the ASR by switching CTCs and at-
tacking a different class than APGD.
4.4. Effect of Conjugate Gradient
Tables 2 and 3 shows that ACG achieved a higher ASR
value than APGD. Because the update function is the major
difference between the two algorithms (see Table 1), we
investigated how the conjugate gradient affected the search
(see the formula (9)).
The top portion of Figure 3 shows the transitions of the
2-norm between two successive search points kx(k+1) 
x(k)k2on APGD and ACG. Regarding the distance moved
between the two points, it may be observed that the search
points of ACG moved further than those of APGD. More-
over, to investigate the effect of the projection on APGD,
we calculatedkz(k+1) z0(k+1)k2
kx(k+1) x(k)k2as the ratio of the dis-
tance traveled between the two search points, which in-
dicates the amount of update distance wasted by the pro-
jection. Although we mainly focused on the numerator
kz(k+1) z0(k+1)k2, we divided it by kx(k+1) x(k)k2
to exclude the effect of the difference in step size. The
ratio of the distance traveled in 100 iterations is shown atDiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
Table 3. Same as in Table 2, but using CIFAR-10 and ImageNet datasets with "=8
255;4
255, respectively.
CIFAR-10 ("= 8=255) Attack Success Rate
paper Architecture APGD(1) ACG(1) APGD(5) ACG(5) diff
(Rade & Moosavi-Dezfooli, 2021) PreActResNet-18 42.33 42.49 42.46 42.65 0.19
(Rade & Moosavi-Dezfooli, 2021) PreActResNet-18 41.51 41.84 41.65 42.12 0.47
(RebufÔ¨Å et al., 2021) PreActResNet-18 42.73 43.01 42.91 43.15 0.24
(Andriushchenko et al., 2020) PreActResNet-18 53.55 54.42 53.82 54.90 1.08
(Sehwag et al., 2021) ResNet-18 43.62 44.16 43.91 44.79 0.88
(Chen et al., 2020) ResNet-50 47.95 48.12 48.08 48.28 0.20
(Wong et al., 2020) ResNet-50 54.02 54.75 54.26 55.44 1.18
(Engstrom et al., 2019) ResNet-50 47.69 48.43 48.08 49.25 1.17
(RebufÔ¨Å et al., 2021) WideResNet-106-16 34.43 34.70 34.71 35.03 0.32
(Carmon et al., 2019) WideResNet-28-10 39.38 39.68 39.59 40.03 0.44
(Gowal et al., 2020) WideResNet-28-10 36.33 36.63 36.45 36.90 0.45
(Hendrycks et al., 2019) WideResNet-28-10 43.53 43.97 43.82 44.36 0.54
(Rade & Moosavi-Dezfooli, 2021) WideResNet-28-10 38.48 38.62 38.64 38.87 0.23
(RebufÔ¨Å et al., 2021) WideResNet-28-10 38.29 38.43 38.47 38.80 0.33
(Sehwag et al., 2020) WideResNet-28-10 41.75 42.07 41.93 42.41 0.48
(Sridhar et al., 2021) WideResNet-28-10 39.27 39.49 39.45 39.85 0.40
(Wang et al., 2020) WideResNet-28-10 41.85 42.12 42.15 42.57 0.42
(Wu et al., 2020) WideResNet-28-10 39.38 39.49 39.56 39.70 0.14
(Zhang et al., 2021) WideResNet-28-10 39.79 39.93 39.98 40.25 0.27
(Ding et al., 2020) WideResNet-28-4 48.73 53.40 49.67 55.77 6.10
(Cui et al., 2021) WideResNet-34-10 46.20 46.42 46.41 46.90 0.49
(Huang et al., 2020) WideResNet-34-10 46.09 46.30 46.19 46.72 0.53
(Rade & Moosavi-Dezfooli, 2021) WideResNet-34-10 36.30 36.57 36.46 36.83 0.37
(Sehwag et al., 2021) WideResNet-34-10 39.32 39.84 39.58 40.18 0.60
(Sitawarin et al., 2021) WideResNet-34-10 46.91 47.58 47.23 48.02 0.79
(Wu et al., 2020) WideResNet-34-10 43.21 43.24 43.36 43.60 0.24
(Zhang et al., 2019a) WideResNet-34-10 52.79 53.46 53.08 54.15 1.07
(Zhang et al., 2019b) WideResNet-34-10 46.44 46.75 46.65 47.18 0.53
(Zhang et al., 2020) WideResNet-34-10 45.54 45.78 45.68 46.12 0.44
(Chen et al., 2021) WideResNet-34-10 47.33 47.47 47.58 48.00 0.42
(Sridhar et al., 2021) WideResNet-34-15 38.75 38.93 38.90 39.15 0.25
(Cui et al., 2021) WideResNet-34-20 45.63 45.91 45.88 46.23 0.35
(Gowal et al., 2020) WideResNet-34-20 42.55 42.72 42.65 42.86 0.21
(Pang et al., 2020) WideResNet-34-20 44.54 44.96 44.75 45.33 0.58
(Rice et al., 2020) WideResNet-34-20 44.69 45.25 44.92 45.69 0.77
(Huang et al., 2021) WideResNet-34-R 36.12 36.34 36.27 36.76 0.49
(Huang et al., 2021) WideResNet-34-R 37.10 37.27 37.33 37.79 0.46
(Gowal et al., 2020) WideResNet-70-16 33.24 33.49 33.42 33.70 0.28
(Gowal et al., 2020) WideResNet-70-16 41.95 42.20 42.12 42.45 0.33
(Gowal et al., 2021) WideResNet-70-16 32.25 32.71 32.57 33.04 0.47
(RebufÔ¨Å et al., 2021) WideResNet-70-16 32.28 32.44 32.46 32.75 0.29
(RebufÔ¨Å et al., 2021) WideResNet-70-16 34.76 34.95 35.04 35.27 0.23
ImageNet ("= 4=255)
(Salman et al., 2020) ResNet-18 72.80 73.34 73.00 73.72 0.72
(Salman et al., 2020) ResNet-50 62.72 63.06 62.86 63.70 0.84
(Wong et al., 2020) ResNet-50 71.58 71.64 71.70 71.94 0.24
(Engstrom et al., 2019) ResNet-50 67.74 68.08 67.86 68.60 0.74
(Salman et al., 2020) WideResNet-50-2 58.88 59.36 58.96 59.92 0.96
Summary
the number of bold models 0 0 1 63
the number of underlined models 1 49 14 0DiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
Table 4. The average execution time of APGD and ACG. We measure the execution time 5 times with Intel(R) Xeon(R) Gold 6240R
CPU @ 2.40GHz and NVIDIA GeForce RTX 3090 GPUs.
(Ding et al., 2020) APGD(1) ACG(1) APGD(5) ACG(5) CPU RAM GPU
ASR 48.73 53.40 49.67 55.77 Intel(R) Xeon(R) 786GB NVIDIA
time 6m45.26s 6m56.78s 22m5.88s 21m15.67s Gold 6240R GeForce
ratio 0.97 1 3.18 3.06 2 RTX 30904
Table 5. Average percentage of successful attacks and differ-
ent CTC. "Success(xx.xx%)" indicates the percentage of images
with successful attacks among all attacked images, and "Fail-
ure(xx.xx%)" indicates that for images with failed attacks. The
"Different CTC" column shows the percentage of the images in
which ACG attacks have a different CTC from APGD in the cor-
responding row.
APGD ACG Different CTC
Success(42.33%)Success(42.27%) 6.18%
Failure(0.06%) 11.96%
Failure(57.67%)Success(0.68%) 96.14 %
Failure(56.99%) 6.05%
the bottom of Figure 3. APGD exhibited a higher ratio of
projection in the distance traveled than ACG. That is, in
the update of APGD after moving rf(x(k)), the projec-
tion returns it to the vicinity of the original point x(k 1).
Therefore, we can see that ACG moves more than APGD
owing to the introduction of the conjugate direction.
5. Search Diversity Analysis of ACG
The experimental results described in Section 4 show that
ACG exhibited a higher ASR than APGD. In addition,
the Euclidean distance between the successive two points
of ACG was larger than that of APGD (see Section 4.4),
which suggests that the search of ACG was more diverse.
In this section, we verify this hypothesis. While a search
is intensiÔ¨Åed when the distance between successive two
points is small (see the white points in Figure 1), the search
is not always diversiÔ¨Åed even if the successive two points
are distant (see bottom of Figure 4). We then regard the
search as intensiÔ¨Åed (diversiÔ¨Åed) when the search points
(do not) form clusters and propose an index that measures
the degree of diversiÔ¨Åcation by utilizing a global clustering
coefÔ¨Åcient.
5.1. DeÔ¨Ånition of Diversity Index
The global clustering coefÔ¨Åcient (Kemper, 2010) repre-
sents the strength of the connections between the nodes
of a graph, and is often used in complex network analysis
(Tabak et al., 2014; Said et al., 2018). To apply the global
020406080100iterations1234kz(k+1)¬∞z0(k+1)k2kx(k+1)¬∞x(k)k2020406080100012kx(k+1)¬∞x(k)k2ACGAPGD
iterationsACGAPGDFigure 3. Effects of the projection on the feasible region and mov-
ing distances of APGD and ACG. The upper part shows the 2-
norm of two successive points. The lower part shows the ratio
of the 2-norm of the two consecutive points to the 2-norm of the
projection onto the feasible region. Of note, z(k+1)=x(k+1)for
ACG.
clustering coefÔ¨Åcient to our analysis, we consider a graph
whose nodes are search points.
Given the set of search points X, we deÔ¨Åne a graph
GX() := (X;E ()), whereE() =f(v;w)2XXj
kv wk2g. LetC(G)be the global clustering co-
efÔ¨Åcient of a graph G, and leth(;X)be1 C(GX()).
Note that 0C(GX())1from the deÔ¨Ånition of the
global clustering coefÔ¨Åcient (see Section K.A). Of note,
whenGX()is disjoint union of some complete graphs,
C(GX()) = 1 and thush(;X) = 0 . Similarly,h(;X)
takes low value when GX()exhibits clusters. When the
cluster structure of the search point is apparent, the cluster
structure appears in GX()even for a small . Therefore,
we can quantify the cluster structure of the search points by
the transition of hwith the change of .
Hence, we deÔ¨Åne the Diversity Index (DI) as the average of
h(;X)forto quantify the diversity of the search points,
as given below.
DI(X;M ) :=1
MZM
0h(;X)d;
whereM= supfkx yk2jx;y2Sg is the size of the
feasible region. Because we consider an adversarial attackDiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
ùúÉùúÉ‚ÑéùúÉ;ùëã!‚ÑéùúÉ;ùëã"ùëã!ùëã"
ùëÄ
ùëÄ001001
Figure 4. Example of the relationship between the diversity of
search points and the global clustering coefÔ¨Åcients. The white
and black circles are the same as in Figure 1. The left side shows
the movements of the search points X. The right side shows the
transitions of h(;X), and three graphs for several thresholds .
with anl1-ball constraint, we obtain M=ku `k2. When
XandMare obvious, we simply denote DI(X;M )as DI.
From the deÔ¨Ånition of DI, we obtain 0DI1.
In Figure 4, examples of h(;X)are shown for two sets
of search points, Xa(upper) and Xb(lower). Because Xb
includes two clusters, whereas Xadoes not, we regard Xa
as an example of a diverse search and Xbas an intense
search. On the right side of Figure 4, it may be observed
thath(;Xa)h(;Xb)for most2[0;M]and thus
DI(Xa;M)>DI(Xb;M), which reÔ¨Çects the diversity of
the search points. In other words, GXb()was more clus-
tered thanGXa()for most.
As shown in Figure 4, the following relationship holds be-
tween DI and the diversity of the search points: when DI is
small (large), the points in X(do not) form some clusters.
That is, an intensive (diverse) search is conducted. Below,
we use DI to analyze and discuss APGD and ACG.
5.2. Comparison on the behavior of DI
We considered the difference between the conjugate gra-
dient and the gradient in the momentum direction us-
ing DI. In the following analyses, we use X(k):=
x(k 9);;x(k) 
as a set of search points at k-th iter-
ation wherex(i)is thei-th search point. The diversity of
the search and the transition of the search trends may be
observed from DI calculated for the latest 10 search points.
Figure 5 shows the transitions of DI in each algorithm for
the models whose results exhibited the most signiÔ¨Åcant dif-
ferences in the ASR for each dataset. From Figure 5, it
may be observed that the DIs during the search for ACG
were larger than those obtained in the search by APGD.
This suggests that ACG moves to a wide variety of differ-
ent points and conducts more diverse searches. In particu-
lar, when the step size is large, ACG extensively explores
the feasible region. By contrast, APGD exhibited a smallDI value from the early iterations during the search. Sec-
tion 4.4 shows that the movement distance of the search
point accompanied with an update of APGD was shorter
than that of ACG. In addition, the results in this section
show that the movement distance in the latest 10 search
points of APGD was limited in comparison to that of ACG.
These results demonstrate that ACG conducted a more di-
verse search than APGD, which is one possible reason for
the difference of CTC and the higher ASR of ACG. We
believe that utilizing DI to control the balance of diversi-
Ô¨Åcation and intensiÔ¨Åcation may be considered a promising
approach to further improve ASR.
5.3. Comparison of APGD and ACG on the ability to
diversiÔ¨Åcation and intensiÔ¨Åcation
In the previous section, we observed that ACG conducts a
more diverse search than APGD. In this section, we inves-
tigate how the diverse search for ACG contributes to the
recorded increase in ASR. We used the CIFAR-10 as the
dataset and a WideResNet-28-4 model trained using the
method proposed in (Ding et al., 2020), and CW loss as
the objective function. APGD utilizes the gradient direc-
tion (GD) as the update direction. ACG and APGD utilize
the same step-size selection strategy, which halves the step
size from 2"when either condition (I) or (II) is satisÔ¨Åed.
We use GD-to-CG ( ) and CG-to-GD ( ) to indicate that
the update direction was changed at a step size of from
GD to CG and CG to GD, respectively. For example, GD-
to-CG(=4) searches in GD when (k)"=2, and in CG
when(k)"=4. In Figure 6, we observe that DI ini-
tially increased when switching from GD to CG, but did
not increase when switching toward the end of the search.
In addition, we found that the DI decreased when switch-
ing from CG to GD, indicating that the diversiÔ¨Åcation de-
creased. By contrast, DI did not decrease in the latter half
of the search, indicating that diversiÔ¨Åcation did not signiÔ¨Å-
cantly decrease.
Table 6 shows the ASR results for 100 iterations and 5
restarts when switching from GD to CG or CG to GD at
each step size. It may be observed from the table entry of
CG-to-GD, we can see that the ASR when using CG in the
early stage was higher than that of ACG. In summary, the
early stage of ACG performs a more diversiÔ¨Åed search than
that of APGD, and the diversiÔ¨Åcation in the early phase of
ACG contributed to the higher ASR of ACG in comparison
to that of APGD.
6. Conclusion
In this study, we have proposed ACG using conjugate di-
rections inspired by the CG method. We have conducted
extensive experiments to evaluate the performance of theDiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
iterationsDiversity Indexiterations (Ding et al., 2019)1030507090APGD0.00.20.40.6ACG
iterationsiterations (Hendrycset al., 2019)10305070900.00.20.40.6Diversity IndexACGAPGD
iterationsiterations (Salman et al., 2020)10305070900.00.20.40.6Diversity IndexACGAPGD
Figure 5. Transition of the average DI over 10,000 images using three models.
10 20 30 40 50 60 70 80 90100
iterations0.00.20.40.6Diversity IndexGD-to-CG ()
GD-to-CG (/4)
GD-to-CG (/64)
CG-to-GD ()
CG-to-GD (/4)
CG-to-GD (/64)
APGD
Figure 6. The transitions of DI of the switching update direction from GD and CG averaged over 10,000 images.
Table 6. ASR when switching from GD to CG (GD-to-CG) or CG to GD (CG-to-GD) at the speciÔ¨Åc step size.
step size " "= 2"=4"=8"=16"=32"=64 APGD ACG
GD-to-CG 50.85 49.70 49.66 49.66 49.67 49.67 49.6749.67 55.77CG-to-GD 55.77 55.84 55.80 55.80 55.78 55.77 55.77
proposed approach on a total of 64 models listed in Ro-
bustBench. We found that ACG signiÔ¨Åcantly improved the
ASR compared to APGD in all but one of the models. In
particular, ACG with one restart outperformed the ASR of
APGD with 5 restarts for many of the models. This indi-
cates that ACG may be promising in that it achieves a high
ASR with only deterministic operations. In addition, ACG
frequently switched the attacked class during the search
and succeeded in attacking owing to its attacked class being
different from APGD, whereas APGD rarely switched the
attacked class. This result implies that varying the attacked
class contributed signiÔ¨Åcantly to the improvement in terms
of ASR exhibited by ACG.
To analyze the difference in the search performance be-
tween ACG and APGD, we have proposed the Diversity
Index (DI), which measures the degree of diversiÔ¨Åcation of
a search. DI is calculated based on the global clustering
coefÔ¨Åcients on a graph the nodes of which are the latest
among several search points. A higher DI indicates that
corresponding search points are sparsely distributed. Ac-
cording to the analyses using DI, one of the reasons forthe higher ASR of ACG is that the CG direction exhibited
greater diversiÔ¨Åcation, particularly in the early phase of the
search. DI may also be considered a valuable metric in ana-
lyzing other attack algorithms. In the future, we expect that
an algorithm designed to efÔ¨Åciently control diversiÔ¨Åcation
and intensiÔ¨Åcation using DI would be effective.
Acknowledgement
This research project was supported by the Japan Science
and Technology Agency (JST), the Core Research of Evo-
lutionary Science and Technology (CREST), the Center of
Innovation Science and Technology based Radical Innova-
tion and Entrepreneurship Program (COI Program), JSPS
KAKENHI Grant Number JP20H04.DiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
References
Addepalli, S., Jain, S., Sriramanan, G., Khare, S., and
Babu, R. V . Towards achieving adversarial robustness
beyond perceptual limits. ICML 2021 Workshop , 2021.
Andriushchenko, M., Croce, F., Flammarion, N., and Hein,
M. Square attack: a query-efÔ¨Åcient black-box adversar-
ial attack via random search. In European Conference
on Computer Vision , pp. 484‚Äì501. Springer, 2020.
Carlini, N. and Wagner, D. Towards evaluating the robust-
ness of neural networks. In 2017 ieee symposium on se-
curity and privacy (sp) , pp. 39‚Äì57, 2017.
Carmon, Y ., Raghunathan, A., Schmidt, L., Liang, P., and
Duchi, J. C. Unlabeled Data Improves Adversarial Ro-
bustness. Advances in Neural Information Processing
Systems , 32, may 2019. ISSN 10495258.
Chen, E.-C. and Lee, C.-R. Ltd: Low temperature
distillation for robust adversarial training. CoRR ,
abs/2111.02331, 11 2021.
Chen, J., Cheng, Y ., Gan, Z., Gu, Q., and Liu, J. EfÔ¨Åcient
robust training via backward smoothing, 2021.
Chen, T., Liu, S., Chang, S., Cheng, Y ., Amini, L.,
and Wang, Z. Adversarial Robustness: From Self-
Supervised Pre-Training to Fine-Tuning. Proceedings
of the IEEE Computer Society Conference on Com-
puter Vision and Pattern Recognition , pp. 696‚Äì705, mar
2020. ISSN 10636919. doi: 10.1109/CVPR42600.2020.
00078.
Cheng, S., Shi, Y ., Qin, Q., Zhang, Q., and Bai, R. Popula-
tion Diversity Maintenance In Brain Storm Optimization
Algorithm. Journal of ArtiÔ¨Åcial Intelligence and Soft
Computing Research , 4(2):83‚Äì97, 2014. ISSN 2083-
2567. doi: 10.1515/jaiscr-2015-0001.
ÀáCrepin≈°ekMatej, LiuShih-Hsi, and MernikMarjan. Explo-
ration and exploitation in evolutionary algorithms. ACM
Computing Surveys (CSUR) , 45(3):33, jul 2013. doi:
10.1145/2480741.2480752.
Croce, F. and Hein, M. Minimally distorted adversarial ex-
amples with a fast adaptive boundary attack. In Interna-
tional Conference on Machine Learning , pp. 2196‚Äì2205.
PMLR, 2020a.
Croce, F. and Hein, M. Reliable evaluation of adversarial
robustness with an ensemble of diverse parameter-free
attacks. In International Conference on Machine Learn-
ing, pp. 2206‚Äì2216. PMLR, 2020b.
Croce, F., Andriushchenko, M., Sehwag, V ., Debenedetti,
E., Flammarion, N., Chiang, M., Mittal, P., and Hein,M. Robustbench: a standardized adversarial robustness
benchmark. In Thirty-Ô¨Åfth Conference on Neural In-
formation Processing Systems Datasets and Benchmarks
Track , 2021.
Cui, J., Liu, S., Wang, L., and Jia, J. Learnable bound-
ary guided adversarial training. In Proceedings of the
IEEE/CVF International Conference on Computer Vi-
sion (ICCV) , pp. 15721‚Äì15730, October 2021.
Ding, G. W., Sharma, Y ., Lui, K. Y . C., and Huang, R.
MMA training: Direct input space margin maximization
through adversarial training. In 8th International Con-
ference on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net,
2020.
Dong, Y ., Liao, F., Pang, T., Su, H., Zhu, J., Hu, X., and
Li, J. Boosting adversarial attacks with momentum. In
Proceedings of the IEEE conference on computer vision
and pattern recognition , pp. 9185‚Äì9193, 2018.
Engstrom, L., Ilyas, A., Salman, H., Santurkar, S., and
Tsipras, D. Robustness (python library), 2019.
Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain-
ing and harnessing adversarial examples. In 3rd Inter-
national Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference
Track Proceedings , 2015.
Gowal, S., Uesato, J., Qin, C., Huang, P., Mann, T. A.,
and Kohli, P. An alternative surrogate loss for pgd-based
adversarial testing. CoRR , abs/1910.09338, 2019. URL
http://arxiv.org/abs/1910.09338 .
Gowal, S., Qin, C., Uesato, J., Mann, T., and Kohli,
P. Uncovering the Limits of Adversarial Training
against Norm-Bounded Adversarial Examples. CoRR ,
abs/2010.03593, oct 2020.
Gowal, S., RebufÔ¨Å, S.-A., Wiles, O., Stimberg, F., Calian,
D. A., and Mann, T. Improving robustness using gener-
ated data. In NeurIPS , 2021.
Hager, W. W. W. and Zhang, H. A Survey of Nonlinear
Conjugate Gradient Methods. PaciÔ¨Åc journal of Opti-
mization , 2(1):35‚Äì58, 2006.
Hendrycks, D., Lee, K., and Mazeika, M. Using pre-
training can improve model robustness and uncertainty.
Proceedings of the International Conference on Machine
Learning , pp. 2712‚Äì2721, 5 2019. ISSN 2640-3498.
Hestenes, M. and Stiefel, E. Methods of conjugate gra-
dients for solving linear systems. Journal of Research
of the National Bureau of Standards , 49(6):409, 1952.
ISSN 0091-0635. doi: 10.6028/jres.049.044.DiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
Huang, H., Wang, Y ., Erfani, S. M., Gu, Q., Bailey, J., and
Ma, X. Exploring architectural ingredients of adversari-
ally robust deep neural networks. In NeurIPS , 2021.
Huang, L., Zhang, C., and Zhang, H. Self-Adaptive
Training: beyond Empirical Risk Minimization. Ad-
vances in Neural Information Processing Systems , 2020-
December, feb 2020. ISSN 10495258.
Kemper, A. Valuation of Network Effects in Software Mar-
kets. 2010. doi: 10.1007/978-3-7908-2367-7.
Kurakin, A., Goodfellow, I., and Bengio, S. Adversarial
examples in the physical world. ICLR Workshop , 2017.
Lin, J., Song, C., He, K., Wang, L., and Hopcroft, J. E. Nes-
terov accelerated gradient and scale invariance for adver-
sarial attacks. In International Conference on Learning
Representations , 2020.
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and
Vladu, A. Towards deep learning models resistant to ad-
versarial attacks. In International Conference on Learn-
ing Representations , 2018.
Morales-Casta√±eda, B., Zald√≠var, D., Cuevas, E., Fausto,
F., and Rodr√≠guez, A. A better balance in metaheuris-
tic algorithms: Does it exist? Swarm and Evolutionary
Computation , 54:100671, may 2020. ISSN 22106502.
doi: 10.1016/j.swevo.2020.100671.
Pang, T., Yang, X., Dong, Y ., Xu, K., Zhu, J., and Su,
H. Boosting Adversarial Training with Hypersphere Em-
bedding. Advances in Neural Information Processing
Systems , 2020-December, feb 2020. ISSN 10495258.
Rade, R. and Moosavi-Dezfooli, S.-M. Helper-based ad-
versarial training: Reducing excessive margin to achieve
a better accuracy vs. robustness trade-off. In ICML 2021
Workshop on Adversarial Machine Learning , 2021.
RebufÔ¨Å, S.-A., Gowal, S., Calian, D. A., Stimberg, F.,
Wiles, O., and Mann, T. A. Fixing data augmentation to
improve adversarial robustness. CoRR , abs/2103.01946,
2021.
Rice, L., Wong, E., and Kolter, Z. OverÔ¨Åtting in adver-
sarially robust deep learning. In III, H. D. and Singh,
A. (eds.), Proceedings of the 37th International Confer-
ence on Machine Learning , volume 119 of Proceedings
of Machine Learning Research , pp. 8093‚Äì8104. PMLR,
13‚Äì18 Jul 2020.
Said, A., Abbasi, R. A., Maqbool, O., Daud, A., and Aljo-
hani, N. R. CC-GA: A clustering coefÔ¨Åcient based ge-
netic algorithm for detecting communities in social net-
works. Applied Soft Computing , 63:59‚Äì70, feb 2018.
ISSN 1568-4946. doi: 10.1016/J.ASOC.2017.11.014.Salman, H., Ilyas, A., Engstrom, L., Kapoor, A., and
Madry, A. Do adversarially robust imagenet models
transfer better? CoRR , abs/2007.08489, 2020.
Sehwag, V ., Wang, S., Mittal, P., and Jana, S. HYDRA:
Pruning Adversarially Robust Neural Networks. Ad-
vances in Neural Information Processing Systems , 2020-
December, feb 2020. ISSN 10495258.
Sehwag, V ., Mahloujifar, S., Handina, T., Dai, S., Xiang,
C., Chiang, M., and Mittal, P. Robust learning meets
generative models: Can proxy distributions improve ad-
versarial robustness?, 2021.
Sitawarin, C., Chakraborty, S., and Wagner, D. Sat: Im-
proving adversarial training via curriculum-based loss
smoothing. In Proceedings of the 14th ACM Work-
shop on ArtiÔ¨Åcial Intelligence and Security , AISec ‚Äô21,
pp. 25‚Äì36, New York, NY , USA, 2021. Association for
Computing Machinery. ISBN 9781450386579. doi:
10.1145/3474369.3486878.
Sridhar, K., Sokolsky, O., Lee, I., and Weimer, J. Improv-
ing Neural Network Robustness via Persistency of Exci-
tation. jun 2021.
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,
D., Goodfellow, I., and Fergus, R. Intriguing proper-
ties of neural networks. In 2nd International Confer-
ence on Learning Representations, ICLR 2014, Banff,
AB, Canada, April 14-16, 2014, Conference Track Pro-
ceedings , 2014.
Tabak, B. M., Takami, M., Rocha, J. M., Cajueiro, D. O.,
and Souza, S. R. Directed clustering coefÔ¨Åcient as
a measure of systemic risk in complex banking net-
works. Physica A: Statistical Mechanics and its Appli-
cations , 394:211‚Äì216, jan 2014. ISSN 0378-4371. doi:
10.1016/J.PHYSA.2013.09.010.
Wang, Y ., Zou, D., Yi, J., Bailey, J., Ma, X., and Gu,
Q. Improving adversarial robustness requires revisiting
misclassiÔ¨Åed examples. In International Conference on
Learning Representations , 2020.
Wong, E., Rice, L., and Kolter, J. Z. Fast is better than free:
Revisiting adversarial training. In 8th International Con-
ference on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net,
2020.
Wu, D., Xia, S. T., and Wang, Y . Adversarial
Weight Perturbation Helps Robust Generalization. Ad-
vances in Neural Information Processing Systems , 2020-
December, apr 2020. ISSN 10495258.DiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
Xie, C., Zhang, Z., Zhou, Y ., Bai, S., Wang, J., Ren, Z.,
and Yuille, A. L. Improving transferability of adversar-
ial examples with input diversity. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , June 2019.
Yao, Z., Gholami, A., Xu, P., Keutzer, K., and Mahoney,
M. W. Trust region based adversarial attack on neural
networks. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) ,
June 2019.
Zhang, D., Zhang, T., Lu, Y ., Zhu, Z., and Dong, B. You
Only Propagate Once: Accelerating Adversarial Train-
ing via Maximal Principle. Advances in Neural In-
formation Processing Systems , 32, may 2019a. ISSN
10495258.
Zhang, H., Yu, Y ., Jiao, J., Xing, E. P., Ghaoui, L. E.,
and Jordan, M. I. Theoretically Principled Trade-off
between Robustness and Accuracy. 36th International
Conference on Machine Learning, ICML 2019 , 2019-
June:12907‚Äì12929, jan 2019b.
Zhang, J., Xu, X., Han, B., Niu, G., Cui, L., Sugiyama, M.,
and Kankanhalli, M. Attacks which do not kill train-
ing make adversarial learning stronger. In III, H. D.
and Singh, A. (eds.), Proceedings of the 37th Interna-
tional Conference on Machine Learning , volume 119 of
Proceedings of Machine Learning Research , pp. 11278‚Äì
11287. PMLR, 13‚Äì18 Jul 2020.
Zhang, J., Zhu, J., Niu, G., Han, B., Sugiyama, M., and
Kankanhalli, M. Geometry-aware instance-reweighted
adversarial training. In International Conference on
Learning Representations , 2021.
Zheng, T., Chen, C., and Ren, K. Distributionally adversar-
ial attack. Proceedings of the AAAI Conference on Ar-
tiÔ¨Åcial Intelligence , 33(01):2253‚Äì2260, Jul. 2019. doi:
10.1609/aaai.v33i01.33012253.DiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
Appendix
In this appendix, we provide the following additional infor-
mation regarding the experimental results and the behavior
of the proposed approach during the search for ACG.
‚Ä¢ An example of desirable search behavior. (Ap-
pendix B)
‚Ä¢ Transitions of the best objective values and the effect
of increasing the iterations. (Appendix C)
‚Ä¢ The effect of random restarts on the performance of
ACG. (Appendix D)
‚Ä¢ A discussion of the diversiÔ¨Åcation and intensiÔ¨Åcation
of APGD search. (Appendix E)
‚Ä¢ The effect of the objective function on the perfor-
mance of ACG. (Appendix F)
‚Ä¢ The effect of the operation conducted to render non-
negative, which is known to perform better for a gen-
eral nonlinear optimization. (Appendix G)
‚Ä¢ SpeciÔ¨Åcations of the computational environments
used in our experiments. (Appendix H)
‚Ä¢ Differences in the ASR among different formulas used
to calculate. (Appendix I)
‚Ä¢ Differences in the search behavior of APGD and ACG
for the model in which the ASR of ACG was lower
than that of APGD and the other models. (Appendix J)
‚Ä¢ A deÔ¨Ånition of DI on arbitrary bounded distance
spaces. (Appendix K)
‚Ä¢ The relationship between distributions of the point
clouds and DI. (Appendix L)
A. Related works
We classify some previous works to clarify the position of
the proposed method. Figure 7 shows an overview.
"Budget" is a type of adversarial attacks using the formula-
tion described in Section 2. (Gowal et al., 2019; Xie et al.,
2019; Zheng et al., 2019) are examples of this type of at-
tacks.
"Minimum distortion" is another type of adversarial attack
which generates minimally distorted adversarial examples
by minimizing the norm of adversarial perturbation. For-
mally, this problem is described as follows.
min
x2Dd(xorig;x)s.t.arg max
k=1;:::Kgk(x)6=c
Figure 7. ClassiÔ¨Åcation of previous works.
. (Yao et al., 2019; Croce & Hein, 2020a) are examples of
this type of attacks.
In the case of white-box attacks, we can assume that all
information about the model to be attacked is known the
attacker, including its weights and gradients.
In the case of black-box attacks, only use the output of tar-
get models can be used. The proposed method, ACG, is a
white-box attack that uses a budget formulation.
B. Examples of DiversiÔ¨Åcation and
IntensiÔ¨Åcation
In this section, we detail the balance between diversiÔ¨Åca-
tion and intensiÔ¨Åcation that we aim to realize.
Most white-box adversarial attacks are formulated as opti-
mization problems where the objective function is nonlin-
ear, nonconvex, and multimodal. We observe the diversiÔ¨Å-
cation and intensiÔ¨Åcation of the search to Ô¨Ånd the optimal
solution by using the following multimodal function.
f(x;y) = 10 exp 
 0:2r
1
2(x4+y2)!
+ exp1
2(cos(2x) + cos(2y))
:(12)
Figure 8 shows examples of an intensiÔ¨Åed search, a diversi-
Ô¨Åed search, and an appropriate search exhibiting the proper
balance of diversiÔ¨Åcation and intensiÔ¨Åcation. Six local so-
lutions may be observed from the contour lines of the func-
tion in the Ô¨Ågure, of which the local solution at position "E"
is the global optimum.DiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
ùê¥ùêµùê∂ùê∑ùê∏ùêπ
Figure 8. Examples of searches for multimodal functions. The initial search point is shown by the white star, and the search ended at
a white square. The circles represent the search points. The black circle indicates that the search was diversiÔ¨Åed, and the white circle
indicates an intense search. "A" to "F" on the left Ô¨Ågure refers to the local optima, and "E" shows the global optima. The examples on the
left and center are those of failed searches that were not able to Ô¨Ånd the global optima owing to a lack of diversiÔ¨Åcation or intensiÔ¨Åcation,
respectively. The example on the right shows a search that has successfully found the global optima owing to an appropriate balance of
diversiÔ¨Åcation and intensiÔ¨Åcation, which is our aim in the present work.
In the Ô¨Ågure on the left, the search (white circles) was
intensiÔ¨Åed in the local solution near the initial point. In
the middle Ô¨Ågure, the search (black circles) was diversiÔ¨Åed
even when reaching the neighborhood of the local solution.
Therefore, it appears to be impossible to reach an optimal
solution if the search is excessively intensiÔ¨Åed or diversi-
Ô¨Åed. In the Ô¨Ågure on the right, it may be observed that the
search changed from a focus on diversiÔ¨Åcation to a focus
on intensiÔ¨Åcation and that the search reached the neighbor
of the global optima through a diversiÔ¨Åed search (black cir-
cles) and converged to the optimal solution through an in-
tensive search (white circles). Thus, for multimodal func-
tions, a balance between diversiÔ¨Åcation and intensiÔ¨Åcation
of the search is necessary when searching an entire feasible
region to Ô¨Ånding an optimum. The right side of Figure 6
shows our intended search, in which the balance between
diversiÔ¨Åcation and intensiÔ¨Åcation is properly controlled.
C. ACG vs. APGD: Comparison of the best
objective values
From Figure 9, we predicted that ACG (100 iterations)
could still improve the objective value because it improves
the best objective value to a relatively large degree even at
the end of the search (dashed line, orange). We then in-
vestigated the search performance of ACG in terms of the
best objective values. Figure 9 shows the transitions of the
best loss of ACG after 100 iterations, and those of APGD
and ACG after 1000 iterations. The x-axis represents the
percentage of the iterations to Niter. The best loss at it-
erationkis deÔ¨Åned as best loss(k)= max
ikL
g(x(i));c
.
As expected, ACG (1000 iterations) exhibited higher objec-
tive values than ACG (100 iterations), whereas ACG (1000
iterations) improved on the best objective values even at
APGD (1000 iterations)ACG (100 iterations)020406080100123average best lossprogress (%)ACG (1000 iterations)Figure 9. Transitions of the best objective values averaged over
10,000 images. Here, "progress" means the percentage of the it-
erations to the total iterations.
the end of the search. By contrast, the improvement of the
best objective values of APGD (1000 iterations) was small
at the end of the attack compared to ACG (100 iterations,
1000 iterations). To summarize these results, ACG may
be considered a more promising algorithm than APGD be-
cause, in contrast to APGD, it can signiÔ¨Åcantly improve on
its best loss even at the end of the search.
D. Effects of random restarts for the
performance of ACG
In this section, we compare the results of APGD with 5
restarts for 100 iterations, ACG with 5 restarts for 100 it-
erations, and ACG with 1 restart for 500 iterations. Ta-
bles 9 to 11 show that ACG(5), which is randomized by the
restarts, and ACG-500iter, which is deterministic, achieved
almost the same ASR. This result suggests that the searchDiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
diversity of ACG does not depend on the random sampling
of the initial points, but that the update direction itself has
the property of a diverse search compared to APGD.
E. DiversiÔ¨Åcation and IntensiÔ¨Åcation of
APGD Search
Herein, we discuss the diversiÔ¨Åcation and intensiÔ¨Åcation
of APGD. APGD is a PGD-based adversarial attack that
achieves a higher ASR than previous SOTA methods by
gradually reducing the step size, as shown in Section 3.2.
APGD introduced this step size reduction to gradually
switch from exploration to exploitation. Note that ‚Äúexplo-
ration‚Äù and ‚Äúexploitation‚Äù are synonymous with ‚ÄúdiversiÔ¨Å-
cation‚Äù and ‚ÄúintensiÔ¨Åcation‚Äù as we used the terms in this
work. However, to the best of out knowledge, whether
APGD switches from diversiÔ¨Åcation to intensiÔ¨Åcation has
not been veriÔ¨Åed.
Figure 5 in Section 5.2 show that the DIs of APGD gradu-
ally decreased during the search for 100 iterations. These
results indicate that APGD can switch from diversiÔ¨Åcation
to intensiÔ¨Åcation in terms of DI, and thus the motivation
for the step size selection was achieved. However, the DI
value of APGD was smaller than that of ACG. We discuss
the reason for this based on the results described in Sec-
tion 4.4. From the image at the top of Figure 3, it may
be observed that the projected distance of the PGD-based
updated search point to the feasible region was larger than
that of the CG-based search point. This means that with
APGD, the search points are close to the boundary of the
region and are updated toward the outside of the boundary.
Because the PGD-based method tends to update toward the
same local optimum, doing so is natural when there is a
local optimum outside the feasible region. As a result, a
small distance was induced between two successive search
points (as shown at the bottom of Figure 3), dense search
points, and small DI values.
Because ACG, which exhibited a more variable DI, delivers
a better performance than APGD, we expect that higher-
performing adversarial attacks will be developed in future
research via sophisticated control of the DI transition.
F. DLR loss
In this section, we compare the ASR under the experimen-
tal setup of Section 4; however, we used the DLR loss pro-
posed in (Croce & Hein, 2020b) as the objective function
instead of the CW loss in Section 4. Tables 12 and 13 show
that the ASR of ACG was higher than that of APGD for all
64 models when the objective function was the DLR loss.Table 7. The average percentage of the iterations in which +
HS=0
among all 495 iterations. The CIFAR-10 dataset was used for this
experiment, and the diameter of the feasible region "was8=255.
paper Ratio of+
HSis0
(Ding et al., 2020) 40.59%
(Carmon et al., 2019) 30.41%
(RebufÔ¨Å et al., 2021) 38.57%
G. Analysis of effect of the assumption of
HS0
When discussing the convergence of the conjugate gradi-
ent method, it has occasionally been assumed that HS0
(Hager & Zhang, 2006). In addition, some studies have
suggested that it is better to assume HS0is prefer-
able in practice. In this section, we investigate the effect
of assuming HS0by comparing the behavior of ACG
without any assumptions on HS. To render HSbe non-
negative, we can obtain the following.
+
HS:= max(HS;0): (13)
We call ACG using +
HSas ACG+, as determined using
(13). Table 8 show that the ASR of ACG+was equal to
or less than that of APGD, suggesting a decrease in the
search performance of ACG+. In addition, the percent-
age of iterations where +
HS= 0 was 30% to 40% of all
495 iterations, excluding iteration 0, where the steepest de-
scent occured(see Table 7). That is, ACG+updated in the
same way as APGD without a momentum update method
once every three iterations. In addition, from Figure 10,
it may be observed that ACG+diversiÔ¨Åed the search less
than ACG. These results show that operations that make
HSnonnegative, such as (13), are unsuitable for this prob-
lem. Applying the operation to ACG limits the diversiÔ¨Åca-
tion performance, which is one of the strengths of ACG.
H. Experiment Environments
The computational environments for our experiments, such
as the CPU and GPU speciÔ¨Åcations and RAM capacity, are
provided in Table 15. More information is also provided in
the source codes.
I. Experimental results of the evaluation of
the representative formulas
In this section, among the formulas used to determine
proposed in prior works, we veriÔ¨Åed the effectiveness
of seven representative formulas are veriÔ¨Åed through sec-
ondary experiments. Based on these experiments, we chose
the formula for our approach. The formulas we veriÔ¨Åed inDiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
(Ding et al., 2020) (RebufÔ¨Å et al., 2021) (Carmon et al., 2019)Diversity Index
0.00.10.20.30.40.50.60.70.8
10iterations2030405060708090100
0.00.10.20.30.40.50.60.70.8
10iterations2030405060708090100
0.00.10.20.30.40.50.60.70.8
10iterations2030405060708090100ACG+ACGAPGD
Figure 10. Comparison of DI of ACG+, ACG, and APGD. The CIFAR-10 dataset was used for this comparison.
Table 8. ASR of ACG+and APGD: The dataset is CIFAR-10, and "=8
255.
CIFAR-10 ("= 8=255) Attack Success Rate
paper Architecture APGD(1) ACG+(1) APGD(5) ACG+(5) diff
(RebufÔ¨Å et al., 2021) PreActResNet-18 42.73 42.71 42.91 42.88 -0.03
(Carmon et al., 2019) WideResNet-28-10 39.38 39.28 39.59 39.51 -0.08
(Ding et al., 2020) WideResNet-28-4 48.73 49.66 49.67 50.68 1.01
this experiment are given as follows.
‚Ä¢ FR:(k)
FR=krf(x(k))k2
2
kf(x(k 1))k2
2.
‚Ä¢ PR:(k)
PR=hrf(x(k));y(k 1)i
kf(x(k 1))k2
2.
‚Ä¢ HS:(k)
HS= hrf(x(k));y(k 1)i
hs(k 1);y(k 1)i.
‚Ä¢ DY:(k)
DY=krf(x(k))k2
2
hs(k 1);y(k 1)i.
‚Ä¢ HZ:(k)
HZ=
y(k 1) 2s(k 1)ky(k 1)k2
2
hs(k 1);y(k 1)i;rf(x(k))
hs(k 1);y(k 1)i.
‚Ä¢ DL:(k)
DL= 
y(k 1) ts(k 1);rf(x(k))
hs(k 1);y(k 1)i; t0.
‚Ä¢ LS:(k)
LS= hrf(x(k));y(k 1)i
s(k 1);rf(x(k 1)).
We conducted small experiments on only models described
in (Ding et al., 2020; Carmon et al., 2019; RebufÔ¨Å et al.,
2021) using the same experimental setup described in Sec-
tion 4. The CIFAR-10 dataset was used, and the results are
presented in Table 14. Table 14 shows that the ASR of the
formulas whose enumerator is hrf(x);yi(PR, HS, LS)were higher than those of the formulas whose numerator is
krf(x)k2
2(FR, DY), and that the ASR of the HS formula
was the highest.
J. Analysis of WideResNet-34-10 (Addepalli
et al., 2021)
We analyzed the search behavior of ACG on a
WideResNet-34-10 model trained by the method proposed
in (Addepalli et al., 2021) using CIFAR-100, the only
model in which the ASR of ACG was lower than that of
APGD in the experiment using the CW loss (see Table 2).
From Figure 11, it may be observed that the DI of ACG of
this model was lower than that of ACG on the other models.
This means that the search for ACG in this model tended to
be more intensiÔ¨Åed than the attacks on the other two mod-
els (Sitawarin et al., 2021; Wu et al., 2020). By contrast, the
DI of APGD on this model was higher than that of APGD
in the other models, indicating that it tended to be more di-
versiÔ¨Åed. These results suggest that the reason why APGD
was superior to ACG for this model was because intensi-
Ô¨Åcation was required to achieve a higher ASR. However,
the models requiring intensiÔ¨Åcation for more effective at-
tacks were rare because ACG exhibited a higher ASR than
APGD for most of the models in Tables 2 to 3.DiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
0.00.10.20.30.4Diversity Index0.50.60.70.8
10iterations2030405060708090100Addepalli2021Towards\_WRN34-APGDAddepalli2021Towards\_WRN34-ACGSitawarin2020Improving-APGDSitawarin2020Improving-ACGWu2020Adversarial-APGDWu2020Adversarial-ACG
Figure 11. DI of ACG and APGD on three models (Addepalli
et al., 2021; Wu et al., 2020; Sitawarin et al., 2021)
K. Generalization of DI
In this section, we generalize the DI deÔ¨Åned in Section 5.1
to arbitrary bounded distance spaces. First, we describe
the deÔ¨Ånition of local and global clustering coefÔ¨Åcients for
undirected graphs and then use the global clustering co-
efÔ¨Åcient to deÔ¨Åne the DI for arbitrarily bounded distance
spaces. Then, using the global clustering coefÔ¨Åcient, we
deÔ¨Åne the DI for arbitrary bounded distance spaces.
K.A. DeÔ¨Ånition of the local clustering coefÔ¨Åcient
(Kemper, 2010)
LetG= (V;E)be an undirected graph. Let the set of
neighbor nodes of node vi2VbeNi:=fvj2Vjeij2
Eg. Then the local clustering coefÔ¨Åcient Cvi(G)at node
viof graphGis deÔ¨Åned as follows.
Cvi(G) :=2#fejk2Ejeij2E^eik2Eg
#Ni(#Ni 1)(14)
According to this deÔ¨Ånition, Cvi(G)2[0;1].
K.B. DeÔ¨Ånition of the global clustering coefÔ¨Åcient
(Kemper, 2010)
LetG= (V;E)be an undirected graph, and the global
clustering coefÔ¨Åcient C(G)of the graph Gbe deÔ¨Åned using
the equation (14) as follows.
C(G) :=1
#VX
v2VCv(G) (15)
In the same manner as Cvi(G),C(G)also satisÔ¨Åes 0
C(G)1.K.C. DeÔ¨Ånition of DI on the general bounded metric
spaces
Let(U;d)be a bounded distance space, VUbe its Ô¨Ånite
subset, and 2Rbe the graph of GV() := (V;E()),
whereE() =f(v;w)2VVjd(v;w)g. In this
case, asM= supv;w2Ud(v;w), DI is deÔ¨Åned as follows.
DI(V;M) := 1 1
MZM
0C(GV())d (16)
L. Examples of DI
Figure 12 shows an example of the calculation of DI for
point clouds with different distributions. The Ô¨Årst row of
Figure 12 shows an example of a point cloud and its DI in
which the point clouds form a single cluster. The second
row shows an example of a point cloud and its DI , in
which the point clouds form approximately three to four
clusters. The third row shows an example of a point cloud
and its DI where most points are distributed on the bound-
ary. The Ô¨Ånal example is diverse in that it shows a diver-
sity search performed on the boundary. These examples
show that DI takes a small value when the point cloud is
dense or when the clusters are formed, and DI takes a rel-
atively large value when there are no clusters the elements
of which number greater than 2.
M. Evaluating the performance of ACG
combined with Auto Attack
We did not compare the ASR of Auto Attack(AA) and
ACG directly because we focused on generating many ad-
versarial examples quickly. AA comprises of four different
algorithms for adversarial attacks, and it takes much longer
than ACG. We considered that evaluating ACG as a com-
ponent of AA would be more suitable; we therefore con-
structed AA(ACG-CE) using ACG with untargeted cross-
entropy loss instead of APGD with the same loss. From
the numerical results shown in Table 16 below, it may be
observed that ACG is also a useful as a component of AA.
We expect that future work along there lines will adopt an
appropriate combination of existing algorithms, including
ACG.DiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
Table 9. The ASR of APGD with 5 restarts, ACG with 5 restarts, and ACG with 1 restart for 500 iterations. The CIFAR-10 dataset was
used with"=8
255. The highest ASR is in bold, and the second is underlined. APGD( N) refers to APGD with N-times the initial point
selection. The meanings of the other columns are the same. diffis the difference between APGD(5) and maxfACG(5), ACG-500iter g.
CIFAR-10 ("= 8=255) Attack Success Rate
paper Architecture APGD(5) ACG(5) ACG-500iter diff
(Rade & Moosavi-Dezfooli, 2021) PreActResNet-18 42.46 42.65 42.67 0.21
(Rade & Moosavi-Dezfooli, 2021) PreActResNet-18 41.65 42.12 42.07 0.47
(RebufÔ¨Å et al., 2021) PreActResNet-18 42.91 43.15 43.17 0.26
(Andriushchenko et al., 2020) PreActResNet-18 53.82 54.90 54.81 1.08
(Sehwag et al., 2021) ResNet-18 43.91 44.79 44.53 0.88
(Chen et al., 2020) ResNet-50 48.08 48.28 48.29 0.21
(Wong et al., 2020) ResNet-50 54.26 55.44 55.34 1.18
(Engstrom et al., 2019) ResNet-50 48.08 49.25 49.08 1.17
(RebufÔ¨Å et al., 2021) WideResNet-106-16 34.71 35.03 34.98 0.32
(Carmon et al., 2019) WideResNet-28-10 39.59 40.03 39.98 0.44
(Gowal et al., 2020) WideResNet-28-10 36.45 36.90 36.96 0.51
(Hendrycks et al., 2019) WideResNet-28-10 43.82 44.36 44.37 0.55
(Rade & Moosavi-Dezfooli, 2021) WideResNet-28-10 38.64 38.87 38.79 0.23
(RebufÔ¨Å et al., 2021) WideResNet-28-10 38.47 38.80 38.77 0.33
(Sehwag et al., 2020) WideResNet-28-10 41.93 42.41 42.48 0.55
(Sridhar et al., 2021) WideResNet-28-10 39.45 39.85 39.88 0.43
(Wang et al., 2020) WideResNet-28-10 42.15 42.57 42.53 0.42
(Wu et al., 2020) WideResNet-28-10 39.56 39.70 39.72 0.16
(Zhang et al., 2021) WideResNet-28-10 39.98 40.25 40.24 0.27
(Ding et al., 2020) WideResNet-28-4 49.67 55.77 55.32 6.10
(Cui et al., 2021) WideResNet-34-10 46.41 46.90 46.90 0.49
(Huang et al., 2020) WideResNet-34-10 46.19 46.72 46.67 0.53
(Rade & Moosavi-Dezfooli, 2021) WideResNet-34-10 36.46 36.83 36.77 0.37
(Sehwag et al., 2021) WideResNet-34-10 39.58 40.18 40.18 0.60
(Sitawarin et al., 2021) WideResNet-34-10 47.23 48.02 48.05 0.82
(Wu et al., 2020) WideResNet-34-10 43.36 43.60 43.62 0.26
(Zhang et al., 2019a) WideResNet-34-10 53.08 54.15 54.17 1.09
(Zhang et al., 2019b) WideResNet-34-10 46.65 47.18 47.19 0.54
(Zhang et al., 2020) WideResNet-34-10 45.68 46.12 46.13 0.45
(Chen et al., 2021) WideResNet-34-10 47.58 48.00 47.99 0.42
(Sridhar et al., 2021) WideResNet-34-15 38.90 39.15 39.22 0.32
(Cui et al., 2021) WideResNet-34-20 45.88 46.23 46.14 0.35
(Gowal et al., 2020) WideResNet-34-20 42.65 42.86 42.91 0.26
(Pang et al., 2020) WideResNet-34-20 44.75 45.33 45.34 0.59
(Rice et al., 2020) WideResNet-34-20 44.92 45.69 45.73 0.81
(Huang et al., 2021) WideResNet-34-R 37.33 37.79 37.90 0.57
(Huang et al., 2021) WideResNet-34-R 36.27 36.76 36.81 0.54
(Gowal et al., 2020) WideResNet-70-16 33.42 33.70 33.82 0.40
(Gowal et al., 2020) WideResNet-70-16 42.12 42.45 42.40 0.33
(Gowal et al., 2021) WideResNet-70-16 32.57 33.04 32.95 0.47
(RebufÔ¨Å et al., 2021) WideResNet-70-16 35.04 35.27 35.19 0.23
(RebufÔ¨Å et al., 2021) WideResNet-70-16 32.46 32.75 32.69 0.29DiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
Table 10. The ASR of APGD with 5 restarts, ACG with 5 restarts, and ACG with 1 restart for 500 iterations. The CIFAR-100 dataset
with"=8
255was used. The highest ASR is in bold, and the second is underlined. APGD( N) refers to APGD with N-times the
initial point selection. The meanings of the other columns are the same. diffis the difference between APGD(5) and maxfACG(5),
ACG-500iter g.
CIFAR-100 ( "= 8=255) Attack Success Rate
paper Architecture APGD(5) ACG(5) ACG-500iter diff
(Addepalli et al., 2021) PreActResNet-18 72.25 72.47 72.36 0.22
(Rade & Moosavi-Dezfooli, 2021) PreActResNet-18 70.55 70.86 70.77 0.31
(RebufÔ¨Å et al., 2021) PreActResNet-18 70.93 71.29 71.21 0.36
(Rice et al., 2020) PreActResNet-18 79.99 80.63 80.55 0.64
(Hendrycks et al., 2019) WideResNet-28-10 69.50 70.51 70.44 1.01
(RebufÔ¨Å et al., 2021) WideResNet-28-10 66.67 67.27 67.16 0.60
(Addepalli et al., 2021) WideResNet-34-10 68.74 68.52 68.59 -0.15
(Chen & Lee, 2021) WideResNet-34-10 68.36 68.77 68.68 0.41
(Cui et al., 2021) WideResNet-34-10 69.87 70.33 70.32 0.46
(Cui et al., 2021) WideResNet-34-10 72.15 72.56 72.64 0.49
(Sitawarin et al., 2021) WideResNet-34-10 73.43 74.27 74.09 0.84
(Wu et al., 2020) WideResNet-34-10 69.32 70.11 69.97 0.79
(Chen et al., 2021) WideResNet-34-10 71.96 72.18 72.19 0.23
(Cui et al., 2021) WideResNet-34-20 68.72 69.13 69.08 0.41
(Gowal et al., 2020) WideResNet-70-16 61.55 62.19 62.15 0.64
(Gowal et al., 2020) WideResNet-70-16 69.04 69.43 69.35 0.39
(RebufÔ¨Å et al., 2021) WideResNet-70-16 64.17 64.77 64.61 0.60
Table 11. The ASR of APGD with 5 restarts, ACG with 5 restarts, and ACG with 1 restart for 500 iterations. The ImageNet dataset with
"=4
255is used. The highest ASR is in bold, and the second is underlined. APGD( N) refers to APGD with N-times the initial point
selection. The meanings of the other columns are the same. diffis the difference between APGD(5) and maxfACG(5), ACG-500iter g.
ImageNet ("= 4=255) Attack Success Rate
paper Architecture APGD(5) ACG(5) ACG-500iter diff
(Salman et al., 2020) ResNet-18 73.00 73.72 73.56 0.72
(Salman et al., 2020) ResNet-50 62.86 63.70 63.54 0.84
(Wong et al., 2020) ResNet-50 71.70 71.94 71.92 0.24
(Engstrom et al., 2019) ResNet-50 67.86 68.60 68.58 0.74
(Salman et al., 2020) WideResNet-50-2 58.96 59.92 59.82 0.96DiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
Table 12. Comparison of the ASR of APGD and ACG with DLR loss as the objective function. Dataset: CIFAR10( "=8
255),
ImageNet("=4
255)
CIFAR-10 ("= 8=255) Attack Success Rate
paper Architecture clean acc APGD(5) ACG(5) diff
(Rade & Moosavi-Dezfooli, 2021) PreActResNet-18 89.02 41.61 42.13 0.52
(Rade & Moosavi-Dezfooli, 2021) PreActResNet-18 86.86 42.45 42.72 0.27
(RebufÔ¨Å et al., 2021) PreActResNet-18 83.53 42.88 43.22 0.34
(Rice et al., 2020) PreActResNet-18 85.34 44.25 46.06 1.81
(Andriushchenko et al., 2020) PreActResNet-18 79.84 52.99 55.38 2.39
(Sehwag et al., 2021) ResNet-18 84.59 43.38 45.11 1.73
(Chen et al., 2020) ResNet-50 86.04 47.82 48.35 0.53
(Wong et al., 2020) ResNet-50 83.34 53.25 55.72 2.47
(Engstrom et al., 2019) ResNet-50 87.03 47.36 49.82 2.46
(RebufÔ¨Å et al., 2021) WideResNet-106-16 88.50 34.67 35.19 0.52
(Carmon et al., 2019) WideResNet-28-10 89.69 39.35 40.14 0.79
(Gowal et al., 2020) WideResNet-28-10 89.48 36.29 37.00 0.71
(Hendrycks et al., 2019) WideResNet-28-10 87.11 43.03 44.75 1.72
(Rade & Moosavi-Dezfooli, 2021) WideResNet-28-10 88.16 38.62 39.00 0.38
(RebufÔ¨Å et al., 2021) WideResNet-28-10 87.33 38.37 39.12 0.75
(Sehwag et al., 2020) WideResNet-28-10 88.98 41.80 42.56 0.76
(Sridhar et al., 2021) WideResNet-28-10 89.46 39.14 40.06 0.92
(Wang et al., 2020) WideResNet-28-10 87.50 41.52 43.08 1.56
(Wu et al., 2020) WideResNet-28-10 88.25 39.50 39.86 0.36
(Zhang et al., 2021) WideResNet-28-10 89.36 39.79 40.57 0.78
(Ding et al., 2020) WideResNet-28-4 84.36 49.70 56.00 6.30
(Cui et al., 2021) WideResNet-34-10 88.22 44.28 46.92 2.64
(Huang et al., 2020) WideResNet-34-10 83.48 45.76 46.93 1.17
(Rade & Moosavi-Dezfooli, 2021) WideResNet-34-10 91.47 36.40 36.93 0.53
(Sehwag et al., 2021) WideResNet-34-10 86.68 39.22 40.61 1.39
(Sitawarin et al., 2021) WideResNet-34-10 86.84 46.79 48.78 1.99
(Wu et al., 2020) WideResNet-34-10 85.36 43.36 43.72 0.36
(Zhang et al., 2019a) WideResNet-34-10 87.20 52.59 54.65 2.06
(Zhang et al., 2019b) WideResNet-34-10 84.92 46.51 47.27 0.76
(Zhang et al., 2020) WideResNet-34-10 84.52 45.44 46.26 0.82
(Chen et al., 2021) WideResNet-34-10 85.32 47.32 48.24 0.92
(Sridhar et al., 2021) WideResNet-34-15 86.53 38.65 39.27 0.62
(Cui et al., 2021) WideResNet-34-20 88.70 44.68 46.39 1.71
(Gowal et al., 2020) WideResNet-34-20 85.64 42.57 43.02 0.45
(Pang et al., 2020) WideResNet-34-20 85.14 43.98 45.92 1.94
(Huang et al., 2021) WideResNet-34-R 90.56 36.91 38.04 1.13
(Huang et al., 2021) WideResNet-34-R 91.23 35.91 36.93 1.02
(Gowal et al., 2020) WideResNet-70-16 91.10 33.33 33.91 0.58
(Gowal et al., 2020) WideResNet-70-16 85.29 42.04 42.59 0.55
(Gowal et al., 2021) WideResNet-70-16 88.74 32.08 33.45 1.37
(RebufÔ¨Å et al., 2021) WideResNet-70-16 88.54 35.02 35.54 0.52
(RebufÔ¨Å et al., 2021) WideResNet-70-16 92.23 32.40 33.13 0.73
ImageNet ("= 4=255)
(Engstrom et al., 2019) ResNet-50 62.56 67.36 69.58 2.22
(Salman et al., 2020) ResNet-18 52.92 72.78 74.34 1.56
(Salman et al., 2020) WideResNet-50-2 68.46 58.38 60.90 2.52
(Wong et al., 2020) ResNet-50 55.62 71.38 73.00 1.62
(Salman et al., 2020) ResNet-50 64.02 62.40 64.70 2.30DiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
Table 13. Comparison of the ASR of APGD and ACG with DLR loss as the objective function. Dataset: CIFAR-100( "=8
255)
CIFAR-100 ( "= 8=255) Attack Success Rate
paper Architecture clean acc APGD(5) ACG(5) diff
(Rade & Moosavi-Dezfooli, 2021) PreActResNet-18 61.50 70.52 71.03 0.51
(Wu et al., 2020) WideResNet-34-10 60.38 68.97 70.64 1.67
(RebufÔ¨Å et al., 2021) WideResNet-28-10 62.41 66.64 67.71 1.07
(RebufÔ¨Å et al., 2021) WideResNet-70-16 63.56 64.14 65.07 0.93
(Chen et al., 2021) WideResNet-34-10 62.14 71.77 72.50 0.73
(Chen & Lee, 2021) WideResNet-34-10 64.07 68.31 69.11 0.80
(Rice et al., 2020) PreActResNet-18 53.83 79.83 80.76 0.93
(Hendrycks et al., 2019) WideResNet-28-10 59.23 68.37 70.73 2.36
(Cui et al., 2021) WideResNet-34-20 62.55 67.78 69.47 1.69
(RebufÔ¨Å et al., 2021) PreActResNet-18 56.87 70.86 71.42 0.56
(Sitawarin et al., 2021) WideResNet-34-10 62.82 72.83 74.93 2.10
(Addepalli et al., 2021) WideResNet-34-10 65.73 68.65 68.80 0.15
(Cui et al., 2021) WideResNet-34-10 60.64 70.92 72.89 1.97
(Gowal et al., 2020) WideResNet-70-16 60.86 69.01 69.85 0.84
(Gowal et al., 2020) WideResNet-70-16 69.15 61.39 62.73 1.34
(Addepalli et al., 2021) PreActResNet-18 62.02 72.19 72.58 0.39
(Cui et al., 2021) WideResNet-34-10 70.25 69.35 70.72 1.37
Table 14. The experimental result of the representative seven formulas to calculate for ACG.
CIFAR-10 ("= 8=255) Attack Success Rate
paper Architecture FR PR HS DY HZ DL LS
(Ding et al., 2020) WideResNet-28-4 48.88 52.78 55.77 48.08 49.98 44.87 52.05
(Carmon et al., 2019) WideResNet-28-10 39.03 39.55 40.03 35.43 39.05 33.70 39.56
(RebufÔ¨Å et al., 2021) PreActResNet-18 42.75 42.88 43.15 40.68 42.50 40.20 42.90
Table 15. Computational environments: Information on the Ô¨Åve computers used in the experiments are shown.
Machine No.1 & No.2 No.3 & No.4 No.5
CPUIntel(R) Xeon(R) Gold 6240R
CPU @ 2.40GHz2Intel(R) Xeon(R) Silver 4216
CPU @ 2.10GHz2Intel(R) Xeon(R) Gold 5120
CPU @ 2.20GHz2
GPU NVIDIA GeForce RTX 3090 4
RAM 768GB 256GBDiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
ùúÉ1.02.010-110-11.00.50.0‚ÑéùúÉ0average stepsize = 0.393
Diversity Index : 0.105
ùúÉ01.02.010-110-11.00.50.0‚ÑéùúÉaverage stepsize = 0.650
Diversity Index : 0.190
ùúÉ01.02.0
10-110-11.00.50.0‚ÑéùúÉaverage stepsize = 1.403
Diversity Index : 0.519
Figure 12. Additional examples of point clouds and its DIs.DiversiÔ¨Åed Adversarial Attacks based on Conjugate Gradient Method
Table 16. The ASR of AutoAttack(Reported) and AutoAttack combined with ACG with CE loss (AA(ACG-CE)).
paper Architecture AA(ACG-CE) Reported
(Ding et al., 2020) WideResNet-28-4 58.60 58.56
(Carmon et al., 2019) WideResNet-28-10 40.48 40.47
(Andriushchenko et al., 2020) WideResNet-18 56.07 56.07
(RebufÔ¨Å et al., 2021) PreActResNet-18 43.34 43.34
CIFAR 100
(Rice et al., 2020) PreActResNet-18 81.02 81.05
ImageNet
(Engstrom et al., 2019) ResNet-50 70.76 70.78
(Wong et al., 2020) ResNet-50 73.80 73.76
(Salman et al., 2020) ResNet-50 65.36 65.04