Wasserstein Adversarial Examples via Projected Sinkhorn Iterations
Eric Wong1Frank R. Schmidt2J. Zico Kolter3 4
Abstract
A rapidly growing area of work has studied the ex-
istence of adversarial examples, datapoints which
have been perturbed to fool a classiﬁer, but the
vast majority of these works have focused primar-
ily on threat models deﬁned by `pnorm-bounded
perturbations. In this paper, we propose a new
threat model for adversarial attacks based on the
Wasserstein distance. In the image classiﬁcation
setting, such distances measure the cost of mov-
ing pixel mass, which naturally cover “standard”
image manipulations such as scaling, rotation,
translation, and distortion (and can potentially
be applied to other settings as well). To generate
Wasserstein adversarial examples, we develop a
procedure for projecting onto the Wasserstein ball,
based upon a modiﬁed version of the Sinkhorn it-
eration. The resulting algorithm can successfully
attack image classiﬁcation models, bringing tra-
ditional CIFAR10 models down to 3% accuracy
within a Wasserstein ball with radius 0.1 (i.e.,
moving 10% of the image mass 1 pixel), and we
demonstrate that PGD-based adversarial training
can improve this adversarial accuracy to 76%. In
total, this work opens up a new direction of study
in adversarial robustness, more formally consid-
ering convex metrics that accurately capture the
invariances that we typically believe should ex-
ist in classiﬁers. Code for all experiments in the
paper is available at https://github :com/
locuslab/projected sinkhorn .
1. Introduction
A substantial effort in machine learning research has gone to-
wards studying adversarial examples (Szegedy et al., 2014),
commonly described as datapoints that are indistinguishable
1Machine Learning Department, Carnegie Mellon Univer-
sity, Pittsburgh, Pennsylvania, USA2Bosch Center for Artiﬁcial
Intelligence, Renningen, Germany3Computer Science Depart-
ment, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA
4Bosch Center for Artiﬁcial Intelligence, Pittsburgh, Pennsylvania,
USA. Correspondence to: Eric Wong .
Preprint.
+ W=
+ 1=
Figure 1. A minimal example exemplifying the difference between
Wasserstein perturbations and `1perturbations on an image with
six pixels. The top example utilizes a perturbation Wto shift
the image one pixel to the right, which is small with respect to
Wasserstein distance since each pixel moved a minimal amount,
but large with respect to `1distance since each pixel changed
a maximal amount. In contrast, the bottom example utilizes a
perturbation 1which changes all pixels to be grayer. This is
small with respect to `1distance, since each pixel changes by a
small amount, but large with respect to Wasserstein distance, since
the mass on each pixel on the left had to move halfway across the
image to the right.
from “normal” examples, but are speciﬁcally perturbed to
be misclassiﬁed by machine learning systems. This notion
of indistinguishability, later described as the threat model
for attackers, was originally taken to be `1bounded pertur-
bations, which model a small amount of noise injected to
each pixel (Goodfellow et al., 2015). Since then, subsequent
work on understanding, attacking, and defending against
adversarial examples has largely focused on this `1threat
model and its corresponding `pgeneralization. While the `p
ball is a convenient source of adversarial perturbations, it is
by no means a comprehensive description of all possible ad-
versarial perturbations. Other work (Engstrom et al., 2017)
has looked at perturbations such as rotations and transla-
tions, but beyond these speciﬁc transforms, there has been
little work considering broad classes of attacks beyond the
`pball.
In this paper, we propose a new type of adversarial pertur-
bation that encodes a general class of attacks that is funda-
mentally different from the `pball. Speciﬁcally, we propose
an attack model where the perturbed examples are bounded
in Wasserstein distance from the original example. This
distance can be intuitively understood for images as the
cost of moving around pixel mass to move from one image
to another. Note that the Wasserstein ball and the `pball
can be quite different in their allowable perturbations: ex-
amples that are close in Wasserstein distance can be quite
far in`pdistance, and vice versa (a pedagogical example
demonstrating this is in Figure 1).arXiv:1902.07906v2 [cs.LG] 18 Jan 2020Wasserstein Adversarial Examples via Projected Sinkhorn Iterations
We develop this idea of Wasserstein adversarial examples
in two main ways. Since adversarial examples are typi-
cally best generated using variants of projected gradient
descent, we ﬁrst derive an algorithm that projects onto the
Wasserstein ball. However, performing an exact projection
is computationally expensive, so our main contribution here
is to derive a fast method for approximate projection. The
procedure can be viewed as a modiﬁed Sinkhorn iteration,
but with a more complex set of update equations. Second,
we develop efﬁcient methods for adversarial training un-
der this threat method. Because this involves repeatedly
running this projection within an inner optimization loop,
speedups that use a local transport plan are particularly cru-
cial (i.e. only moving pixel mass to nearby pixels), making
the projection complexity linear in the image size.
We evaluate the attack quality on standard models, showing
for example that we can reduce the adversarial accuracy of
a standard CIFAR10 classiﬁer from 94.7% to 3% using a
Wasserstein ball of radius 0.1 (equivalent to moving 10%
of the mass of the image by one pixel), whereas the same
attack reduces the adversarial accuracy of a model certi-
ﬁably trained against `1perturbations from 66% to 61%.
In contrast, we show that with adversarial training, we are
able to improve the adversarial accuracy of this classiﬁer
to 76% while retaining a nominal accuracy of 80.7%. We
additionally show, however, that existing certiﬁed defenses
cannot be easily extended to this setting; building models
provably robust to Wasserstein attacks will require funda-
mentally new techniques. In total, we believe this work
highlights a new direction in adversarial examples: convex
perturbation regions which capture a much more intuitive
form of structure in their threat model, and which move
towards a more “natural” notion of adversarial attacks.
2. Background and Related Work
Much of the work in adversarial examples has focused on
the original`1threat model presented by Goodfellow et al.
(2015), some of which also extends naturally to `ppertur-
bations. Since then, there has been a plethora of papers
studying this threat model, ranging from improved attacks,
heuristic and certiﬁed defenses, and veriﬁers. As there are
far too many to discuss here, we highlight a few which are
the most relevant to this work.
The most commonly used method for generating adversarial
examples is to use a form of projected gradient descent over
the region of allowable perturbations, originally referred to
as the Basic Iterative Method (Kurakin et al., 2017). Since
then, there has been a back-and-forth of new heuristic de-
fenses followed by more sophisticated attacks. To name a
few, distillation was proposed as a defense but was defeated
(Papernot et al., 2016; Carlini & Wagner, 2017), realistic
transformations seen by vehicles were thought to be safe un-til more robust adversarial examples were created (Lu et al.,
2017; Athalye et al., 2018b), and many defenses submitted
to ICLR 2018 were broken before the review period even
ﬁnished (Athalye et al., 2018a). One undefeated heuristic
defense is to use the adversarial examples in adversarial
training, which has so far worked well in practice (Madry
et al., 2018). While this method has traditionally been used
for`1and`2balls (and has a natural `pgeneralization), in
principle, the method can be used to project onto any kind
of perturbation region.
Another set of related papers are veriﬁers and provable
defenses, which aim to produce (or train on) certiﬁcates
that are provable guarantees of robustness against adver-
sarial attacks. Veriﬁcation methods are now applicable
to multi-layer neural networks using techniques ranging
from semi-deﬁnite programming relaxations (Raghunathan
et al., 2018), mixed integer linear programming (Tjeng
et al., 2019), and duality (Dvijotham et al., 2018). Prov-
able defenses are able to tie veriﬁcation into training non-
trivial deep networks by backpropagating through certiﬁ-
cates, which are generated with duality-based bounds (Wong
& Kolter, 2018; Wong et al., 2018), abstract interpreta-
tions (Mirman et al., 2018), and interval bound propagation
(Gowal et al., 2018). These methods have subsequently
inspired new heuristic training defenses, where the resulting
models can be independently veriﬁed as robust (Croce et al.,
2018; Xiao et al., 2019). Notably, some of these approaches
arenotoverly reliant on speciﬁc types of perturbations (e.g.
duality-based bounds). Despite their generality, these cer-
tiﬁcates have only been trained and evaluated in the context
of`1and`2balls, and we believe this is due in large part
to a lack of alternatives.
Highly relevant to this work are attacks that lie outside the
traditional`pball of imperceptible noise. For example, sim-
ple rotations and translations form a fairly limited set of
perturbations that can be quite large in `pnorm, but are
sometimes sufﬁcient in order to fool classiﬁers (Engstrom
et al., 2017). On the other hand, adversarial examples that
work in the real world do not necessarily conform to the no-
tion of being “imperceptible”, and need to utilize a stronger
adversary that is visible to real world systems. Some exam-
ples include wearing adversarial 3D printed glasses to fool
facial recognition (Sharif et al., 2017), the use of adversarial
grafﬁti to attack trafﬁc sign classiﬁcation (Eykholt et al.,
2018), and printing adversarial textures on objects to attack
image classiﬁers (Athalye et al., 2018b). While Sharif et al.
(2017) allows perturbations that are physical glasses, the
others use an `pthreat model with a larger radius, when a
different threat model could be a more natural description
of adversarial examples that are perceptible on camera.
Last but not least, our paper relies heavily on the Wasserstein
distance, which has seen applications throughout machineWasserstein Adversarial Examples via Projected Sinkhorn Iterations
learning. The traditional notion of Wasserstein distance has
the drawback of being computationally expensive: com-
puting a single distance involves solving an optimal trans-
port problem (a linear program) with a number of variables
quadratic in the dimension of the inputs. However, it was
shown that by subtracting an entropy regularization term,
one can compute approximate Wasserstein distances ex-
tremely quickly using the Sinkhorn iteration (Cuturi, 2013),
which was later shown to run in near-linear time (Altschuler
et al., 2017). Relevant but orthogonal to our work, is that
of Sinha et al. (2018) on achieving distributional robust-
ness using the Wasserstein distance. While we both use the
Wasserstein distance in the context of adversarial training,
the approach is quite different: Sinha et al. (2018) use the
Wasserstein distance to perturb the underlying data distribu-
tion, whereas we use the Wasserstein distance as an attack
model for perturbing each example .
Contributions This paper takes a step back from using `p
as a perturbation metric, and proposes using the Wasserstein
distance instead as an equivalently general but qualitatively
different way of generating adversarial examples. To tackle
the computational complexity of projecting onto a Wasser-
stein ball, we use ideas from the Sinkhorn iteration (Cuturi,
2013) to derive a fast method for an approximate projection.
Speciﬁcally, we show that subtracting a similar entropy-
regularization term to the projection problem results in a
Sinkhorn-like algorithm, and using local transport plans
makes the procedure tractable for generating adversarial
images. In contrast to `1and`2perturbations, we ﬁnd
that the Wasserstein metric generates adversarial examples
whose perturbations have inherent structure reﬂecting the
actual image itself (see Figure 2 for a comparison). We
demonstrate the efﬁcacy of this attack on standard models,
models trained against this attack, and provably robust mod-
els (against`1attacks) on MNIST and CIFAR10 datasets.
While the last of these models are not trained to be robust
speciﬁcally against this attack, we observe that that some
(but not all) robustness empirically transfers over to pro-
tection against the Wasserstein attack. More importantly,
we show that while the Wasserstein ball does ﬁt naturally
into duality based frameworks for generating and training
against certiﬁcates, there is a fundamental roadblock pre-
venting these methods from generating non-vacuous bounds
on Wasserstein balls.
3. Preliminaries
PGD-based adversarial attacks The most common
method of creating adversarial examples is to use a vari-
ation of projected gradient descent. Speciﬁcally, let (x;y)
be a datapoint and its label, and let B(x;)be some ball
aroundxwith radius, which represents the threat model
for the adversary. We ﬁrst deﬁne the projection operator
+
 =
+
 =
0 1
−1 1
0 1
Figure 2. A comparison of a Wasserstein (top) vs an `1(bottom)
adversarial example for an MNIST classiﬁer (for = 0:4and
0:3respectively), showing the original image (left), the added
perturbation (middle), and the ﬁnal perturbed image (right). We
ﬁnd that the Wasserstein perturbation has a structure reﬂecting
the actual content of the image, whereas the `1perturbation also
attacks the background pixels.
ontoB(x;)to be
proj
B(x;)(w) = arg min
z2B(x;)kw zk2
2 (1)
which ﬁnds the point closest (in Euclidean space) to the
inputwthat lies within the ball B(x;). Then, for some
step sizeand some loss `(e.g. cross-entropy loss), the
algorithm consists of the following iteration:
x(t+1)= proj
B(x;) 
x(t)+ arg max
kvkvTr`(x(t);y)!
(2)
wherex(0)=xor any randomly initialized point within
B(x;). This is sometimes referred to as projected steepest
descent, which is used to generated adversarial examples
since the standard gradient steps are typically too small. If
we consider the `1ballB1(x;) =fx+  :kk1g
and use steepest descent with respect to the `1norm, then
we recover the Basic Iterative Method originally presented
by Kurakin et al. (2017).
Adversarial training One of the heuristic defenses that
works well in practice is to use adversarial training with
a PGD adversary. Speciﬁcally, instead of minimizing the
loss evaluated at a example x, we minimize the loss on
an adversarially perturbed example xadv, wherexadvis
obtained by running the projected gradient descent attack
for the ballB(x;)for some number of iterations, as shown
in Algorithm 1. Taking B(x;)to be an`1ball recovers
the procedure used by Madry et al. (2018).
Wasserstein distance Finally, we deﬁne the most crucial
component of this work, an alternative metric from `pdis-
tances. The Wasserstein distance (also referred to as the
Earth mover’s distance) is an optimal transport problem that
can be intuitively understood in the context of distributions
as the minimum cost of moving probability mass to change
one distribution into another. When applied to images, thisWasserstein Adversarial Examples via Projected Sinkhorn Iterations
Algorithm 1 An epoch of adversarial training for a loss
function`, classiﬁerfwith parameters , and step size
parameterfor some ballB.
input: Training data (xi;yi),i= 1:::n
fori= 1:::n do
// Run PGD adversary
xadv:=xi
fort= 1:::T do
:= arg maxkvkvTr`(xadv;yi)
xadv:= projB(xi;)(xadv+)
end for
// Backpropagate with xadv, e.g. with SGD
Updatewithr`(f(xadv);yi)
end for
can be interpreted as the cost of moving pixel mass from
one pixel to another another, where the cost increases with
distance.
More speciﬁcally, let x;y2Rn
+be two non-negative data
points such thatP
ixi=P
jyj= 1, so images and other
inputs need to be normalized, and let C2Rnn
+ be some
non-negative cost matrix where Cijencodes the cost of
moving mass from xitoyj. Then, the Wasserstein distance
dWbetweenxandyis deﬁned to be
dW(x;y) = min
2Rnn
+h;Ci
subject to 1 = x;T1 =y(3)
where the minimization over transport plans , whose en-
triesijencode how the mass moves from xitoyj. Then,
we can deﬁne the Wasserstein ball with radius as
BW(x;) =fx+  :dW(x;x+ )g (4)
4. Wasserstein Adversarial Examples
The crux of this work relies on offering a fundamentally
different type of adversarial example from typical, `pper-
turbations: the Wasserstein adversarial example.
4.1. Projection onto the Wasserstein Ball
In order to generate Wasserstein adversarial examples, we
can run the projected gradient descent attack from Equation
(2), dropping in the Wasserstein ball BWfrom Equation
(4)in place ofB. However, while projections onto regions
such as`1and`2balls are straightforward and have closed
form computations, simply computing the Wasserstein dis-
tance itself requires solving an optimization problem. Thus,
the ﬁrst natural requirement to generating Wasserstein ad-
versarial examples is to derive an efﬁcient way to project
examples onto a Wasserstein ball of radius . Speciﬁcally,
projectingwonto the Wasserstein ball around xwith radiusand transport cost matrix Ccan be written as solving the
following optimization problem:
minimize
z2Rn
+;2Rnn
+1
2kw zk2
2
subject to 1 = x;T1 =z
h;Ci(5)
While we could directly solve this optimization problem
(using an off-the-shelf quadratic programming solver), this
is prohibitively expensive to do for every iteration of pro-
jected gradient descent, especially since there is a quadratic
number of variables. However, Cuturi (2013) showed that
the standard Wasserstein distance problem from Equation
(3)can be approximately solved efﬁciently by subtracting
an entropy regularization term on the transport plan W, and
using the Sinkhorn-Knopp matrix scaling algorithm. Mo-
tivated by these results, instead of solving the projection
problem in Equation (5)exactly, the key contribution that
allows us to do the projection efﬁciently is to instead solve
the following entropy-regularized projection problem:
minimize
z2Rn
+;2Rnn
+1
2kw zk2
2+1
X
ijijlog(ij)
subject to 1 = x;T1 =z
h;Ci:(6)
Although this is an approximate projection onto the Wasser-
stein ball, importantly, the looseness in the approximation is
only in ﬁnding the projection zwhich is closest (in `2norm)
to the original example x. All feasible points, including the
optimal solution, are still within the actual -Wasserstein
ball, so examples generated using the approximate projec-
tion are still within the Wasserstein threat model.
Using the method of Lagrange multipliers, we can introduce
dual variables (;; )and derive an equivalent dual prob-
lem in Lemma 3 (the proof is deferred to Appendix A.1).
Lemma 1. The dual of the entropy-regularized Wasserstein
projection problem in Equation (6)is
maximize
;2Rn; 2R+g(;; ) (7)
where
g(;; ) = 1
2kk2
2  +Tx+Tw
 X
ijexp(i) exp(  Cij 1) exp(j)(8)
Note that the dual problem here differs from the traditional
dual problem for Sinkhorn iterates by having an additionalWasserstein Adversarial Examples via Projected Sinkhorn Iterations
quadratic term on and an additional dual variable .
Nonetheless, we can still derive a Sinkhorn-like algorithm
by performing block coordinate ascent over the dual vari-
ables (the full derivation can be found in Appendix A.3).
Speciﬁcally, maximizing gwith respect to results in
arg max
ig(;; ) =
log (xi) log0
@X
jexp(  Cij 1) exp(j)1
A;(9)
which is identical (up to a log transformation of variables)
to the original Sinkhorn iterate proposed in Cuturi (2013).
The maximization step for can also be done analytically
with
arg max
jg(;; ) =
wj W 
exp(wj)X
iexp(i) exp(  Cij 1)!
(10)
whereWis the Lambert Wfunction, which is deﬁned as the
inverse off(x) =xex. Finally, since cannot be solved
for analytically, we can perform the following Newton step
 0=  t@g=@ 
@2g=@ 2(11)
where
@g=@ = +X
ijexp(i)Cijexp(  Cij) exp(j)
@2g=@ 2= X
ijexp(i)C2
ijexp(  Cij) exp(j)
(12)
and wheretis small enough such that 00. Once we
have solved the dual problem, we can recover the primal
solution (to get the actual projection), which is described in
Lemma 4 and proved in Appendix A.2.
Lemma 2. Suppose;; maximize the dual problem
gin Equation (16). Then,
z
i=wi i=

ij= exp(
i) exp(  Cij 1) exp(
j)(13)
are the corresponding solutions that minimize the primal
problem in Equation (6).
The whole algorithm can then be vectorized and imple-
mented as Algorithm 2, which we call projected Sinkhorn
iterates. The algorithm uses a simple line search to ensure
that the constraint 0is not violated. Each iteration
has 8O(n2)operations (matrix-vector product or matrix-
matrix element-wise product), in comparison to the original
Sinkhorn iteration which has 2 matrix-vector products.Algorithm 2 Projected Sinkhorn iteration to project xonto
theWasserstein ball around y. We useto denote element-
wise multiplication. The logandexpoperators also apply
element-wise.
input:x;w2Rn;C2Cnn;2R
Initializei;i:= log(1=n)fori= 1;:::;n and := 1
u;v:= exp();exp()
while;; not converged do
// updateK
K := exp(  C 1)
// block coordinate descent iterates
:= log(x) log(K v)
u:= exp()
:=w W 
uTK exp(w)
v:= exp()
// Newton step
g:= +uT(CK )v
h:= uT(CCK )v
// ensure 0
:= 1
while  g=h< 0do
:==2
end while
 :=  g=h
end while
return:w =
Matrix scaling interpretation The original Sinkhorn it-
eration has a natural interpretation as a matrix scaling al-
gorithm, iteratively rescaling the rows and columns of a
matrix to achieve the target distributions. The Projected
Sinkhorn iteration has a similar interpretation: while the
step rescales the rows of exp(  C 1)to sum tox,
thestep rescales the columns of exp(  C 1)to sum
to =+w, which is the primal transformation of the
projected variable zat optimality as described in Lemma
4. Lastly, the step can be interpreted as correcting for
the transport cost of the current scaling: the numerator of
the Newton step is simply the difference between the trans-
port cost of the current matrix scaling and the maximum
constraint. A full derivation of the algorithm and a more
detailed explanation on this interpretation can be found in
Appendix A.3.
4.2. Local Transport Plans
The quadratic runtime dependence on input dimension can
grow quickly, and this is especially true for images. Rather
than allowing transport plans to move mass to and from
any pair of pixels, we instead restrict the transport plan toWasserstein Adversarial Examples via Projected Sinkhorn Iterations
Table 1. Classiﬁcation accuracies for models used in the experi-
ments.
DATA SET MODEL NOMINAL ACCURACY
MNIST S TANDARD 98.90%
BINARIZE 98.73%
ROBUST 98.20%
ADV. TRAINING 96.95%
CIFAR10 S TANDARD 94.70%
ROBUST 66.33%
ADV. TRAINING 80.69%
move mass only within a kkregion of the originating
pixel, similar in spirit to a convolutional ﬁlter. As a result,
the cost matrix Conly needs to deﬁne the cost within a
kkregion, and we can utilize tools used for convolutional
ﬁlters to efﬁciently apply the cost to each kkregion.
This reduces the computational complexity of each iteration
toO(nk2). For images with more than one channel, we
can use the same transport plan for each channel and only
allow transport within a channel, so the cost matrix remains
kk. For 55local transport plans on CIFAR10, the
projected Sinkhorn iterates typically converge in around
30-40 iterations, taking about 0.02 seconds per iteration
on a Titan X for minibatches of size 100. Note that if we
use a cost matrix Cthat reﬂects the 1-Wasserstein distance,
then this problem could be solved even more efﬁciently
using Kantrovich duality, however we use this formulation
to enable more general p-Wasserstein distances, or even
non-standard cost matrices.
Projected gradient descent on the Wasserstein ball
With local transport plans, the method is fast enough to be
used within a projected gradient descent routine to generate
adversarial examples on images, and further used for ad-
versarial training as in Algorithm 1 (using steepest descent
with respect to `1norm), except that we do an approximate
projection onto the Wasserstein ball using Algorithm 2.
5. Results
In this section, we run the Wasserstein examples through
a range of typical experiments in the literature of adver-
sarial examples. Table 1 summarizes the nominal er-
ror rates obtained by all considered models. All exper-
iments can be run on a single GPU, and all code for
the experiments is available at https://github :com/
locuslab/projected sinkhorn .
Architectures For MNIST we used the convolutional
ReLU architecture used in Wong & Kolter (2018), with
two convolutional layers with 16 and 32 44ﬁlters each,
followed by a fully connected layer with 100 units, which
achieves a nominal accuracy of 98.89%. For CIFAR10 we
+
 =
standard,= 0:53
binary,= 0:44
`1robust,= 0:78
adv. training, = 0:86
Figure 3. Wasserstein adversarial examples on the MNIST dataset
for the four different models. Note that the `1robust and the
adversarially trained models require a much larger radius for
the Wasserstein ball in order to generate an adversarial example.
Each model classiﬁes the corresponding perturbed example as an 8
instead of a 5, except for the ﬁrst one which classiﬁes the perturbed
example as a 6.
focused on the standard ResNet18 architecture (He et al.,
2016), which achieves a nominal accuracy of 94.76%.
Hyperparameters For all experiments in this section, we
focused on using 55local transport plans for the Wasser-
stein ball, and used an entropy regularization constant of
1000 for MNIST and 3000 for CIFAR10. The cost matrix
used for transporting between pixels is taken to be the 2-
norm of the distance in pixel space (e.g. the cost of going
from pixel (i;j)to(k;l)isp
ji jj2+jk lj2), which
makes the optimal transport cost a metric more formally
known as the 1-Wasserstein distance. For more extensive
experiments on using different sizes of transport plans, dif-
ferent regularization constants, and different cost matrices,
we direct the reader to Appendix C.
Evaluation at test time We use the follow evaluation pro-
cedure to attack models with projected gradient descent on
the Wasserstein ball. For each MNIST example, we start
with= 0:3and increase it by a factor of 1.1 every 10
iterations until either an adversarial example is found or
until 200 iterations have passed, allowing for a maximum
perturbation radius of = 2. For CIFAR10, we start with
= 0:001and increase it by a factor of 1.17 until either
and adversarial example is found or until 400 iterations
have passed, allowing for a maximum perturbation radius
of= 0:53.
5.1. MNIST
For MNIST, we consider a standard model, a model with
binarization, a model provably robust to `1perturbations
of at most= 0:1, and an adversarially trained model. We
provide a visual comparison of the Wasserstein adversarial
examples generated on each of the four models in Figure
3. The susceptibility of all four models to the Wasserstein
attack is plotted in Figure 4.Wasserstein Adversarial Examples via Projected Sinkhorn Iterations
0.0 0.5 1.0 1.5
ε radius for Wasserstein ball0.00.20.40.60.81.0Adversarial accuracystandard
ℓ∞ robust
adv. training
binarize
Figure 4. Adversarial accuracy of various models on MNIST when
attacked by a Wasserstein adversary over varying sizes of -
Wasserstein balls. We ﬁnd that all models not trained with adver-
sarial training against this attack eventually achieve 0% accuracy,
however we do observe that models trained to be provably ro-
bust against`1perturbations are still somewhat more robust than
standard models, or models utilizing binarization as a defense.
Standard model and binarization For MNIST, despite
restricting the transport plan to local 55regions, a stan-
dard model is easily attacked by Wasserstein adversarial
examples. In Figure 4, we see that Wasserstein attacks with
= 0:5can successfully attack a typical MNIST classiﬁer
50% of the time, which goes up to 94% for = 1. A Wasser-
stein radius of = 0:5can be intuitively understood as mov-
ing 50% of the pixel mass over by 1 pixel, or alternatively
moving less than 50% of the pixel mass more than 1 pixel.
Furthermore, while preprocessing images with binarization
is often seen as a way to trivialize adversarial examples on
MNIST, we ﬁnd that it performs only marginally better than
the standard model against Wasserstein perturbations.
`1robust model We also run the attack on the model
trained by Wong et al. (2018), which is guaranteed to be
provably robust against `1perturbations with 0:1.
While not speciﬁcally trained against Wasserstein perturba-
tions, in Figure 4 we ﬁnd that it is substantially more robust
than either the standard or the binarized model, requiring a
signiﬁcantly larger to have the same attack success rate.
Adversarial training Finally, we apply this attack as an
inner procedure within an adversarial training framework
for MNIST. To save on computation, during training we
adopt a weaker adversary and use only 50 iterations of pro-
jected gradient descent. We also let grow within a range
and train on the ﬁrst adversarial example found (essentially
a budget version of the attack used at test time). Speciﬁc
details regarding this schedule and also the learning pa-
rameters used can be found in Appendix B.1. We ﬁnd that
the adversarially trained model is empirically the most well
defended against this attack of all four models, and cannot
be attacked down to 0% accuracy (Figure 4).
10−310−210−1
ε radius for Wasserstein ball0.00.20.40.60.8Adversarial accuracystandard
ℓ∞ robust
adv. trainingFigure 5. Adversarial accuracy of various models on the CIFAR10
dataset when attacked by a Wasserstein adversary. We ﬁnd that the
model trained to be provably robust against `1perturbations is not
as robust as adversarial training against a Wasserstein adversary.
plane
car
bird
deerbird
cat
deer
dogdeer
dog
+
 =
horse
catfrog
horse
bird
dogship
truck
plane
car
Figure 6. Wasserstein adversarial examples for CIFAR10 on a typ-
ical ResNet18 for all 10 classes. The perturbations here represents
the total change across all three channels, where total change is
plotted within the range 0:165(the maximum total change ob-
served in a single pixel) for images scaled to [0,1].
5.2. CIFAR10
For CIFAR10, we consider a standard model, a model prov-
ably robust to `1perturbations of at most = 2=255, and
an adversarially trained model. We plot the susceptibility of
each model to the Wasserstein attack in Figure 5.
Standard model We ﬁnd that for a standard ResNet18
CIFAR10 classiﬁer, a perturbation radius of as little as 0:01
is enough to misclassify 25% of the examples, while a radius
of0:1is enough to fool the classiﬁer 97% of the time (Figure
5). Despite being such a small , we see in Figure 6 that the
structure of the perturbations still reﬂect the actual content of
the images, though certain classes require larger magnitudes
of change than others.
`1robust model We further empirically evaluate the at-
tack on a model that was trained to be provably robust
against`1perturbations. We use the models weights from
Wong et al. (2018), which are trained to be provably robustWasserstein Adversarial Examples via Projected Sinkhorn Iterations
against`1perturbations of at most = 2=255. Further
note that this CIFAR10 model actually is a smaller ResNet
than the ResNet18 architecture considered in this paper, and
consists of 4 residual blocks with 16, 16, 32, and 64 ﬁl-
ters. Nonetheless, we ﬁnd that while the model suffers from
poor nominal accuracy (achieving only 66% accuracy on
unperturbed examples as noted in Table 1), the robustness
against`1attacks remarkably seems to transfer quite well
to robustness against Wasserstein attacks in the CIFAR10
setting, achieving 61% adversarial accuracy for = 0:1in
comparison to 3% for the standard model.
Adversarial training To perform adversarial training for
CIFAR10, we use a similar scheme to that used for MNIST:
we adopt a weaker adversary that uses only 50 iterations
of projected gradient descent during training and allow 
to grow within a range (speciﬁc details can be found in
Appendix B.2). We ﬁnd that adversarial training here is also
able to defend against this attack, and at the same threshold
of= 0:1, we ﬁnd that the adversarial accuracy has been
improved from 3% to 76%.
5.3. Provable Defenses against Wasserstein
Perturbations
Lastly, we present some analysis on how this attack ﬁts into
the context of provable defenses, along with a negative result
demonstrating a fundamental gap that needs to be solved.
The Wasserstein attack can be naturally incorporated into
duality based defenses: Wong et al. (2018) show that to use
their certiﬁcates to defend against other inputs, one only
needs to solve the following optimization problem:
max
x2B(x;) xTy (14)
for some constant yand for some perturbation region
B(x;)(a similar approach can be taken to adapt the dual
veriﬁcation from Dvijotham et al. (2018)). For the Wasser-
stein ball, this is highly similar to the problem of projecting
onto the Wasserstein ball from Equation (6), with a linear
objective instead of a quadratic objective and fewer vari-
ables. In fact, a Sinkhorn-like algorithm can be derived to
solve this problem, which ends up being a simpliﬁed version
of Algorithm 2 (this is shown in Appendix D).
However, there is a fundamental obstacle towards generat-
ing provable certiﬁcates against Wasserstein attacks: these
defenses (and many other, non-duality based approaches)
depend heavily on propagating interval bounds from the in-
put space through the network, in order to efﬁciently bound
the output of ReLU units. This concept is inherently at odds
with the notion of Wasserstein distance: a “small” Wasser-
stein ball can use a low-cost transport plan to move all the
mass at a single pixel to its neighbors, or vice versa. As a
result, when converting a Wasserstein ball to interval con-straints, the interval bounds immediately become vacuous:
each individual pixel can attain their minimum or maximum
value under some cost transport plan. In order to guarantee
robustness against Wasserstein adversarial attacks, signiﬁ-
cant progress must be made to overcome this limitation.
6. Conclusion
In this paper, we have presented a new, general threat model
for adversarial examples based on the Wasserstein distance,
a metric that captures a kind of perturbation that is fun-
damentally different from traditional `pperturbations. To
generate these examples, we derived an algorithm for fast,
approximate projection onto the Wasserstein ball that can
use local transport plans for even more speedup on im-
ages. We successfully attacked standard networks, showing
that these adversarial examples are structurally perturbed
according to the content of the image, and demonstrated
the empirical effectiveness of adversarial training. Finally,
we observed that networks trained to be provably robust
against`1attacks are more robust than the standard net-
works against Wasserstein attacks, however we show that
the current state of provable defenses is insufﬁcient to di-
rectly apply to the Wasserstein ball due to their reliance on
interval bounds.
We believe overcoming this roadblock is crucial to the de-
velopment of veriﬁers or provable defenses against not just
the Wasserstein attack, but also to improve the robustness of
classiﬁers against other attacks that do not naturally convert
to interval bounds (e.g. `0or`1attacks). Whether we can
develop efﬁcient veriﬁcation or provable training methods
that do not rely on interval bounds remains an open question.
Perhaps the most natural future direction for this work is to
begin to understand the properties of Wasserstein adversarial
examples and what we can do to mitigate them, even if
only at a heuristic level. However, at the end of the day,
the Wasserstein threat model deﬁnes just one example of a
convex region capturing structure that is different from `p
balls. By no means have we characterized all reasonable
adversarial perturbations, and so a signiﬁcant gap remains
in determining how to rigorously deﬁne general classes
of adversarial examples that can characterize phenomena
different from the `pand Wasserstein balls.
Finally, although we focused primarily on adversarial exam-
ples in this work, the method of projecting onto Wasserstein
balls may be applicable outside of deep learning. Projec-
tion operators play a major role in optimization algorithms
beyond projected gradient descent (e.g. ADMM and alter-
nating projections). Perhaps even more generally, the tech-
niques in this paper could be used to derive Sinkhorn-like
algorithms for classes of problems that consider Wasserstein
constrained variables.Wasserstein Adversarial Examples via Projected Sinkhorn Iterations
References
Altschuler, J., Weed, J., and Rigollet, P. Near-linear time ap-
proximation algorithms for optimal transport via sinkhorn
iteration. In Advances in Neural Information Processing
Systems , pp. 1964–1974, 2017.
Athalye, A., Carlini, N., and Wagner, D. Obfuscated gra-
dients give a false sense of security: Circumventing de-
fenses to adversarial examples. In Proceedings of the 35th
International Conference on Machine Learning, ICML
2018 , July 2018a. URL https://arxiv :org/abs/
1802:00420 .
Athalye, A., Engstrom, L., Ilyas, A., and Kwok, K. Syn-
thesizing robust adversarial examples. In Dy, J. and
Krause, A. (eds.), Proceedings of the 35th International
Conference on Machine Learning , volume 80 of Pro-
ceedings of Machine Learning Research , pp. 284–293,
Stockholmsmssan, Stockholm Sweden, 10–15 Jul 2018b.
PMLR. URL http://proceedings :mlr:press/
v80/athalye18b :html .
Carlini, N. and Wagner, D. Towards evaluating the robust-
ness of neural networks. In Security and Privacy (SP),
2017 IEEE Symposium on , pp. 39–57. IEEE, 2017.
Croce, F., Andriushchenko, M., and Hein, M. Provable
robustness of relu networks via maximization of linear
regions. CoRR , abs/1810.07481, 2018. URL http:
//arxiv:org/abs/1810 :07481 .
Cuturi, M. Sinkhorn distances: Lightspeed computation
of optimal transport. In Burges, C. J. C., Bottou, L.,
Welling, M., Ghahramani, Z., and Weinberger, K. Q.
(eds.), Advances in Neural Information Processing
Systems 26 , pp. 2292–2300. Curran Associates, Inc.,
2013. URL http://papers :nips:cc/paper/
4927-sinkhorn-distances-lightspeed-
computation-of-optimal-transport :pdf.
Dvijotham, K., Stanforth, R., Gowal, S., Mann, T., and
Kohli, P. A dual approach to scalable veriﬁcation of
deep networks. In Proceedings of the Thirty-Fourth Con-
ference Annual Conference on Uncertainty in Artiﬁcial
Intelligence (UAI-18) , pp. 162–171, Corvallis, Oregon,
2018. AUAI Press.
Engstrom, L., Tran, B., Tsipras, D., Schmidt, L., and Madry,
A. A rotation and a translation sufﬁce: Fooling cnns with
simple transformations. arXiv preprint arXiv:1712.02779 ,
2017.
Eykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A.,
Xiao, C., Prakash, A., Kohno, T., and Song, D. Robust
physical-world attacks on deep learning visual classiﬁca-
tion. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pp. 1625–1634, 2018.Goodfellow, I., Shlens, J., and Szegedy, C. Explaining
and harnessing adversarial examples. In International
Conference on Learning Representations , 2015. URL
http://arxiv :org/abs/1412 :6572 .
Gowal, S., Dvijotham, K., Stanforth, R., Bunel, R., Qin,
C., Uesato, J., Arandjelovic, R., Mann, T. A., and
Kohli, P. On the effectiveness of interval bound prop-
agation for training veriﬁably robust models. CoRR ,
abs/1810.12715, 2018. URL http://arxiv :org/
abs/1810:12715 .
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition ,
pp. 770–778, 2016.
Kurakin, A., Goodfellow, I., and Bengio, S. Adversarial
examples in the physical world. ICLR Workshop , 2017.
URL https://arxiv :org/abs/1607 :02533 .
Lu, J., Sibai, H., Fabry, E., and Forsyth, D. No need to
worry about adversarial examples in object detection in
autonomous vehicles. arXiv preprint arXiv:1707.03501 ,
2017.
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and
Vladu, A. Towards deep learning models resistant
to adversarial attacks. In International Conference
on Learning Representations , 2018. URL https://
openreview :net/forum?id=rJzIBfZAb .
Mirman, M., Gehr, T., and Vechev, M. Differ-
entiable abstract interpretation for provably ro-
bust neural networks. In International Confer-
ence on Machine Learning (ICML) , 2018. URL
https://www :icml:cc/Conferences/2018/
Schedule?showEvent=2477 .
Papernot, N., McDaniel, P., Wu, X., Jha, S., and Swami,
A. Distillation as a defense to adversarial perturbations
against deep neural networks. In Security and Privacy
(SP), 2016 IEEE Symposium on , pp. 582–597. IEEE,
2016.
Raghunathan, A., Steinhardt, J., and Liang, P. S. Semideﬁ-
nite relaxations for certifying robustness to adversarial
examples. In Bengio, S., Wallach, H., Larochelle,
H., Grauman, K., Cesa-Bianchi, N., and Garnett, R.
(eds.), Advances in Neural Information Processing
Systems 31 , pp. 10900–10910. Curran Associates, Inc.,
2018. URL http://papers :nips:cc/paper/
8285-semidefinite-relaxations-for-
certifying-robustness-to-adversarial-
examples:pdf.Wasserstein Adversarial Examples via Projected Sinkhorn Iterations
Sharif, M., Bhagavatula, S., Bauer, L., and Reiter, M. K.
Adversarial generative nets: Neural network attacks
on state-of-the-art face recognition. arXiv preprint
arXiv:1801.00349 , 2017.
Sinha, A., Namkoong, H., and Duchi, J. Certifying some dis-
tributional robustness with principled adversarial training.
2018.
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Er-
han, D., Goodfellow, I., and Fergus, R. Intriguing
properties of neural networks. In International Confer-
ence on Learning Representations , 2014. URL http:
//arxiv:org/abs/1312 :6199 .
Tjeng, V ., Xiao, K. Y ., and Tedrake, R. Evaluating ro-
bustness of neural networks with mixed integer program-
ming. In International Conference on Learning Repre-
sentations , 2019. URL https://openreview :net/
forum?id=HyGIdiRqtm .
Wong, E. and Kolter, Z. Provable defenses against adver-
sarial examples via the convex outer adversarial polytope.
InInternational Conference on Machine Learning , pp.
5283–5292, 2018.
Wong, E., Schmidt, F., Metzen, J. H., and Kolter,
J. Z. Scaling provable adversarial defenses. In Ben-
gio, S., Wallach, H., Larochelle, H., Grauman, K.,
Cesa-Bianchi, N., and Garnett, R. (eds.), Advances
in Neural Information Processing Systems 31 , pp.
8410–8419. Curran Associates, Inc., 2018. URL http:
//papers:nips:cc/paper/8060-scaling-
provable-adversarial-defenses :pdf.
Xiao, K. Y ., Tjeng, V ., Shaﬁullah, N. M. M., and Madry,
A. Training for faster adversarial robustness veriﬁcation
via inducing reLU stability. In International Conference
on Learning Representations , 2019. URL https://
openreview :net/forum?id=BJfIVjAcKm .Wasserstein Adversarial Examples via Projected Sinkhorn Iterations
A. Projected Sinkhorn derivation
A.1. Proof of Lemma 3
Lemma 3. The dual of the entropy-regularized Wasserstein
projection problem in Equation (6)is
maximize
;2Rn; 2R+g(;; ) (15)
where
g(;; ) = 1
2kk2
2  +Tx+Tw
 X
ijexp(i) exp(  Cij 1) exp(j)(16)
Proof. For convenience, we multiply the objective by and
solve this problem instead:
minimize
z2Rn
+;2Rnn
+
2kw zk2
2+X
ijijlog(ij)
subject to 1 = x
T1 =z
h;Ci:(17)
Introducing dual variables (;; )where 0, the La-
grangian is
L(z;;;; )
=
2kw zk2
2+X
ijijlog(ij) + (h;Ci )
+T(x 1) +T(z T1):(18)
The KKT optimality conditions are now
@L
@ij= Cij+ (1 + log( ij)) i j= 0
@L
@zj=(zj wj) +j= 0(19)
so at optimality, we must have
ij= exp(i) exp(  Cij 1) exp(j)
z= 
+w(20)
Plugging in the optimality conditions, we get
L(z;;;; )
= 1
2kk2
2  +Tx+Tw
 X
ijexp(i) exp(  Cij 1) exp(j)
=g(;; )(21)
so the dual problem is to maximize gover;; 0.A.2. Proof of Lemma 4
Lemma 4. Suppose;; maximize the dual problem
gin Equation (16). Then,
z
i=wi i=

ij= exp(
i) exp(  Cij 1) exp(
j)(22)
are the corresponding solutions that minimize the primal
problem in Equation (6).
Proof. These equations follow directly from the KKT opti-
mality conditions from Equation (20).
A.3. Algorithm derivation and interpretation
Derivation To derive the algorithm, note that since this is
a strictly convex problem to get the anditerates we solve
for setting the gradient to 0. The derivative with respect to
is
@g
@i=xi exp(i)X
jexp(  Cij 1) exp(j)(23)
and so setting this to 0 and solving for igives theiterate.
The derivative with respect to is
@g
@j= 1
+w exp(j)X
iexp(i) exp(  Cij 1)
(24)
and setting this to 0 and solving for jgives theiterate
(this step can be done using a symbolic solver, we used
Mathematica). Lastly, the updates are straightforward
scalar calculations of the derivative and second derivative.
Interpretation Recall from the transformation of dual to
primal variables from Lemma 4. To see how the Projected
Sinkhorn iteration is a (modiﬁed) matrix scaling algorithm,
we can interpret these quantities before optimality as primal
iterates. Namely, at each iteration t, let
z(t)
i=wi (t)
i=
(t)
ij= exp((t)) exp(  (t)Cij 1) exp((t))(25)
Then, since the andsteps are equivalent to setting Equa-
tions (23) and(24) to 0, we know that after an update for
(t), we have that
xi=X
j(t)
ij (26)
so thestep rescales the transport matrix to sum to x.
Similarly, after an update for (t), we have that
z(t)
i=X
i(t)
ij (27)Wasserstein Adversarial Examples via Projected Sinkhorn Iterations
which is a rescaling of the transport matrix to sum to the
projected value. Lastly, the numerator of the (t)step can
be rewritten as
 (t+1)= (t)+th(t);Ci 
h(t);CCi(28)
as a simple adjustment based on whether the current trans-
port plan (t)is above or below the maximum threshold
.
B. Experimental setup
B.1. MNIST
AdaptiveDuring adversarial training for MNIST, we
adopt an adaptive scheme to avoid selecting a speciﬁc .
Speciﬁcally, to ﬁnd an adversarial example, we ﬁrst let =
0:1on the ﬁrst iteration of projected gradient descent, and
increase it by a factor of 1:4every 5 iterations. We terminate
the projected gradient descent algorithm when either an
adversarial example is found, or when 50 iterations have
passed, allowing to take on values in the range [0:1;2:1]
Optimizer hyperparameters To update the model
weights during adversarial training, we use the SGD op-
timizer with 0.9 momentum and 0.0005 weight decay, and
batch sizes of 128. We begin with a learning rate of 0.1,
reduce it to 0.01 after 10 epochs.
B.2. CIFAR10
AdaptiveWe also use an adaptive scheme for adver-
sarial training in CIFAR10. Speciﬁcally, we let = 0:01on
the ﬁrst iteration of projected gradient descent, and increase
it by a factor of 1.5 every 5 iterations. Similar to MNIST,
we terminate the projected gradient descent algorithm when
either an adversarial example is found, or 50 iterations have
passed, allowing to take on values in the range [0:01;0:38].
Optimizer hyperparameters Similar to MNIST, to up-
date the model weights, we use the SGD optimizer with 0.9
momentum and 0.0005 weight decay, and batch sizes of 128.
The learning rate is also the same as in MNIST, starting at
0.1, and reducing to 0.01 after 10 epochs.
B.3. Motivation for adaptive 
A commonly asked question of models trained to be robust
against adversarial examples is “what if the adversary has a
perturbation budget of +instead of?” This is referring
to a “robustness cliff,” where a model trained against an
strong adversary has a sharp drop in robustness when
attacked by an adversary with a slightly larger budget. To
address this, we advocate for the slightly modiﬁed version
of typical adversarial training used in this work: rather thanpicking a ﬁxed and running projected gradient descent,
we instead allow for an adversarial to have a range of 2
[min;max]. To do this, we begin with =min, and then
gradually increase it by a multiplicative factor until either
an adversarial example is found or until max is reached.
While similar ideas have been used before for evaluating
model robustness, we speciﬁcally advocate for using this
schema during adversarial training . This has the advantage
of extending robustness of the classiﬁer beyond a single 
threshold, allowing a model to achieve a potentially higher
robustness threshold while not being signiﬁcantly harmed
by “impossible” adversarial examples.
C. Auxiliary experiments
In this section, we explore the space of possible parameters
that we treated as ﬁxed in the main paper. While this is not
an exhaustive search, we hope to provide some intuition as
to why we chose the parameters we did.
C.1. Effect of andC
We ﬁrst study the effect of and the cost matrix C. First,
note thatcould be any positive value. Furthermore, note
that to construct Cwe used the 2-norm which reﬂects the
1-Wasserstein metric, but in theory we could use any p-
Wasserstein metric, where the the cost of moving from pixel
(i;j)to(k;l)is 
ji jj2+jk lj2p=2. Figure 8 shows
the effects of andpon both the adversarial example and
the radius at which it was found for varying values of =
[1;10;100;500;1000] andp= [1;2;3;4;5].
We ﬁnd that it is important to ensure that is large enough,
otherwise the projection of the image is excessively blurred.
In addition to qualitative changes, smaller seems to make
it harder to ﬁnd Wasserstein adversarial examples, making
theradius go up as gets smaller. In fact, for = (1;10)
and almost all of = 100 , the blurring is so severe that no
adversarial example can be found.
In contrast, we ﬁnd that increasing pfor the Wasserstein
distance used in the cost matrix Cseems to make the images
more “blocky”. Speciﬁcally, as pgets higher tested, more
pixels seem to be moved in larger amounts. This seems to
counteract the blurring observed for low to some degree.
Naturally, the radius also grows since the overall cost of
the transport plan has gone up.
C.2. Size of local transport plan
In this section we explore the effects of different sized trans-
port plans. In the main paper, we used a 55local transport
plan, but this could easily be something else, e.g. 33or
77. We can see a comparison on the robustness of a stan-
dard and the `1robust model against these different sizedWasserstein Adversarial Examples via Projected Sinkhorn Iterations
0.0 0.5 1.0 1.5 2.0
ε radius for Wasserstein ball0.00.20.40.60.81.0Adversarial accuracyStandard 3x3
Standard 5x5
Standard 7x7
Robust 3x3
Robust 5x5
Robust 7x7
Figure 7. Adversarial accuracy of a standard model and a model
trained to be provably robust against `1attacks for different sizes
of transport plans. In most cases the size of the transport plan
doesn’t seem to matter, except for the 33local transport plan.
In this case, the adversary isn’t quite able to reach 0% accuracy for
the standard model, reaching 2.8% for for = 1:83. The adversary
is also unable to attack the robust MNIST model, bottoming out at
41% adversarial accuracy at = 1:83.
transport plans in Figure 7, using = 1000 . We observe
that while 33transport plans have difﬁculty attacking
the robust MNIST model, all other plan sizes seem to have
similar performance.
D. Provable defense
In this section we show how a Sinkhorn-like algorithm can
be derived for provable defenses, and that the resulting al-
gorithm is actually just a simpliﬁed version of the Projected
Sinkhorn iteration, which we call the Conjugate Sinkhorn
iteration (since it solves the conjugate problem).
D.1. Conjugate Sinkhorn iteration
By subtracting the same entropy term to the conjugate ob-
jective from Equation (14), we can get a problem similar to
that of projecting onto the Wasserstein ball.
minimize
z2Rn
+;2Rnn
+ zTy+X
ijijlog(ij)
subject to 1 = x
T1 =z
h;Ci:(29)
where again we’ve multiplied the objective by for con-
venience. Following the same framework as before, we
introduce dual variables (;; )where 0, to con-struct the Lagrangian as
L(z;;;; )
= zTy+X
ijijlog(ij) + (h;Ci )
+T(x 1) +T(z T1):(30)
Note that since all the terms with ijare the same, the
corresponding KKT optimality condition for ijalso re-
mains the same. The only part that changes is the optimality
condition for z, which becomes
=y (31)
Plugging the optimality conditions into the Lagrangian, we
get the following dual problem:
L(z;;;; )
=  +Tx
 X
ijexp(i) exp(  Cij 1) exp(j)
=g(; )(32)
Finally, if we minimize this with respect to and we
get exactly the same update steps as the Projected Sinkhorn
iteration. Consequently, the Conjugate Sinkhorn iteration is
identical to the Projected Sinkhorn iteration except that we
replace thestep with the ﬁxed value =y.Wasserstein Adversarial Examples via Projected Sinkhorn Iterations
ε=N/Ap=1λ=1
ε=N/Aλ=10
ε=1.83λ=100
ε=0.71λ=500
ε=0.58λ=1000
ε=N/Ap=2
ε=N/A
ε=N/A
ε=1.04
ε=0.71
ε=N/Ap=3
ε=N/A
ε=N/A
ε=1.14
ε=0.78
ε=N/Ap=4
ε=N/A
ε=N/A
ε=1.25
ε=0.53
ε=N/Ap=5
ε=N/A
ε=N/A
ε=1.25
ε=0.71
Figure 8. A plot of the adversarial examples generated with different p-Wasserstein metrics used for the cost matrix Cand different
regularization parameters . Note that when regularization is low, the image becomes blurred, and it is harder to ﬁnd adversarial examples.
In contrast, changing pdoes not seem to make any signiﬁcant changes.