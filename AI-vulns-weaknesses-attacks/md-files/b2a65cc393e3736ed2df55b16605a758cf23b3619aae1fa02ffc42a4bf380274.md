Boosting Adversarial Attacks with Momentum
Yinpeng Dong1, Fangzhou Liao1, Tianyu Pang1, Hang Su1, Jun Zhu1, Xiaolin Hu1, Jianguo Li2
1Department of Computer Science and Technology, Tsinghua Lab of Brain and Intelligence
1Beijing National Research Center for Information Science and Technology, BNRist Lab
1Tsinghua University, 100084 China
2Intel Labs China
fdyp17, liaofz13, pty17 g@mails.tsinghua.edu.cn, fsuhangss, dcszj, xlhu g@mail.tsinghua.edu.cn, jianguo.li@intel.com
Abstract
Deep neural networks are vulnerable to adversarial ex-
amples, which poses security concerns on these algorithms
due to the potentially severe consequences. Adversarial at-
tacks serve as an important surrogate to evaluate the ro-
bustness of deep learning models before they are deployed.
However, most of existing adversarial attacks can only fool
a black-box model with a low success rate. To address
this issue, we propose a broad class of momentum-based
iterative algorithms to boost adversarial attacks. By inte-
grating the momentum term into the iterative process for
attacks, our methods can stabilize update directions and
escape from poor local maxima during the iterations, re-
sulting in more transferable adversarial examples. To fur-
ther improve the success rates for black-box attacks, we ap-
ply momentum iterative algorithms to an ensemble of mod-
els, and show that the adversarially trained models with a
strong defense ability are also vulnerable to our black-box
attacks. We hope that the proposed methods will serve as
a benchmark for evaluating the robustness of various deep
models and defense methods. With this method, we won the
Ô¨Årst places in NIPS 2017 Non-targeted Adversarial Attack
and Targeted Adversarial Attack competitions.
1. Introduction
Deep neural networks (DNNs) are challenged by their
vulnerability to adversarial examples [23, 5], which are
crafted by adding small, human-imperceptible noises to
legitimate examples, but make a model output attacker-
desired inaccurate predictions. It has garnered an increasing
attention to generating adversarial examples since it helps
to identify the vulnerability of the models before they are
launched. Besides, adversarial samples also facilitate vari-
ous DNN algorithms to assess the robustness by providing
Corresponding author.
Alps: 94.39%Dog: 99.99%
Puffer: 97.99%Crab: 100.00%Figure 1. We show two adversarial examples generated by the pro-
posed momentum iterative fast gradient sign method (MI-FGSM)
for the Inception v3 [22] model. Left column : the original images.
Middle column : the adversarial noises by applying MI-FGSM for
10iterations. Right column : the generated adversarial images.
We also show the predicted labels and probabilities of these im-
ages given by the Inception v3.
more varied training data [5, 10].
With the knowledge of the structure and parameters
of a given model, many methods can successfully gener-
ate adversarial examples in the white-box manner, includ-
ing optimization-based methods such as box-constrained L-
BFGS [23], one-step gradient-based methods such as fast
gradient sign [5] and iterative variants of gradient-based
methods [9]. In general, a more severe issue of adversarial
examples is their good transferability [23, 12, 14], i.e., the
adversarial examples crafted for one model remain adver-
sarial for others, thus making black-box attacks practical in
real-world applications and posing real security issues. The
phenomenon of transferability is due to the fact that differ-
ent machine learning models learn similar decision bound-
aries around a data point, making the adversarial examples
crafted for one model also effective for others.
1arXiv:1710.06081v3 [cs.LG] 22 Mar 2018However, existing attack methods exhibit low efÔ¨Åcacy
when attacking black-box models, especially for those with
a defense mechanism. For example, ensemble adversarial
training [24] signiÔ¨Åcantly improves the robustness of deep
neural networks and most of existing methods cannot suc-
cessfully attack them in the black-box manner. This fact
largely attributes to the trade-off between the attack ability
and the transferability. In particular, the adversarial exam-
ples generated by optimization-based and iterative methods
have poor transferability [10], and thus make black-box at-
tacks less effective. On the other hand, one-step gradient-
based methods generate more transferable adversarial ex-
amples, however they usually have a low success rate for
the white-box model [10], making it ineffective for black-
box attacks. Given the difÔ¨Åculties of practical black-box
attacks, Papernot et al. [16] use adaptive queries to train a
surrogate model to fully characterize the behavior of the tar-
get model and therefore turn the black-box attacks to white-
box attacks. However, it requires the full prediction conÔ¨Å-
dences given by the target model and tremendous number
of queries, especially for large scale datasets such as Ima-
geNet [19]. Such requirements are impractical in real-world
applications. Therefore, we consider how to effectively at-
tack a black-box model without knowing its architecture
and parameters, and further, without querying.
In this paper, we propose a broad class of momen-
tum iterative gradient-based methods to boost the success
rates of the generated adversarial examples. Beyond itera-
tive gradient-based methods that iteratively perturb the in-
put with the gradients to maximize the loss function [5],
momentum-based methods accumulate a velocity vector in
the gradient direction of the loss function across iterations,
for the purpose of stabilizing update directions and escap-
ing from poor local maxima. We show that the adversarial
examples generated by momentum iterative methods have
higher success rates in both white-box and black-box at-
tacks. The proposed methods alleviate the trade-off be-
tween the white-box attacks and the transferability, and act
as a stronger attack algorithm than one-step methods [5] and
vanilla iterative methods [9].
To further improve the transferability of adversarial ex-
amples, we study several approaches for attacking an en-
semble of models, because if an adversarial example fools
multiple models, it is more likely to remain adversarial for
other black-box models [12]. We show that the adversar-
ial examples generated by the momentum iterative methods
for multiple models, can successfully fool robust models
obtained by ensemble adversarial training [24] in the black-
box manner. The Ô¨Åndings in this paper raise new security
issues for developing more robust deep learning models,
with a hope that our attacks will be used as a benchmark
to evaluate the robustness of various deep learning models
and defense methods. In summary, we make the followingcontributions:
We introduce a class of attack algorithms called mo-
mentum iterative gradient-based methods, in which we
accumulate gradients of the loss function at each iter-
ation to stabilize optimization and escape from poor
local maxima.
We study several ensemble approaches to attack multi-
ple models simultaneously, which demonstrates a pow-
erful capability of transferability by preserving a high
success rate of attacks.
We are the Ô¨Årst to show that the models obtained by
ensemble adversarial training with a powerful defense
ability are also vulnerable to the black-box attacks.
2. Backgrounds
In this section, we provide the background knowledge
as well as review the related works about adversarial attack
and defense methods. Given a classiÔ¨Åer f(x) :x2X !
y2 Y that outputs a label yas the prediction for an in-
putx, the goal of adversarial attacks is to seek an example
xin the vicinity of xbut is misclassiÔ¨Åed by the classiÔ¨Åer.
SpeciÔ¨Åcally, there are two classes of adversarial examples‚Äî
non-targeted andtargeted ones. For a correctly classiÔ¨Åed
inputxwith ground-truth label ysuch thatf(x) =y, a
non-targeted adversarial example xis crafted by adding
small noise to xwithout changing the label, but misleads
the classiÔ¨Åer as f(x)6=y; and a targeted adversarial ex-
ample aims to fool the classiÔ¨Åer by outputting a speciÔ¨Åc la-
bel asf(x) =y, whereyis the target label speciÔ¨Åed by
the adversary, and y6=y. In most cases, the Lpnorm of
the adversarial noise is required to be less than an allowed
valueaskx xkp, wherepcould be 0;1;2;1.
2.1. Attack methods
Existing approaches for generating adversarial examples
can be categorized into three groups. We introduce their
non-targeted version of attacks here, and the targeted ver-
sion can be simply derived.
One-step gradient-based approaches , such as the fast
gradient sign method (FGSM) [5], Ô¨Ånd an adversarial exam-
plexby maximizing the loss function J(x;y), whereJ
is often the cross-entropy loss. FGSM generates adversarial
examples to meet the L1norm boundkx xk1as
x=x+sign(rxJ(x;y)); (1)
whererxJ(x;y)is the gradient of the loss function w.r.t.
x. The fast gradient method (FGM) is a generalization of
FGSM to meet the L2norm boundkx xk2as
x=x+rxJ(x;y)
krxJ(x;y)k2: (2)
2Iterative methods [9] iteratively apply fast gradient
multiple times with a small step size . The iterative version
of FGSM (I-FGSM) can be expressed as:
x
0=x;x
t+1=x
t+sign(rxJ(x
t;y)): (3)
To make the generated adversarial examples satisfy the L1
(orL2) bound, one can clip x
tinto thevicinity of xor
simply set==TwithTbeing the number of iterations.
It has been shown that iterative methods are stronger white-
box adversaries than one-step methods at the cost of worse
transferability [10, 24].
Optimization-based methods [23] directly optimize the
distance between the real and adversarial examples sub-
ject to the misclassiÔ¨Åcation of adversarial examples. Box-
constrained L-BFGS can be used to solve such a problem.
A more sophisticated way [1] is solving:
arg min
xkx xkp J(x;y): (4)
Since it directly optimizes the distance between an adver-
sarial example and the corresponding real example, there
is no guarantee that the L1(L2) distance is less than the
required value. Optimization-based methods also lack the
efÔ¨Åcacy in black-box attacks just like iterative methods.
2.2. Defense methods
Among many attempts [13, 3, 15, 10, 24, 17, 11], adver-
sarial training is the most extensively investigated way to
increase the robustness of DNNs [5, 10, 24]. By injecting
adversarial examples into the training procedure, the adver-
sarially trained models learn to resist the perturbations in
the gradient direction of the loss function. However, they do
not confer robustness to black-box attacks due to the cou-
pling of the generation of adversarial examples and the pa-
rameters being trained. Ensemble adversarial training [24]
augments the training data with the adversarial samples pro-
duced not only from the model being trained, but also from
other hold-out models. Therefore, the ensemble adversari-
ally trained models are robust against one-step attacks and
black-box attacks.
3. Methodology
In this paper, we propose a broad class of momentum
iterative gradient-based methods to generate adversar-
ial examples, which can fool white-box models as well as
black-box models. In this section, we elaborate the pro-
posed algorithms. We Ô¨Årst illustrate how to integrate mo-
mentum into iterative FGSM, which induces a momentum
iterative fast gradient sign method (MI-FGSM) to generate
adversarial examples satisfying the L1norm restriction in
the non-targeted attack fashion. We then present several
methods on how to efÔ¨Åciently attack an ensemble of mod-
els. Finally, we extend MI-FGSM to L2norm bound and
targeted attacks, yielding a broad class of attack methods.Algorithm 1 MI-FGSM
Input: A classiÔ¨Åerfwith loss function J; a real example xand
ground-truth label y;
Input: The size of perturbation ; iterationsTand decay factor .
Output: An adversarial example xwithkx xk1.
1:==T;
2:g0= 0;x
0=x;
3:fort= 0toT 1do
4: Inputx
ttofand obtain the gradient rxJ(x
t;y);
5: Update gt+1by accumulating the velocity vector in the
gradient direction as
gt+1=gt+rxJ(x
t;y)
krxJ(x
t;y)k1; (6)
6: Update x
t+1by applying the sign gradient as
x
t+1=x
t+sign(gt+1); (7)
7:end for
8:return x=x
T.
3.1. Momentum iterative fast gradient sign method
The momentum method [18] is a technique for accelerat-
ing gradient descent algorithms by accumulating a velocity
vector in the gradient direction of the loss function across
iterations. The memorization of previous gradients helps to
barrel through narrow valleys, small humps and poor local
minima or maxima [4]. The momentum method also shows
its effectiveness in stochastic gradient descent to stabilize
the updates [20]. We apply the idea of momentum to gener-
ate adversarial examples and obtain tremendous beneÔ¨Åts.
To generate a non-targeted adversarial example xfrom
a real example x, which satisÔ¨Åes the L1norm bound,
gradient-based approaches seek the adversarial example by
solving the constrained optimization problem
arg max
xJ(x;y);s:t:kx xk1; (5)
whereis the size of adversarial perturbation. FGSM gen-
erates an adversarial example by applying the sign of the
gradient to a real example only once (in Eq. (1)) by the
assumption of linearity of the decision boundary around
the data point. However in practice, the linear assump-
tion may not hold when the distortion is large [12], which
makes the adversarial example generated by FGSM ‚Äúunder-
Ô¨Åts‚Äù the model, limiting its attack ability. In contrast, it-
erative FGSM greedily moves the adversarial example in
the direction of the sign of the gradient in each iteration (in
Eq. (3)). Therefore, the adversarial example can easily drop
into poor local maxima and ‚ÄúoverÔ¨Åt‚Äù the model, which is
not likely to transfer across models.
In order to break such a dilemma, we integrate momen-
tum into the iterative FGSM for the purpose of stabiliz-
ing update directions and escaping from poor local max-
ima. Therefore, the momentum-based method remains the
transferability of adversarial examples when increasing it-
3erations, and at the same time acts as a strong adversary for
the white-box models like iterative FGSM. It alleviates the
trade-off between the attack ability and the transferability,
demonstrating strong black-box attacks.
The momentum iterative fast gradient sign method (MI-
FGSM) is summarized in Algorithm 1. SpeciÔ¨Åcally, gt
gathers the gradients of the Ô¨Årst titerations with a decay
factor, deÔ¨Åned in Eq. (6). Then the adversarial example
x
tuntil thet-th iteration is perturbed in the direction of the
sign of gtwith a step size in Eq. (7). If equals to 0,
MI-FGSM degenerates to the iterative FGSM. In each iter-
ation, the current gradient rxJ(x
t;y)is normalized by the
L1distance (any distance measure is feasible) of itself, be-
cause we notice that the scale of the gradients in different
iterations varies in magnitude.
3.2. Attacking ensemble of models
In this section, we study how to attack an ensemble of
models efÔ¨Åciently. Ensemble methods have been broadly
adopted in researches and competitions for enhancing the
performance and improving the robustness [6, 8, 2]. The
idea of ensemble can also be applied to adversarial attacks,
due to the fact that if an example remains adversarial for
multiple models, it may capture an intrinsic direction that
always fools these models and is more likely to transfer to
other models at the same time [12], thus enabling powerful
black-box attacks.
We propose to attack multiple models whose logit activa-
tions1are fused together, and we call this method ensemble
in logits . Because the logits capture the logarithm relation-
ships between the probability predictions, an ensemble of
models fused by logits aggregates the Ô¨Åne detailed outputs
of all models, whose vulnerability can be easily discovered.
SpeciÔ¨Åcally, to attack an ensemble of Kmodels, we fuse
the logits as
l(x) =PK
k=1wklk(x); (8)
where lk(x)are the logits of the k-th model,wkis the en-
semble weight with wk0andPK
k=1wk= 1. The loss
functionJ(x;y)is deÔ¨Åned as the softmax cross-entropy
loss given the ground-truth label yand the logits l(x)
J(x;y) = 1ylog(softmax( l(x))); (9)
where 1yis the one-hot encoding of y. We summarize the
MI-FGSM algorithm for attacking multiple models whose
logits are averaged in Algorithm 2.
For comparison, we also introduce two alternative en-
semble schemes, one of which is already studied [12].
SpeciÔ¨Åcally, Kmodels can be averaged in predictions [12]
asp(x) =PK
k=1wkpk(x), where pk(x)is the predicted
probability of the k-th model given input x.Kmodels can
also be averaged in loss as J(x;y) =PK
k=1wkJk(x;y).
1Logits are the input values to softmax.Algorithm 2 MI-FGSM for an ensemble of models
Input: The logits of KclassiÔ¨Åers l1;l2;:::;lK; ensemble weights
w1;w2;:::;wK; a real example xand ground-truth label y;
Input: The size of perturbation ; iterationsTand decay factor .
Output: An adversarial example xwithkx xk1.
1:==T;
2:g0= 0;x
0=x;
3:fort= 0toT 1do
4: Inputx
tand output lk(x
t)fork= 1;2;:::;K ;
5: Fuse the logits as l(x
t) =PK
k=1wklk(x
t);
6: Get softmax cross-entropy loss J(x
t;y)based on l(x
t)
and Eq. (9);
7: Obtain the gradient rxJ(x
t;y);
8: Update gt+1by Eq. (6);
9: Update x
t+1by Eq. (7);
10:end for
11:return x=x
T.
In these three methods, the only difference is where to com-
bine the outputs of multiple models, but they result in differ-
ent attack abilities. We empirically Ô¨Ånd that the ensemble
in logits performs better than the ensemble in predictions
and the ensemble in loss, among various attack methods and
various models in the ensemble, which will be demonstrated
in Sec. 4.3.1.
3.3. Extensions
The momentum iterative methods can be easily general-
ized to other attack settings. By replacing the current gradi-
ent with the accumulated gradient of all previous steps, any
iterative method can be extended to its momentum variant.
Here we introduce the methods for generating adversarial
examples in terms of the L2norm bound attacks and the
targeted attacks.
To Ô¨Ånd an adversarial examples within the vicinity of a
real example measured by L2distance askx xk2, the
momentum variant of iterative fast gradient method (MI-
FGM) can be written as
x
t+1=x
t+gt+1
kgt+1k2; (10)
where gt+1is deÔ¨Åned in Eq. (6) and ==TwithTstand-
ing for the total number of iterations.
For targeted attacks, the objective for Ô¨Ånding an adver-
sarial example misclassiÔ¨Åed as a target class yis to mini-
mize the loss function J(x;y). The accumulated gradient
is derived as
gt+1=gt+J(x
t;y)
krxJ(x
t;y)k1: (11)
The targeted MI-FGSM with an L1norm bound is
x
t+1=x
t sign(gt+1); (12)
4Attack Inc-v3 Inc-v4 IncRes-v2 Res-152 Inc-v3 ens3 Inc-v3 ens4 IncRes-v2 ens
Inc-v3FGSM 72.328.2 26.2 25.3 11.3 10.9 4.8
I-FGSM 100.022.8 19.9 16.2 7.5 6.4 4.1
MI-FGSM 100.048.8 48.0 35.6 15.1 15.2 7.8
Inc-v4FGSM 32.7 61.026.6 27.2 13.7 11.9 6.2
I-FGSM 35.8 99.924.7 19.3 7.8 6.8 4.9
MI-FGSM 65.6 99.954.9 46.3 19.8 17.4 9.6
IncRes-v2FGSM 32.6 28.1 55.325.8 13.1 12.1 7.5
I-FGSM 37.8 20.8 99.622.8 8.9 7.8 5.8
MI-FGSM 69.8 62.1 99.550.6 26.1 20.9 15.7
Res-152FGSM 35.0 28.2 27.5 72.914.6 13.2 7.5
I-FGSM 26.7 22.7 21.2 98.69.3 8.9 6.2
MI-FGSM 53.6 48.9 44.7 98.522.1 21.7 12.9
Table 1. The success rates (%) of non-targeted adversarial attacks against seven models we study. The adversarial examples are crafted for
Inc-v3, Inc-v4, IncRes-v2 and Res-152 respectively using FGSM, I-FGSM and MI-FGSM.indicates the white-box attacks.
and the targeted MI-FGM with an L2norm bound is
x
t+1=x
t gt+1
kgt+1k2: (13)
Therefore, we introduce a broad class of momentum iter-
ative methods for attacks in various settings, whose effec-
tiveness is demonstrated in Sec. 4.
4. Experiments
In this section, we conduct extensive experiments on
the ImageNet dataset [19] to validate the effectiveness of
the proposed methods. We Ô¨Årst specify the experimen-
tal settings in Sec. 4.1. Then we report the results for
attacking a single model in Sec. 4.2 and an ensemble of
models in Sec. 4.3. Our methods won both the NIPS
2017 Non-targeted and Targeted Adversarial Attack com-
petitions, with the conÔ¨Ågurations introduced in Sec. 4.4.
4.1. Setup
We study seven models, four of which are normally
trained models‚ÄîInception v3 (Inc-v3) [22], Inception v4
(Inc-v4), Inception Resnet v2 (IncRes-v2) [21], Resnet v2-
152 (Res-152) [7] and the other three of which are trained
by ensemble adversarial training‚ÄîInc-v3 ens3, Inc-v3 ens4,
IncRes-v2 ens[24]. We will simply call the last three models
as ‚Äúadversarially trained models‚Äù without ambiguity.
It is less meaningful to study the success rates of attacks
if the models cannot classify the original image correctly.
Therefore, we randomly choose 1000 images belonging to
the1000 categories from the ILSVRC 2012 validation set,
which are all correctly classiÔ¨Åed by them.
In our experiments, we compare our methods to one-
step gradient-based methods and iterative methods. Since
optimization-based methods cannot explicitly control the
distance between the adversarial examples and the corre-
sponding real examples, they are not directly comparable to
ours, but they have similar properties with iterative meth-
ods as discussed in Sec. 2.1. For clarity, we only report theresults based on L1norm bound for non-targeted attacks,
and leave the results based on L2norm bound and targeted
attacks in the supplementary material. The Ô¨Åndings in this
paper are general across different attack settings.
4.2. Attacking a single model
We report in Table 1 the success rates of attacks against
the models we consider. The adversarial examples are gen-
erated for Inc-v3, Inc-v4, InvRes-v2 and Res-152 respec-
tively using FGSM, iterative FGSM (I-FGSM) and MI-
FGSM attack methods. The success rates are the misclassi-
Ô¨Åcation rates of the corresponding models with adversarial
images as inputs. The maximum perturbation is set to 16
among all experiments, with pixel value in [0;255]. The
number of iterations is 10for I-FGSM and MI-FGSM, and
the decay factor is1:0, which will be studied in Sec. 4.2.1.
From the table, we can observe that MI-FGSM remains
as a strong white-box adversary like I-FGSM since it can at-
tack a white-box model with a near 100% success rate. On
the other hand, it can be seen that I-FGSM reduces the suc-
cess rates for black-box attacks than one-step FGSM. But
by integrating momentum, our MI-FGSM outperforms both
FGSM and I-FGSM in black-box attacks signiÔ¨Åcantly. It
obtains more than 2times of the success rates than I-FGSM
in most black-box attack cases, demonstrating the effective-
ness of the proposed algorithm. We show two adversarial
images in Fig. 1 generated for Inc-v3.
It should be noted that although our method greatly im-
proves the success rates for black-box attacks, it is still inef-
fective for attacking adversarially trained models ( e.g., less
than16% for IncRes-v2 ens) in the black-box manner. Later
we show that ensemble-based approaches greatly improve
the results in Sec. 4.3. Next, we study several aspects of
MI-FGSM that are different from vanilla iterative methods,
to further explain why it performs well in practice.
4.2.1 Decay factor 
The decay factor plays a key role for improving the suc-
cess rates of attacks. If = 0, momentum-based iterative
5Decay Factor 700.20.40.60.811.21.41.61.82Success Rate (%)020406080100Inc-v3Inc-v4IncRes-v2Res-152Figure 2. The success rates (%) of the adversarial examples gener-
ated for Inc-v3 against Inc-v3 (white-box), Inc-v4, IncRes-v2 and
Res-152 (black-box), with ranging from 0:0to2:0.
Number of Iterations12345678910Success Rate (%)020406080100Inc-v3 vs. MI-FGSMInc-v3 vs. I-FGSMInc-v4 vs. MI-FGSMInc-v4 vs. I-FGSMIncRes-v2 vs. MI-FGSMIncRes-v2 vs. I-FGSMRes-152 vs. MI-FGSMRes-152 vs. I-FGSM
Figure 3. The success rates (%) of the adversarial examples gener-
ated for Inc-v3 model against Inc-v3 (white-box), Inc-v4, IncRes-
v2 and Res-152 (black-box). We compare the results of I-FGSM
and MI-FGSM with different iterations. Please note that the curves
of Inc-v3 vs. MI-FGSM and Inc-v3 vs. I-FGSM overlap together.
methods trivially turn to vanilla iterative methods. There-
fore, we study the appropriate value of the decay factor.
We attack Inc-v3 model by MI-FGSM with the perturba-
tion= 16 , the number of iterations 10, and the de-
cay factor ranging from 0:0to2:0with a granularity 0:1.
We show the success rates of the generated adversarial ex-
amples against Inc-v3, Inc-v4, IncRes-v2 and Res-152 in
Fig. 2. The curve of the success rate against a black-box
model is unimodal whose maximum value is obtained at
around= 1:0. When= 1:0, another interpretation of
gtdeÔ¨Åned in Eq. (6) is that it simply adds up all previous
gradients to perform the current update.
4.2.2 The number of iterations
We then study the effect of the number of iterations on the
success rates when using I-FGSM and MI-FGSM. We adopt
the same hyper-parameters ( i.e.,= 16 ,= 1:0) for at-
tacking Inc-v3 model with the number of iterations ranging
from 1to10, and then evaluate the success rates of adversar-
ial examples against Inc-v3, Inc-v4, IncRes-v2 and Res-152
Iterations2468101214161820Cosine Similarity0.860.880.90.920.940.960.981I-FGSMMI-FGSMFigure 4. The cosine similarity of two successive perturbations in
I-FGSM and MI-FGSM when attacking Inc-v3 model. The results
are averaged over 1000 images.
models, with the results shown in Fig. 3.
It can be observed that when increasing the number of
iterations, the success rate of I-FGSM against a black-box
model gradually decreases, while that of MI-FGSM main-
tains at a high value. The results prove our argument that the
adversarial examples generated by iterative methods eas-
ily overÔ¨Åt a white-box model and are not likely to trans-
fer across models. But momentum-based iterative methods
help to alleviate the trade-off between the white-box attacks
and the transferability, thus demonstrating a strong attack
ability for white-box and black-box models simultaneously.
4.2.3 Update directions
To interpret why MI-FGSM demonstrates better transfer-
ability, we further examine the update directions given by I-
FGSM and MI-FGSM along the iterations. We calculate the
cosine similarity of two successive perturbations and show
the results in Fig. 4 when attacking Inc-v3. The update di-
rection of MI-FGSM is more stable than that of I-FGSM
due to the larger value of cosine similarity in MI-FGSM.
Recall that the transferability comes from the fact that
models learn similar decision boundaries around a data
point [12]. Although the decision boundaries are similar,
they are unlikely the same due to the highly non-linear
structure of DNNs. So there may exist some exceptional
decision regions around a data point for a model (holes as
shown in Fig. 4&5 in [12]), which are hard to transfer to
other models. These regions correspond to poor local max-
ima in the optimization process and the iterative methods
can easily trap into such regions, resulting in less transfer-
able adversarial examples. On the other hand, the stabilized
update directions obtained by the momentum methods as
observed in Fig. 4 can help to escape from these excep-
tional regions, resulting in better transferability for adver-
sarial attacks. Another interpretation is that the stabilized
updated directions make the L2norm of the perturbations
larger, which may be helpful for the transferability.
6Ensemble methodFGSM I-FGSM MI-FGSM
Ensemble Hold-out Ensemble Hold-out Ensemble Hold-out
-Inc-v3Logits 55.7 45.7 99.7 72.1 99.6 87.9
Predictions 52.3 42.7 95.1 62.7 97.1 83.3
Loss 50.5 42.2 93.8 63.1 97.0 81.9
-Inc-v4Logits 56.1 39.9 99.8 61.0 99.5 81.2
Predictions 50.9 36.5 95.5 52.4 97.1 77.4
Loss 49.3 36.2 93.9 50.2 96.1 72.5
-IncRes-v2Logits 57.2 38.8 99.5 54.4 99.5 76.5
Predictions 52.1 35.8 97.1 46.9 98.0 73.9
Loss 50.7 35.2 96.2 45.9 97.4 70.8
-Res-152Logits 53.5 35.9 99.6 43.5 99.6 69.6
Predictions 51.9 34.6 99.9 41.0 99.8 67.0
Loss 50.4 34.1 98.2 40.1 98.8 65.2
Table 2. The success rates (%) of non-targeted adversarial attacks of three ensemble methods. We report the results for an ensemble of
white-box models and a hold-out black-box target model. We study four models‚ÄîInc-v3, Inc-v4, IncRes-v2 and Res-152. In each row, ‚Äú-‚Äù
indicates the name of the hold-out model and the adversarial examples are generated for the ensemble of the other three models by FGSM,
I-FGSM and MI-FGSM respectively. Ensemble in logits consistently outperform other methods.
The size of perturbation 01471013161922252831343740Success Rate (%)0102030405060708090100Inc-v3 vs. MI-FGSMRes-152 vs. MI-FGSMInc-v3 vs. I-FGSMRes-152 vs. I-FGSMInc-v3 vs. FGSMRes-152 vs. FGSM
Figure 5. The success rates (%) of the adversarial examples gen-
erated for Inc-v3 against Inc-v3 (white-box) and Res-152 (black-
box). We compare the results of FGSM, I-FGSM and MI-FGSM
with different size of perturbation. The curves of Inc-v3 vs. MI-
FGSM and Inc-v3 vs. I-FGSM overlap together.
4.2.4 The size of perturbation
We Ô¨Ånally study the inÔ¨Çuence of the size of adversarial per-
turbation on the success rates. We attack Inc-v3 model by
FGSM, I-FGSM and MI-FGSM with ranging from 1to
40with the image intensity [0;255], and evaluate the per-
formance on the white-box model Inc-v3 and a black-box
model Res-152. In our experiments, we set the step size 
in I-FGSM and MI-FGSM to 1, so the number of iterations
grows linearly with the size of perturbation . The results
are shown in Fig. 5.
For the white-box attack, iterative methods reach the
100% success rate soon, but the success rate of one-step
FGSM decreases when the perturbation is large. The phe-
nomenon largely attributes to the inappropriate assumption
of the linearity of the decision boundary when the pertur-
bation is large [12]. For the black-box attacks, although
the success rates of these three methods grow linearly with
the size of perturbation, MI-FGSM‚Äôs success rate growsfaster. In other words, to attack a black-box model with a
required success rate, MI-FGSM can use a smaller perturba-
tion, which is more visually indistinguishable for humans.
4.3. Attacking an ensemble of models
In this section, we show the experimental results of at-
tacking an ensemble of models. We Ô¨Årst compare the three
ensemble methods introduced in Sec. 3.2, and then demon-
strate that the adversarially trained models are vulnerable to
our black-box attacks.
4.3.1 Comparison of ensemble methods
We compare the ensemble methods for attacks in this sec-
tion. We include four models in our study, which are Inc-
v3, Inc-v4, IncRes-v2 and Res-152. In our experiments, we
keep one model as the hold-out black-box model and attack
an ensemble of the other three models by FGSM, I-FGSM
and MI-FGSM respectively, to fully compare the results of
the three ensemble methods, i.e., ensemble in logits, ensem-
ble in predictions and ensemble in loss. We set to16, the
number of iterations in I-FGSM and MI-FGSM to 10,in
MI-FGSM to 1:0, and the ensemble weights equally. The
results are shown in Table 2.
It can be observed that the ensemble in logits outper-
forms the ensemble in predictions and the ensemble in loss
consistently among all the attack methods and different
models in the ensemble for both the white-box and black-
box attacks. Therefore, the ensemble in logits scheme is
more suitable for adversarial attacks.
Another observation from Table 2 is that the adversar-
ial examples generated by MI-FGSM transfer at a high rate,
enabling strong black-box attacks. For example, by attack-
ing an ensemble of Inc-v4, IncRes-v2 and Res-152 fused
in logits without Inc-v3, the generated adversarial examples
can fool Inc-v3 with a 87:9%success rate. Normally trained
models show their great vulnerability against such an attack.
7Attack Ensemble Hold-out
-Inc-v3 ens3FGSM 36.1 15.4
I-FGSM 99.6 18.6
MI-FGSM 99.6 37.6
-Inc-v3 ens4FGSM 33.0 15.0
I-FGSM 99.2 18.7
MI-FGSM 99.3 40.3
-IncRes-v2 ensFGSM 36.2 6.4
I-FGSM 99.5 9.9
MI-FGSM 99.7 23.3
Table 3. The success rates (%) of non-targeted adversarial attacks
against an ensemble of white-box models and a hold-out black-box
target model. We include seven models‚ÄîInc-v3, Inc-v4, IncRes-
v2, Res-152, Inc-v3 ens3, Inc-v3 ens4and IncRes-v2 ens. In each row,
‚Äú-‚Äù indicates the name of the hold-out model and the adversarial
examples are generated for the ensemble of the other six models.
4.3.2 Attacking adversarially trained models
To attack the adversarially trained models in the black-box
manner, we include all seven models introduced in Sec. 4.1.
Similarly, we keep one adversarially trained model as the
hold-out target model to evaluate the performance in the
black-box manner, and attack the rest six model in an en-
semble, whose logits are fused together with equal ensem-
ble weights. The perturbation is16and the decay factor 
is1:0. We compare the results of FGSM, I-FGSM and MI-
FGSM with 20iterations. The results are shown in Table 3.
It can be seen that the adversarially trained models
also cannot defend our attacks effectively, which can fool
Inc-v3 ens4by more than 40% of the adversarial examples.
Therefore, the models obtained by ensemble adversarial
training, the most robust models trained on the ImageNet
as far as we are concerned, are vulnerable to our attacks in
the black-box manner, thus causing new security issues for
developing algorithms to learn robust deep learning models.
4.4. Competitions
There are three sub-competitions in the NIPS 2017 Ad-
versarial Attacks and Defenses Competition, which are the
Non-targeted Adversarial Attack, Targeted Adversarial At-
tack and Defense Against Adversarial Attack. The organiz-
ers provide 5000 ImageNet-compatible images for evaluat-
ing the attack and defense submissions. For each attack,
one adversarial example is generated for each image with
the size of perturbation ranging from 4to16(speciÔ¨Åed by
the organizers), and all adversarial examples run through all
defense submissions to get the Ô¨Ånal score. We won the Ô¨Årst
places in both the non-targeted attack and targeted attack
by the method introduced in this paper. We will specify the
conÔ¨Ågurations in our submissions.
For the non-targeted attack2, we implement the MI-
FGSM for attacking an ensemble of Inc-v3, Inc-v4, IncRes-
2Source code is available at https://github.com/dongyp13/
Non-Targeted-Adversarial-Attacks .v2, Res-152, Inc-v3 ens3, Inc-v3 ens4, IncRes-v2 ensand Inc-
v3adv[10]. We adopt the ensemble in logits scheme. The
ensemble weights are set as 1=7:25equally for the Ô¨Årst seven
models and 0:25=7:25for Inc-v3 adv. The number of iterations
is10and the decay factor is1:0.
For the targeted attack3, we build two graphs for attacks.
If the size of perturbation is smaller than 8, we attack Inc-v3
and IncRes-v2 enswith ensemble weights 1=3and 2=3; oth-
erwise we attack an ensemble of Inc-v3, Inc-v3 ens3, Inc-
v3ens4, IncRes-v2 ensand Inc-v3 advwith ensemble weights
4=11;1=11;1=11;4=11and 1=11. The number of iterations is 40
and20respectively, and the decay factor is also 1:0.
5. Discussion
Taking a different perspective, we think that Ô¨Ånding an
adversarial example is an analogue to training a model and
the transferability of the adversarial example is also an ana-
logue to the generalizability of the model. By taking a meta
view, we actually ‚Äútrain‚Äù an adversarial example given a set
of models as training data. In this way, the improved trans-
ferability obtained by the momentum and ensemble meth-
ods is reasonable because the generalizability of a model
is usually improved by adopting the momentum optimizer
or training on more data. And we think that other tricks
(e.g., SGD) for enhancing the generalizability of a model
could also be incorporated into adversarial attacks for bet-
ter transferability.
6. Conclusion
In this paper, we propose a broad class of momentum-
based iterative methods to boost adversarial attacks, which
can effectively fool white-box models as well as black-
box models. Our methods consistently outperform one-step
gradient-based methods and vanilla iterative methods in the
black-box manner. We conduct extensive experiments to
validate the effectiveness of the proposed methods and ex-
plain why they work in practice. To further improve the
transferability of the generated adversarial examples, we
propose to attack an ensemble of models whose logits are
fused together. We show that the models obtained by en-
semble adversarial training are vulnerable to our black-box
attacks, which raises new security issues for the develop-
ment of more robust deep learning models.
Acknowledgements
The work is supported by the National NSF of China
(Nos. 61620106010, 61621136008, 61332007, 61571261 and
U1611461), Beijing Natural Science Foundation (No. L172037),
Tsinghua Tiangong Institute for Intelligent Computing and the
NVIDIA NV AIL Program, and partially funded by Microsoft Re-
search Asia and Tsinghua-Intel Joint Research Institute.
3Source code is available at https://github.com/dongyp13/
Targeted-Adversarial-Attacks .
8References
[1] N. Carlini and D. Wagner. Towards evaluating the robustness
of neural networks. In IEEE Symposium on Security and
Privacy , 2017. 3
[2] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.
Ensemble selection from libraries of models. In ICML , 2004.
4
[3] Y . Dong, H. Su, J. Zhu, and F. Bao. Towards interpretable
deep neural networks by leveraging adversarial examples.
arXiv preprint arXiv:1708.05493 , 2017. 3
[4] W. Duch and J. Korczak. Optimization and global minimiza-
tion methods suitable for neural networks. Neural computing
surveys , 2:163‚Äì212, 1998. 3
[5] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and
harnessing adversarial examples. In ICLR , 2015. 1, 2, 3
[6] L. K. Hansen and P. Salamon. Neural network ensembles.
IEEE transactions on pattern analysis and machine intelli-
gence , 12(10):993‚Äì1001, 1990. 4
[7] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks. In ECCV , 2016. 5
[8] A. Krogh and J. Vedelsby. Neural network ensembles, cross
validation and active learning. In NIPS , 1994. 4
[9] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial exam-
ples in the physical world. arXiv preprint arXiv:1607.02533 ,
2016. 1, 2, 3
[10] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial ma-
chine learning at scale. In ICLR , 2017. 1, 2, 3, 8
[11] Y . Li and Y . Gal. Dropout inference in bayesian neural net-
works with alpha-divergences. In ICML , 2017. 3
[12] Y . Liu, X. Chen, C. Liu, and D. Song. Delving into transfer-
able adversarial examples and black-box attacks. In ICLR ,
2017. 1, 2, 3, 4, 6, 7, 10
[13] J. H. Metzen, T. Genewein, V . Fischer, and B. Bischoff. On
detecting adversarial perturbations. In ICLR , 2017. 3
[14] S. M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and
P. Frossard. Universal adversarial perturbations. In CVPR ,
2017. 1
[15] T. Pang, C. Du, and J. Zhu. Robust deep learning via reverse
cross-entropy training and thresholding test. arXiv preprint
arXiv:1706.00633 , 2017. 3
[16] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik,
and A. Swami. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia Confer-
ence on Computer and Communications Security , 2017. 2
[17] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami.
Distillation as a defense to adversarial perturbations against
deep neural networks. In IEEE Symposium on Security and
Privacy , 2016. 3
[18] B. T. Polyak. Some methods of speeding up the convergence
of iteration methods. USSR Computational Mathematics and
Mathematical Physics , 4(5):1‚Äì17, 1964. 3
[19] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al. Imagenet large scale visual recognition challenge.
International Journal of Computer Vision , 115(3):211‚Äì252,
2015. 2, 5[20] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the
importance of initialization and momentum in deep learning.
InICML , 2013. 3
[21] C. Szegedy, S. Ioffe, V . Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning. In AAAI , 2017. 5
[22] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
CVPR , 2016. 1, 5
[23] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus. Intriguing properties of neural
networks. In ICLR , 2014. 1, 3
[24] F. Tram `er, A. Kurakin, N. Papernot, D. Boneh, and P. Mc-
Daniel. Ensemble adversarial training: Attacks and defenses.
InICLR , 2018. 2, 3, 5
9Appendix
In this supplementary material, we provide more results in our
experiments. In Sec. A, we report the success rates of non-targeted
attacks based on L2norm bound. In Sec. B, we provide the results
of targeted attacks. The experiements consistently demonstrate the
effectiveness of the proposed momentum-based methods.
A. Non-targeted attacks based on L2norm
bound
We Ô¨Årst perform non-targeted attacks based on L2norm bound.
Since theL2distance between an adversarial example and a real
example is deÔ¨Åned as
kx xk2=vuutNX
i=1(x
i xi)2; (14)
whereNis the dimension of input xandx, the distance measure
depends onN. For example, if the distance of each dimension of
an adversarial example and a real example is jx
i xij=, theL2
norm isp
Nbetween them while the L1norm is. Therefore,
we set theL2norm bound as 16p
Nin our experiments, where N
is the dimension of the input to a network.
Attack Ensemble Hold-out
-Inc-v3FGM 47.3 52.7
I-FGM 99.1 65.3
MI-FGM 99.2 89.7
-Inc-v4FGM 47.2 49.3
I-FGM 99.3 56.7
MI-FGM 99.4 88.0
-IncRes-v2FGSM 47.3 50.4
I-FGSM 99.4 54.3
MI-FGSM 99.5 86.1
-Res-152FGM 47.6 46.6
I-FGM 99.0 44.7
MI-FGM 99.5 81.4
-Inc-v3 ens3FGM 51.8 35.4
I-FGM 99.8 29.5
MI-FGM 99.6 59.8
-Inc-v3 ens4FGM 51.2 37.5
I-FGM 99.2 36.4
MI-FGM 99.7 66.5
-IncRes-v2 ensFGSM 54.4 32.4
I-FGSM 99.2 19.9
MI-FGSM 99.8 56.4
Table 4. The success rates (%) of non-targeted adversarial attacks
based onL2norm bound against an ensemble of white-box models
and a hold-out black-box target model. In each row, ‚Äú-‚Äù indicates
the name of the hold-out model and the adversarial examples are
generated for the ensemble of the other six models.A.1. Attacking a single model
We include seven networks in this section, which are Inc-v3,
Inc-v4, IncRes-v2, Res-152, Inc-v3 ens3, Inc-v3 ens4and IncRes-
v2ens. We generate adversarial examples for Inc-v3, Inc-v4,
IncRes-v2 and Res-152 respectively, and measure the success rates
of attacks on all models. We compare three attack methods, which
are the fast gradient method (FGM, deÔ¨Åned in Eq. (2)), iterative
FGM (I-FGM) and momentum iterative FGM (MI-FGM, deÔ¨Åned
in Eq. (10)). We set the number of iterations to 10in I-FGM and
MI-FGM, and the decay factor to 1:0in MI-FGM.
The results are shown in Table 5 (See next page.). We can
also see that MI-FGM attacks a white-box model with a near
100% success rate as I-FGM, and outperforms FGM and I-FGM
in black-box attacks signiÔ¨Åcantly. The conclusions are similar to
those ofL1norm bound experiments, which consistently demon-
strate the effectiveness of the proposed momentum-based iterative
methods.
A.2. Attacking an ensemble of models
In this experiments, we also include Inc-v3, Inc-v4, IncRes-v2,
Res-152, Inc-v3 ens3, Inc-v3 ens4and IncRes-v2 ensmodels for our
study. We keep one model as the hold-out black-box model and
attack an ensemble of the other six models by FGM, I-FGM and
MI-FGM respectively. We set the number of iterations to 20in
I-FGM and MI-FGM, the decay factor to 1:0in MI-FGM, and the
ensemble weights to1=6equally.
We show the results in Table 4. Iterative methods including I-
FGM and MI-FGM can obtain a near 100% success rate for an en-
semble of white-box models. And MI-FGM can attack a black-box
model with a much higher success rate, showing the good trans-
ferability of the adversarial examples generated by MI-FGM. For
adversarially trained models, MI-FGM can fool them with about
60% success rates, revealing the great vulnerability of the adver-
sarially trained models against our black-box attacks.
B. Targeted attacks
B.1.L1norm bound
Targeted attacks are much more difÔ¨Åcult than non-targeted at-
tacks in the black-box manner, since they require the black-box
model to output the speciÔ¨Åc target label. For DNNs trained on a
dataset with thousands of output categories such as the ImageNet
dataset, Ô¨Ånding targeted adversarial examples by only one model
to fool a black-box model is impossible [12]. Thus we perform
targeted attacks by integrating the ensemble-based approach.
We show the results in Table 6, where the success rate is mea-
sured by the percentage of the adversarial examples that are clas-
siÔ¨Åed as the target label by the model. Similar to the experimental
settings in Sec. 4.3.2, we keep one model to test the performance
of black-box attacks, with the targeted adversarial examples gen-
erated for the ensemble of the other six models. We set the size of
perturbation to48, decay factor to1:0and the number of iter-
ations to 20for I-FGSM and MI-FGSM. We can see that one-step
FGSM can hardly attack the ensemble of models as well as the
target black-box models. The success rates of the adversarial ex-
amples generated by MI-FGSM are close to 100% for white-box
models and higher than 10% for normally trained black-box mod-
10Attack Inc-v3 Inc-v4 IncRes-v2 Res-152 Inc-v3 ens3 Inc-v3 ens4 IncRes-v2 ens
Inc-v3FGM 76.241.0 43.1 41.3 34.6 34.9 26.2
I-FGM 100.039.9 36.4 27.5 17.5 19.2 10.9
MI-FGM 100.067.6 66.3 56.1 44.4 45.5 33.9
Inc-v4FGM 47.3 63.137.3 39.0 35.3 33.9 27.7
I-FGM 52.8 100.042.0 33.5 21.9 19.9 13.8
MI-FGM 76.9 100.069.6 59.7 51.2 51.0 39.4
IncRes-v2FGM 48.2 38.9 60.439.8 36.6 35.5 30.5
I-FGM 56.0 47.5 99.636.9 27.5 22.9 18.7
MI-FGM 81.7 75.8 99.666.9 62.7 57.7 58.8
-Res-152FGM 50.8 40.7 42.0 75.136.5 36.0 31.6
I-FGM 47.6 43.9 43.9 99.432.7 32.3 25.2
MI-FGM 71.3 65.5 64.3 99.656.7 55.4 51.5
Table 5. The success rates (%) of non-targeted adversarial attacks based on L2norm bound against all models. The adversarial examples
are crafted for Inc-v3, Inc-v4, IncRes-v2 and Res-152 respectively using FGM, I-FGM and MI-FGM.indicates the white-box attacks.
Attack Ensemble Hold-out
-Inc-v3FGSM 0.5 0.5
I-FGSM 99.6 9.0
MI-FGSM 99.5 17.6
-Inc-v4FGSM 0.3 0.4
I-FGSM 99.9 7.0
MI-FGSM 99.8 15.6
-IncRes-v2FGSM 0.4 0.2
I-FGSM 99.9 7.3
MI-FGSM 99.8 16.1
-Res-152FGSM 0.1 0.5
I-FGSM 99.6 3.3
MI-FGSM 99.5 11.4
-Inc-v3 ens3FGSM 0.3 0.1
I-FGSM 99.7 0.1
MI-FGSM 99.7 0.5
-Inc-v3 ens4FGSM 0.2 0.1
I-FGSM 99.9 0.4
MI-FGSM 99.8 0.9
-IncRes-v2 ensFGSM 0.5 0.1
I-FGSM 99.7 0.1
MI-FGSM 99.8 0.2
Table 6. The success rates (%) of targeted adversarial attacks based
onL1norm bound against an ensemble of white-box models and
a hold-out black-box target model. In each row, ‚Äú-‚Äù indicates the
name of the hold-out model and the adversarial examples are gen-
erated for the ensemble of the other six models.
els. Unfortunately, it cannot effectively generate targeted adversar-
ial examples to fool adversarially trained models, which remains
an open issue for future researches.
B.2.L2norm bound
We draw similar conclusions for targeted attacks based on L2
norm bound. In our experiments, we also include Inc-v3, Inc-v4,Attack Ensemble Hold-out
-Inc-v3FGM 0.7 0.4
I-FGM 99.7 17.8
MI-FGM 99.5 21.0
-Inc-v4FGM 0.7 0.5
I-FGM 99.9 15.2
MI-FGM 99.8 21.8
-IncRes-v2FGM 0.7 0.7
I-FGM 99.8 16.4
MI-FGM 99.9 21.7
-Res-152FGM 0.5 0.4
I-FGM 99.5 9.2
MI-FGM 99.6 17.4
-Inc-v3 ens3FGM 0.6 0.2
I-FGM 99.9 0.7
MI-FGM 99.6 1.6
-Inc-v3 ens4FGM 0.5 0.2
I-FGM 99.7 1.7
MI-FGM 100.0 2.0
-IncRes-v2 ensFGM 0.6 0.4
I-FGM 99.6 0.5
MI-FGM 99.8 1.9
Table 7. The success rates (%) of targeted adversarial attacks based
onL2norm bound against an ensemble of white-box models and
a hold-out black-box target model. In each row, ‚Äú-‚Äù indicates the
name of the hold-out model and the adversarial examples are gen-
erated for the ensemble of the other six models.
IncRes-v2, Res-152, Inc-v3 ens3, Inc-v3 ens4and IncRes-v2 ensmod-
els. We keep one model as the hold-out black-box model and
attack an ensemble of the other six models with equal ensemble
weights by FGM, I-FGM and MI-FGM respectively. We set the
maximum perturbation to48p
NwhereNis the dimension of
inputs, the number of iterations to 20in I-FGM and MI-FGM, and
the decay factor to 1:0in MI-FGM. We report the success rates
11of adversarial examples against the white-box ensemble of models
and the black-box target model in Table 7. MI-FGM can easily
fool white-box models, but it cannot fool the adversarially trained
models effectively in the targeted black-box attacks.
12