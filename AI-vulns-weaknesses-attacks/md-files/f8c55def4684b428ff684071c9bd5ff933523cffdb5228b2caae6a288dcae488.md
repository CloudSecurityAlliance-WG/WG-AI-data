Over-the-Air Adversarial Flickering Attacks against Video Recognition
Networks
Roi Pony1\*, Itay Naeh2\*, Shie Mannor1,3
1Department of Electrical Engineering, Technion Institute of Technology, Haifa, Israel
2Rafael - Advanced Defense Systems Ltd., Israel
3Nvidia Research
roipony@gmail.com, itay@naeh.us, shie@technion.ac.il
Abstract
Deep neural networks for video classiﬁcation, just like
image classiﬁcation networks, may be subjected to adver-
sarial manipulation. The main difference between image
classiﬁers and video classiﬁers is that the latter usually
use temporal information contained within the video. In
this work we present a manipulation scheme for fooling
video classiﬁers by introducing a ﬂickering temporal per-
turbation that in some cases may be unnoticeable by hu-
man observers and is implementable in the real world. Af-
ter demonstrating the manipulation of action classiﬁcation
of single videos, we generalize the procedure to make uni-
versal adversarial perturbation, achieving high fooling ra-
tio. In addition, we generalize the universal perturbation
and produce a temporal-invariant perturbation, which can
be applied to the video without synchronizing the pertur-
bation to the input. The attack was implemented on sev-
eral target models and the transferability of the attack was
demonstrated. These properties allow us to bridge the gap
between simulated environment and real-world application,
as will be demonstrated in this paper for the ﬁrst time for
an over-the-air ﬂickering attack.
1. Introduction
In recent years, Deep Neural Networks (DNNs) have
shown phenomenal performance in a wide range of tasks,
such as image classiﬁcation [13], object detection [19], se-
mantic segmentation [23] etc. Despite their success, DNNs
have been found vulnerable to adversarial attacks. Many
works [28, 5, 18] have shown that a small (sometimes im-
perceptible) perturbation added to an image, can make a
given DNNs prediction false. These ﬁndings have raised
\*Equal contribution
”juggling balls”
”skydiving”action
recognition
ˆX predictionδ
X
(a) Diagram of a Flickering Adversarial Attack in a simulated en-
vironment (digital).
”juggling balls”
”skydiving”action
recognition
ˆX predictionδ
X
(b) Diagram of an Over-the-Air Flickering Adversarial Attack in
the real-world (physical).
Figure 1: Top ﬁgure shows the attack diagram in the digital
domain performed by adding a uniform RGB perturbation
to the attacked video. Bottom ﬁgure shows the modeling of
the digitally-developed attack into the real-world by trans-
mitting the perturbation in the scene using a smart RGB led
bulb.
many concerns, particularly for critical systems such as face
recognition systems [26], surveillance cameras [25], au-
tonomous vehicles, and medical applications [17]. In recent
years most of the attention was given to the study of adver-
sarial patterns in images and less in video action recogni-
tion. Only in the past two years works on adversarial video
attacks were published [34, 8, 35, 10], even though DNNs
have been applied to video-based tasks for several years,
in particular video action recognition [2, 33, 4]. In video
action recognition networks temporal information is of the
essence in categorizing actions, in addition to per-frame im-arXiv:2002.05123v4 [cs.LG] 4 Jun 2021age classiﬁcation. In some of the proposed attacks the em-
phasis was, beyond adversarial categorization, the sparsity
of the perturbation. In our work, we consider adversarial
attacks against video action recognition under a white-box
setting, with an emphasis on the imperceptible nature of
the perturbation in the spatio-temporal domain to the hu-
man observer and implementability of the generalized ad-
versarial perturbation in the real-world. We introduce ﬂick-
ering perturbations by applying a uniform RGB perturba-
tion to each frame, thus constructing a temporal adversar-
ial pattern. Unlike previous works, in our case sparsity of
the pattern is undesirable, because it helps the adversarial
perturbation to be detectable by human observers for its un-
natural pattern, and to image based adversarial perturbation
detectors for the exact same reason. The adversarial pertur-
bation presented in this work does not contain any spatial
information on a single frame other than a constant offset.
This type of perturbation often occurs in natural videos by
changing lighting conditions, scene changes, etc. In this
paper, we aim to attack the video action recognition task
[11]. For the targeted model we focus on the I3D [2] model
(Speciﬁcally we attack the RGB stream of the model, rather
than on the easier to inﬂuence optical ﬂow stream) based
on InceptionV1 [27] and we expand our experiments to ad-
ditional models from [30]. The attacked models trained on
the Kinetics-400 Human Action Video Dataset [11].
In order to make the adversarial perturbation unnotice-
able by human observers, we reduce the thickness and tem-
poral roughness of the adversarial perturbation, which will
be deﬁned later in this paper. In order to do so we apply
two regularization terms during the optimization process,
each corresponds to a different effect of the perceptibly of
the adversarial pattern. In addition, we introduce a modi-
ﬁed adversarial-loss function that allows better integration
of these regularization terms with the adversarial loss.
We will ﬁrst focus on the I3D [2] network and introduce
a ﬂickering attack on a single video and present the trade-
off between the different regularization terms. We con-
struct universal perturbations that generalize over classes
and achieve 93% fooling ratio. Another signiﬁcant fea-
ture of our proposed method is time invariant perturbations
that can be applied to the classiﬁer without synchroniza-
tion. This makes the perturbation relevant for real world
scenarios, since frame synchronization is rarely possible.
We show the effectiveness of the ﬂickering attack on other
models [30] and the inter-model transferability, and ﬁnally
demonstrate the over-the-air ﬂickering attack in a real world
scenario for the ﬁrst time. A diagram of the digital attack
and the over-the-air attack pipelines are shown in Figure 1.
The main contributions of this work are:
• A methodology for developing ﬂickering adversarial
attacks against video action recognition networks that
incorporates a new type of regularization for affectingthe visibility of the adversarial pattern.
• A universal time-invariant adversarial perturbation that
does not require frame synchronization.
• Adversarial attacks that are transferable between dif-
ferent networks.
• Adversarial attacks that are implementable using tem-
poral perturbations.
The paper is organized as follows: We brieﬂy review
related work and present the ﬂickering adversarial attack.
Then we show experimental results and the generalization
of the attack. Finally, we present real world examples of the
ﬂickering adversarial attacks, followed by conclusions and
future work. We encourage the readers to view the attack
videos1, over-the-air scene-based attack videos2, and over-
the-air universal attack videos3. Our code can be found in
the following repository4.
2. Related Work
2.1. Video Action Recognition
With deep Convolutional Neural Networks (CNNs)
achieving state-of-the-art performance on image recogni-
tion tasks, many works propose to adapt this achievement
to video-based computer vision tasks. The most straight-
forward approach for achieving this is to add temporally-
recurrent layers such as LSTM [22] models to traditional
2D-CNNs. This way, long-term temporal dependencies
can be assigned to spatial features [32, 24]. Another ap-
proach implemented in C3D [9, 29, 31] extends the 2D
CNNs (image-based) to 3D CNNs (video-based) kernels
and learns hierarchical spatio-temporal representations di-
rectly from raw videos. Despite the simplicity of this ap-
proach, it is very difﬁcult to train such networks due to
their huge parameter space. To address this, [2] proposes
the Inﬂated 3D CNN (I3D) with inﬂated 2D pre-trained ﬁl-
ters [21]. In addition to the RGB pipeline, optical ﬂow is
also useful for temporal information encoding, and indeed
several architectures greatly improved their performance
by incorporating an optical-ﬂow stream [2]. [30] demon-
strated the advantages of 3D CNNs over 2D CNNs within
the framework of residual learning, proposing factorization
of the 3D convolutional ﬁlters into separate spatial and tem-
poral components.
2.2. Adversarial Attack on Video Models
The research of the vulnerability of video-based classi-
ﬁers to adversarial attacks emerged only in the past years.
1https://bit.ly/Flickering\_Attack\_videos
2https://bit.ly/Over\_the\_Air\_scene\_based\_videos
3https://bit.ly/Over\_the\_Air\_videos
4https://bit.ly/Flickering\_Attack\_CodeThe following attacks were performed under the white-box
attack settings: [34] were the ﬁrst to investigate a white-
box attack on video action recognition. They proposed
anL2;1norm based optimization algorithm to compute
sparse adversarial perturbations, focusing on networks with
a CNN+RNN architecture in order to investigate the propa-
gation properties of perturbations. [16] generated an ofﬂine
universal perturbation using a GAN-based model that they
applied to the learned model on unseen input for real-time
video recognition models. [20] proposed a nonlinear adver-
sarial perturbation by using another neural network model
(besides the attacked model), which was optimized to trans-
form the input into adversarial pattern under the L1norm.
[8] proposed both white and black box untargeted attacks
on two-stream model (optical-ﬂow and RGB), based on the
original and the iterative version of FGSM [5, 14], and used
FlowNet2 [7] to estimate optical ﬂow in order to provide
gradients estimation. Several black-box attacks were pro-
posed [10, 35]. Our attack follows the white-box setting
therefore those attacks are beyond the scope of this paper.
3. Flickering Adversarial Attack
The ﬂickering adversarial attack consists of a uniform
offset added to the entire frame that changes each frame.
This novel approach is desirable for several reasons. First,
it contains no spatial pattern within individual frames but
an RGB offset. Second, this type of perturbation can easily
be mistaken in some cases as changing lighting conditions
of the scene or typical sensor behaviour. Third, it is imple-
mentable in the real-world using a simple LED light source.
3.1. Preliminaries
Video action recognition is a function F(X) =ythat
accepts an input X= [x1;x2;::;xT]2RTHWCfrom
Tconsecutive frames with Hrows,Wcolumns and Ccolor
channels, and produces an output y2RKwhich can be
treated as probability distribution over the output domain,
whereKis the number of classes. The model Fimplicitly
depends on some parameters that are ﬁxed during the at-
tack. The classiﬁer assigns the label A(X) = argmaxiyi
to the input X. We denote adversarial video by ^X=
X+where the video perturbation = [1;2;::;T]2
RTHWC, and each individual adversarial frame by
^xi=xi+i.^Xis adversarial when A(^X)6=A(X)(un-
targeted attack) or A(^X) =k6=A(X)for a speciﬁc pre-
determined incorrect class k2[K](targeted attack), while
keeping the distance between ^XandXas small as possible
under the selected metric (e.g., L2norm).
3.2. Methodology
In our attack iis designed to be spatial-constant on the
three color channels of the frame, meaning that for each
pixel in image xi, an offset is added with the same value(RGB). Thus, the ithperturbation i, which corresponds to
theithframexiof the video, can be represented by three
scalars, hence = [1;2;::;T]2RT113, having in
total 3Tparameter to optimize. To generate an adversarial
perturbation we usually use the following objective function
argmin
X
jjDj() +1
NNX
n=1`(F(Xn+);tn)(1)
s:t^xi2[Vmin;Vmax]HWC; (2)
whereNis the total number of training videos, Xnis the
nthvideo,F(Xn+)is the classiﬁer output (probability
distribution or logits), and tnis the original label (in the case
of untargeted attack). The ﬁrst term in Equation (1) is regu-
larization term, while the second is adversarial classiﬁcation
loss, as will be discussed later in this paper. The parameter
weighs the relative importance of being adversarial and
also the regularization terms. The set of functions Dj()
controls the regularization terms that allows us to achieve
better imperceptibility for the human observer. The parame-
terjweighs the relative importance of each regularization
term. The constraint in Equation (2) guarantees that after
applying the adversarial perturbation, the perturbed video
will be clipped between the valid values: Vmin;Vmax, that
represents the minimum and maximum allowed pixel inten-
sity.
3.3. Adversarial loss function
We use a loss mechanism similar to the loss presented by
C&W [1], with a minor modiﬁcation. For untargeted attack:
`(y;t) = max
0;min1
m`m(y;t)2;`m(y;t)
(3)
`m(y;t) =yt max
i6=t(yi) +m; (4)
wherem > 0is the desired margin of the original class
probability below the adversarial class probability. A more
detailed explanation of the motivation in deﬁning the above
loss function is found in the supplementary material.
3.4. Regularization terms
We quantify the distortion introduced by the perturba-
tionwithD()in the spatio-temporal domain. This met-
ric will be constrained in order for the perturbation to be
imperceptible to the human observer while remaining ad-
versarial. Unlike previously published works on adversarial
patches in images, in the video domain imperceptible may
reference thin patches in gray-level space or slow changing
patches in temporal frame space. In contrast to previous re-
lated works [34, 35], in our case temporal sparsity is not of
the essence but the unnoticability to the human observer. Inorder to achieve the most imperceptible perturbation we in-
troduce two regularization terms, each controlling different
aspects of human perception.
In order to simplify the deﬁnition of our regularization
terms and metrics, we deﬁne the following notations for
X= [x1;x2;::;xT]2RTHWC(video or perturba-
tion).
Tensor p-norm:
kXkp= TX
i1=1CX
i4=1jxi1:::i4jp!1=p
; (5)
wherei1;i2;::;i 4refer to dimensions.
Roll operator :Roll(X;)produce the time shifted ten-
sor, whose elements are -cyclic shifted along the ﬁrst axis
(time) :
Roll(X;) = [x(modT)+1;:::;x (T 1+modT)+1]:(6)
1stand2ndorder temporal derivatives : We approximate
the1stand2ndorder temporal derivatives by ﬁnite differ-
ences as follows.
@X
@t=Roll(X;1) Roll(X;0); (7)
@2X
@t2=Roll(X; 1) 2Roll(X;0) +Roll(X;1):(8)
3.4.1 Thickness regularization
This loss term forces the adversarial perturbation to be as
small as possible in gray-level over the three color chan-
nels (per-frame), having no temporal constraint and can be
related to the “thickness” of the adversarial pattern.
D1() =1
3Tkk2
2;
wherekk2deﬁned in Equation (5) with p= 2.
3.4.2 Roughness regularization
We introduce temporal loss functions that incorporate two
different terms,
D2() =1
3T@
@t2
2+1
3T@2
@t22
2; (9)
where@
@tand@2
@t2are deﬁned in Equations (7,8), respec-
tively.
The norm of the ﬁrst order temporal difference shown in
the Equation (9) (ﬁrst term) controls the difference between
each two consecutive frame perturbations. This term penal-
izes temporal changes of the adversarial pattern. Within theAttack Attacked Model Fooling ratio[%] Thickness[%] Roughness[%]
Single Video I3D 100 1.00.5 0.830.4
Single Video R(2+1)D 93.0 2.41.9 2.12.0
Single Class I3D 90.211.72 13.03.6 10.62.2
Universal I3D 93.0 15.5 15.7
Universal R(2+1)D 79.0 18.1 21.0
Universal MC3 77.1 18.3 24.5
Universal R3D 90.3 17.8 25.5
Universal Time Invariance I3D 83.1 18.0 14.0
Table 1: Results over several types of attacks on different at-
tacked models. Thickness and Roughness deﬁned in Equa-
tions (10,11)
context of human visual perception, this term is perceived
as “ﬂickering”, thus we wish to minimize it.
The norm of the second order temporal difference shown
in Equation (9) (second term) controls the trend of the ad-
versarial perturbation. Visually, this term penalizes fast
trend changes, such as spikes, and may be considered as
scintillation reducing term.
The weights of D1andD2will be noted by 1and2,
respectively, throughout the rest of the paper and also in the
YouTube videos.
3.5. Metrics
Let us deﬁne several metrics in order to quantify the per-
formance of our adversarial attacks.
Fooling ratio : is deﬁned as the percentage of adversarial
videos that are successfully misclassiﬁed (higher is better).
Mean Absolute Perturbation per-pixel :
thickness gl() =1
3Tkk1; (10)
wherekk1deﬁned in Equation (5) with p= 1.
Mean Absolute Temporal-diff Perturbation per-pixel :
roughness gl() =1
3T@
@t
1: (11)
The thickness and roughness values in this paper will be
presented as percents from the full applicable values of the
image span, e.g.,
thickness () =thickness gl()
Vmax Vmin100:
4. Experiments on I3D
4.1. Targeted Model
Our attack follows the white-box setting, which assumes
the complete knowledge of the targeted model, its param-
eter values and architecture. The I3D [2] model for video
recognition is used as target model, focused on the RGB
pipeline. The adversarial attacks described in this work canbe a targeted or untargeted, and the theory and implementa-
tion can be easily adapted accordingly. The I3D model was
selected for targeting because common video classiﬁcation
networks are based upon its architecture. Therefore, the in-
sights derived from this work will be relevant for these net-
works. In the I3D conﬁguration T= 90;H= 224;W=
224;C= 3, andVmin= 1;Vmax = 1 (trained on the
kinetics Dataset). Implementation details can be found in
the supplementary material.
4.2. Dataset
We use Kinetics-400 [11] for our experiments. Kinetics
is a standard benchmark for action recognition in videos.
It contains about 275K video of 400 different human ac-
tion categories (220K videos in the training split, 18K in
the validation split, and 35K in the test split). For the single
video attack we have developed the attacks using the val-
idation set. In the class generalization section we trained
on the training set and evaluated on the validation set. In
the universal attack section we trained on the validation set
and evaluated on the test set. We pre-processed the dataset
by excluding movies in which the network misclassiﬁed to
begin with and over-ﬁtted entries. Each video contains 90-
frame snippets.
4.3. Single Video Attack
In order to perform the ﬂickering adversarial attacks on
single videos, a separate optimization has to be solved for
each video, i.e., solving Equation (1) for a single video
(N= 1) s.t. each video have its own tailor-made . In our
experiment we have developed different ’s for hundreds of
randomly picked samples from the kinetics validation set.
TheSingle Video entry in Table 1 shows the statistics of av-
erage and standard deviation of the fooling ratio, thickness
and roughness of untargeted single-video attacks, reaching
100% fooling ratio with low roughness and thickness val-
ues. Video examples of the attack can be found here1. De-
tailed description of the convergence process regarding this
attack can be found in the supplementary material.
4.3.1 Thickness Vs. Roughness
In order to illustrate the trade-off between 1and2under
single video attacks, we have selected a video sample (ki-
netics test set) on which we developed two different ﬂick-
ering attacks by solving Equation (1) (separately) under the
single video attack settings (N= 1) . As described in Sec-
tion 3.4, the j’s coefﬁcients control the importance of each
regularization term, where 1associated with the term that
forces the perturbation to be as small as possible in gray-
level over the three color channels and 2associated with
purely temporal terms (norms of the ﬁrst and second tem-
poral derivatives) forcing the perturbation to be temporally-
Figure 2: Illustration of the trade-off between thickness and
roughness in a single video attack as described in Section
4.3.1.
smooth as possible. The ﬁrst perturbation developed with
1= 1 and2= 0, minimize the thickness while leaving
the roughness unconstrained. The second perturbation de-
veloped with 1= 0 and2= 1, minimize the roughness
while leaving the thickness unconstrained. Both of these
perturbations cause misclassiﬁcations on the I3D model. In
order to visualize the difference between these perturba-
tions, we deliberately picked a difﬁcult example to attack
which that requires large thickness and roughness. In Fig-
ure 2 we plot both attacks in order to visualize the differ-
ence between the two cases. Each row combined 8 consec-
utive frames (out of 90 frames). In the ﬁrst row, the original
(clean) video sample from the “juggling balls” category.
In the second row, the adversarial (misclassiﬁed) video we
developed with 1= 1 and2= 0 (minimizing thick-
ness). In the third row the adversarial video with 1= 0
and2= 1 (minimizing roughness). In the fourth row we
plot the ﬂickering perturbations with 1= 1,2= 0reach-
ing a thickness of 2:97% and roughness of 4:84%. In the
ﬁfth row we plot the ﬂickering perturbations with 1= 0,
2= 1 reaching a thickness of 7:45% and roughness of
2:20%. As expected, the perturbation with the minimized
roughness (last row) is smoother than the one without the
temporal constrain (fourth row). Furthermore, even though
the thickness of temporal constrained perturbation is much
higher (7.45% compare to 2.97%) the adversarial perturba-
tion is less noticeable to the human observer than the one
with the smaller thickness. Video examples of the discussed
attacks can be found here1under “juggling balls” .
4.4. Adversarial Attack Generalization
Unlike single video attack, where the ﬂickering perturba-
tionwas video-speciﬁc, a generalized (or universal) ﬂick-
ering attack is a single perturbation that fools our targeted
model with high probability for all videos (from any classor a speciﬁc class). In order to obtain a universal adversarial
perturbation across videos we solve the optimization prob-
lem in Equation (1) with some attack-speciﬁc modiﬁcations
as described in the following sections.
4.4.1 Class generalization: Untargeted Attack
Adversarial attacks on a single video have limited applica-
bility in the real world. In this section we generalize the
attack to cause misclassiﬁcation to all videos from a spe-
ciﬁc class with a single generalized adversarial perturbation
. Our experiments conducted on 100 (randomly picked)
out of 400 kinetics classes s.t. for each class (separately)
we developed its own by solving the optimization prob-
lem in Equation (1), where fXngN
n=1is the relevant class
training set split. After developing the class generalization
we evaluate its fooling ratio performance, thickness and
roughness as deﬁned in Section 3.5 on the relevant class
evaluation split. The Single Class entry in Table 1 shows
the statistics of average and standard deviation (across 100
different’s) of the fooling ratio, thickness and roughness.
Showing that when applying this attack, on average 90:2%
of the videos from each class were misclassiﬁed. It is ob-
vious that generalization produces perturbation with larger
thickness and roughness.
4.4.2 Universal Untargeted Attack
We take one more step toward real world implementation of
the ﬂickering attack by devising a single universal perturba-
tion that will attack videos from any class. Constructing
such ﬂickering attacks is not trivial due to the small num-
ber of trainable parameters ( TCor270in I3D) and in
particular that they are independent of image dimensions.
Similarly to the previous section, we developed single 
by solving the optimization problem in Equation (1), where
fXngN
n=1is the training set deﬁned as the entire evaluation-
split ( 20Kvideos) of the Kinetics-400. Once the universal
was computed, we evaluated its fooling ratio performance,
thickness and roughness on a random sub-sample of 5K
videos from the kinetics test-split. As can be seen in Uni-
versal Class entry in Table 1, our universal attack reaches a
93% fooling ratio. One might implement the universal ﬂick-
ering attack as a class-targeted attack using the presented
method. In this case, the selected class may affect the efﬁ-
ciency of the adversarial perturbation.
4.5. Time Invariance
Practical implementation of adversarial attacks on video
classiﬁers can not be subjected to prior knowledge regard-
ing the frame numbering or temporal synchronization of the
attacked video. In this section we present a time-invariant
adversarial attack that can be applied to the recorded scenewithout assuming that the perturbation of each frame is ap-
plied at the right time. Once this time-invariant attack is
projected to the scene in a cyclic manner, regardless of
the frame arbitrarily-selected as ﬁrst, the adversarial pat-
tern would prove to be effective. Similar to the general-
ized adversarial attacks described in previous subsections, a
random shift between the perturbation and the model input
was applied during training. The adversarial perturbation in
Equation (1) modiﬁed by adding the Roll operator deﬁned
in Equation (6) s.t. F(Xn+Roll(;))for randomly sam-
pled2f1;2;;Tgin each iteration and on each video
during training and evaluation. This time invariance gen-
eralization of universal adversarial ﬂickering attack reaches
83% fooling ratio, which is luckily a small price to pay in
order to approach real-world implementability.
5. Additional models, baseline comparisons
and transferability
In order to demonstrate the effectiveness of the ﬂicker-
ing adversarial attack (universal in particular) we applied
selected attacks to other relevant models and compared be-
tween the proposed universal ﬂickering attack to other base-
line attacks (Section 5.2) and validate that our attack is in-
deed transferable [28] across models (Section 5.3).
5.1. Targeted Models
Similar to the previous experiment we follow the white-
box setting. In the following experiments we apply our at-
tack on three different models MC3, R3D, R(2+1)D (pre-
trained on the Kinetics Dataset) from [30] which discuss
several forms of spatiotemporal convolutions and study
their effects on action recognition. All three model are
based on 18 layers ResNet architecture [6], accepting spa-
tial and temporal dimensions of: T= 16;H= 112;W=
112;C= 3. Implementation details can be found in this
paper’s supplementary material.
5.2. Baseline comparison
Following the introduction of the ﬁrst ﬂickering attack
against video action recognition models, a baseline compar-
ison of the effectiveness of the universal attack is presented
against several types of random ﬂickering perturbations. We
developed a universal ﬂickering perturbation Fon model
F(I3D, R(2+1)D, etc.) with respect to the Kinetics Dataset
by solving the optimization problem deﬁned by Equation
(1). Following Equation (1) we constrained the `1norm of
Fby clipping s.t.F
1= maxjFjfor some.
In order to evaluate the Fooling ratio of any (and in
particularF) on some model Fwe deﬁne the evaluation set
X=fXigM
i=1whereXi= [xi
1;xi
2;::;xi
T]isithevaluation
video consisting of Tconsecutive frames. On top of Xwe
deﬁne the adversarial evaluation set ^X=f^XigM
i=1where,^Xi= [xi
1+;xi
2+;::;xi
T+]for alli. Therefore, the
fooling ratio is calculated by evaluating Fon^X. In the
following experiments we use the same evaluation set X.
Given a ﬂickering universal adversarial perturbation F
developed on model F, we deﬁne the following random
ﬂickering attacks:
F
U U (minF;maxF): Random variable uniformly
distributed between the minimal and maximal values of F.
F
MinMax : Each element is drawn from the set
fminF;maxFgwith equal probability.
F
shuffle : A random shufﬂe of Falong the frames and
color channels. Table 2 shows the results of our experiments
where each experiment (different `1[%]) was performed as
follows:
1. For given we developing Ffor each one of our four
attacked models: I3D, R(2+1)D, R3D and MC3.
2. For each Fwe developed F
U;F
MinMax;F
shuffle as
described earlier.
3. On each model Fwe evaluate the fooling ratio
of the following perturbation: Random ﬂickering
(F
U;F
MinMax;F
shuffle ), universal ﬂickering devel-
oped upon other models and universal ﬂickering F.
In our experiments the `1[%]norm ofis represented as
the percentage of the allowed pixel intensity range ( Vmax-
Vmin). e.g., ifVmax = 1,Vmin= 1and`1[%] = 10
than= 0:2. In order to obtain statistical attributes we per-
formed the experiments by re-perturbing the random gener-
ated’s (F
U;F
MinMax;F
shuffle ). As shown in Table 2 we
performed the experiments over several values of `1[%]: 5,
10, 15 and 20. The columns (with models names) repre-
sent the attacked model, while the rows represent the type
of ﬂickering attacks. Random ﬂickering attacks are located
at the ﬁrst 3 rows of each experiment, followed by the uni-
versal ﬂickering attack trained upon other models (except
I3D)5– marked with (trns). The universal ﬂickering attack
(ours) is located at the last row of each experiment. Each
cell holds the fooling ratio result (average and standard de-
viation in the case of random generated perturbations) when
evaluating the model on the data with the relevant attack.
As can be seen, the universal ﬂickering attack demonstrates
superiority across all four models, over the transferable at-
tacks and the random ﬂickering attacks. In addition to Table
2, additional analysis is presented in the supplementary ma-
terial.
5.3. Transferability across Models
Transferability [28] is deﬁned as the ability of an attack
to inﬂuence a model which was unknown to the attacker
5The transferabilty between I3D to the other models (and vice versa)
were not evaluated because the input of the models is not compatible.`1[%] AttacknModel I3D R(2+1)D R3D MC3
5Random Uniform 8.40.6% 4.90.8% 8.31.8% 11.01.9%
Random MinMax 12.20.7% 9.02.3% 15.83.5% 17.43.8%
Filckering shufﬂe 11.90.6% 9.41.7% 16.43.3% 16.52.5%
R(2+1)D (trns) - - 27.6% 18.4%
R3D (trns) - 14.9% - 24.0%
MC3 (trns) - 12.3% 31.4% -
Filckering 26.2% 23.3% 34.3% 41.3%
10Random Uniform 14.21.2% 10.73.3% 20.25.3% 17.93.1%
Random MinMax 23.62.4% 19.24.8% 36.76.3% 30.03.7%
Filckering shufﬂe 22.92.1% 18.35.5% 31.97.2% 25.93.7%
R(2+1)D (trns) - - 52.7% 38.4%
R3D (trns) - 30.6% - 35.6%
MC3 (trns) - 25.9% 50.5% -
Filckering 58.4% 47.2% 70.4% 55.3%
15Random Uniform 20.32.1% 16.04.7% 26.24.7% 24.21.8%
Random MinMax 34.23.1% 28.17.9% 48.67.4% 36.44.9%
Filckering shufﬂe 29.33.1% 28.75.0% 44.68.7% 35.32.8%
R(2+1)D (trns) - - 64.4% 48.4%
R3D (trns) - 39.5% - 50.7%
MC3 (trns) - 40.7% 66.1% -
Filckering 78.1% 62.7% 83.4% 73.3%
20Random Uniform 32.13.1% 22.25.7% 37.14.0% 30.04.5%
Random MinMax 48.04.5% 42.03.0% 54.611.0% 44.05.0%
Filckering shufﬂe 42.03.6% 39.08.0% 57.66.4% 47.14.7%
R(2+1)D (trns) - - 74.6% 59.2%
R3D (trns) - 58.5% - 60.7%
MC3 (trns) - 55.8% 70.4% -
Filckering 93.0% 79.0% 90.3% 77.1%
Table 2: Baseline comparison of the universal ﬂickering at-
tack to several types of random ﬂickering attacks and trans-
ferability across different models.
when developing the attack. We examined the transferabil-
ity of the ﬂickering attack on different models of the same
input type. As seen in Table 2, for each `1[%]we eval-
uate the fooling ratio of attacks that was trained on differ-
ent models (trns). The high effectiveness of the attack ap-
plied across models indicates that our attack is transferable
between these different models, e.g., attack that was devel-
oped on R(2+1)D with `1[%] = 20 achieved 74:6%fooling
ratio when applied on R3D model compared to 90:3%.
6. Over-the-Air Real world demonstration
The main advantage of the ﬂickering attack, unlike the
majority of adversarial attacks in published papers, is its
real-world implementability. In this section we demon-
strate, for the ﬁrst time, the ﬂickering attack in a real world
scenario. We used an RGB led light bulb and controlled
it through Wiﬁ connection. Through this connection we
were able to control the red, green and blue illumination
values separately, and create almost any of the previously
developed adversarial RGB patterns introduced in this pa-
per (Figure 1 depicts the modeling of our digital domain
attack in the real-world). As in [15, 3], we have applied
several constraints for better efﬁciency of the adversarial
manipulations in real-world, such as temporal invariance
(Section 4.5) and increased smoothness to address the ﬁ-nite rise (or fall) time of the RGB bulb (Section 3.4.2). Be-
cause the adversarial patterns presented here have positive
and negative amplitude perturbations, the baseline illumi-
nation of the scenario was set to around half of the possi-
ble maximum illumination of the bulb. A chromatic cali-
bration of the RGB intensities was performed in order to
mitigate the difference of the RGB illumination of the light
bulb and RGB responsivity of the camera, which was ob-
viously not the same and also included channel chromatic
crosstalk. The desired scenario for the demonstration of the
attack includes a video camera streaming a video ﬁlmed in
a room with a Wiﬁ-controlled RGB light bulb. A computer
sends over Wiﬁ the adversarial RGB pattern to the bulb. A
ﬁgure performs actions in front of the camera. Implementa-
tion and hardware details can be found in the supplementary
material. We demonstrate our over-the-air attack in two dif-
ferent ways, scene-based and universal ﬂickering attack.
0 200 400 600 800 1000
Frame020406080100Probability[%]original class
adversarial class
max class
Figure 3: Example of our Over-the-Air scene based attack.
The plot was taken from the “ironing” video example2.
6.1. Over-the-Air Scene-based Flickering Attack
In this attack, we assume prior knowledge of the scene
and the action. Therefore, similar to a single video attack
(Section 4.3) we will develop a scene dedicated attack. In
this approach we record a clean video (without perturba-
tion) of the scene we would like to attack. For the clean
recording we develop a time-invariant digital attack as de-
scribed in the paper. Once we have the digital attack, we
transmit it to a “similar” scene (as described in the supple-
mentary material) in an over-the-air approach. Video ex-
amples of our scene based over-the-air adversarial attack
can be found here2. Figure 3 shows the probability of a
real example of our scene based over-the-air attack of the
“ironing” action, where the x-axis (Frame) represents pre-
diction time step and the y-axis (Probability) represents the
output probability of the I3D model for several selected
classes. The area shaded in red represents the period of
time the scene was attacked. As described in the legend,
the yellow graph is the true class ( “ironing” ) probability,
the red graph is the adversarial class ( “drawing” ) probabil-ity and the dashed blue graph represents the probability of
the most probable class the classiﬁer predicts each frame.
It can be seen that when the scene is not attacked (outside
the red area) the model predicts correctly the action being
performed (dashed blue and yellow graphs overlap). Once
the scene is attacked, the true class is suppressed and the
adversarial class is ampliﬁed. At the beginning (end) of
the attack, it can be seen that there is a delay from the mo-
ment the attack begins (ends) until the model responds to
the change due to the time required (90 frames) to ﬁll the
classiﬁer’s frame buffer and perform the prediction.
6.2. Over-the-Air Universal Flickering Attack
This section deals with the case where we do not have
any prior knowledge regarding the scene and action we wish
to attack. Therefore, we would like to develop a universal
attack that will generalize to any scene or action. In this
approach, we will use a universal time-invariant attack as
described in the paper. Once we have the digital attack, we
transmit it to the scene in an over-the-air approach. Video
examples of our universal over-the-air attack can be found
here3. Since our approach is real-world applicable, and thus
we require universality and time-invariability perturbation
(no need to synchronize the video with the transmitted per-
turbation), the pattern is visible to the human observer.
7. Conclusions and future work
The ﬂickering adversarial attack was presented, for the
ﬁrst time, for several models and scenarios summarized in
Tables 1, 2. Furthermore, this attack was demonstrated
in the real world for the ﬁrst time. The ﬂickering attack
has several beneﬁts, such as the relative imperceptability to
the human observer in some cases, achieved by small and
smooth perturbations as can be seen in the videos we have
posted1. The ﬂickering attack was generalized to be uni-
versal, demonstrating superiority over random ﬂickering at-
tacks on several models. In addition, the ﬂickering attack
has demonstrated the ability to transfer between different
models. The ﬂickering adversarial attack is probably the
most applicable real-world attack amongst any video ad-
versarial perturbation this far, as was shown3,2. Thanks to
the simplicity and uniformity of the perturbation across the
frame which can be achieved by subtle lighting changes to
the scene by illumination changes. All of these properties
make this attack implementable in real-world scenarios.
In extreme cases where generalization causes the pattern
to be thick enough to be noticed by human observers, the
usage of such perturbations can be relevant for non-man-
in-the-loop systems or cases where the human observer will
see image-ﬂickering without associating this ﬂickering with
an adversarial attack. In the future, we may expand the cur-
rent approach to develop defensive mechanisms against ad-
versarial attacks of video classiﬁer networks.References
[1] Nicholas Carlini and David Wagner. Towards evaluating the
robustness of neural networks. In 2017 ieee symposium on
security and privacy (sp) , pages 39–57. IEEE, 2017. 3, 11
[2] Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 6299–6308, 2017. 1, 2, 4
[3] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li,
Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi
Kohno, and Dawn Song. Robust physical-world attacks on
deep learning visual classiﬁcation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition , pages 1625–1634, 2018. 7
[4] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and
Kaiming He. Slowfast networks for video recognition. In
Proceedings of the IEEE international conference on com-
puter vision , pages 6202–6211, 2019. 1
[5] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. In Inter-
national Conference on Learning Representations , 2015. 1,
3
[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 6
[7] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper,
Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolu-
tion of optical ﬂow estimation with deep networks. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition , pages 2462–2470, 2017. 3
[8] Nathan Inkawhich, Matthew Inkawhich, Yiran Chen, and
Hai Li. Adversarial attacks for optical ﬂow-based action
recognition classiﬁers. arXiv preprint arXiv:1811.11875 ,
2018. 1, 3
[9] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolu-
tional neural networks for human action recognition. IEEE
Trans. Pattern Anal. Mach. Intell. , 35:221–231, 2013. 2
[10] Linxi Jiang, Xingjun Ma, Shaoxiang Chen, James Bailey,
and Yu-Gang Jiang. Black-box adversarial attacks on video
recognition models. ArXiv , abs/1911.09449, 2019. 1, 3
[11] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
Tim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-
man action video dataset. arXiv preprint arXiv:1705.06950 ,
2017. 2, 5
[12] Diederik Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. International Conference on Learn-
ing Representations , 2014. 11
[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q.
Weinberger, editors, Advances in Neural Information Pro-
cessing Systems 25 , pages 1097–1105. Curran Associates,
Inc., 2012. 1
[14] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Ad-
versarial machine learning at scale. In 5th International Con-ference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings ,
2017. 3
[15] Juncheng Li, Frank Schmidt, and Zico Kolter. Adversarial
camera stickers: A physical camera-based attack on deep
learning systems. In International Conference on Machine
Learning , pages 3896–3904. PMLR, 2019. 7
[16] Shasha Li, Ajaya Neupane, Sujoy Paul, Chengyu Song,
Srikanth V . Krishnamurthy, Amit K. Roy-Chowdhury, and
Ananthram Swami. Stealthy adversarial perturbations
against real-time video classiﬁcation systems. In 26th An-
nual Network and Distributed System Security Symposium,
NDSS 2019, San Diego, California, USA, February 24-27,
2019 , 2019. 3
[17] Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Ar-
naud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen
Ghafoorian, Jeroen A.W.M. van der Laak, Bram van Gin-
neken, and Clara I. S ´anchez. A survey on deep learning in
medical image analysis. Medical Image Analysis , 42:60 –
88, 2017. 1
[18] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt
Fredrikson, Z Berkay Celik, and Ananthram Swami. The
limitations of deep learning in adversarial settings. In 2016
IEEE European symposium on security and privacy (Eu-
roS&P) , pages 372–387. IEEE, 2016. 1
[19] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In C. Cortes, N. D. Lawrence, D. D.
Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neu-
ral Information Processing Systems 28 , pages 91–99. Curran
Associates, Inc., 2015. 1
[20] Roberto Rey-de Castro and Herschel Rabitz. Targeted non-
linear adversarial perturbations in images and videos. arXiv
preprint arXiv:1809.00958 , 2018. 3
[21] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International journal of
computer vision , 115(3):211–252, 2015. 2
[22] Hasim Sak, Andrew W. Senior, and Franc ¸oise Beaufays.
Long short-term memory recurrent neural network architec-
tures for large scale acoustic modeling. In INTERSPEECH
2014, 15th Annual Conference of the International Speech
Communication Association, Singapore, September 14-18,
2014 , pages 338–342, 2014. 2
[23] Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. IEEE
Trans. Pattern Anal. Mach. Intell. , 39(4):640–651, 2017. 1
[24] Karen Simonyan and Andrew Zisserman. Two-stream con-
volutional networks for action recognition in videos. In Ad-
vances in neural information processing systems , pages 568–
576, 2014. 2
[25] Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world
anomaly detection in surveillance videos. In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , June 2018. 1
[26] Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang.
Deep learning face representation by joint identiﬁcation-veriﬁcation. In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, editors, Advances in Neu-
ral Information Processing Systems 27 , pages 1988–1996.
Curran Associates, Inc., 2014. 1
[27] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In Computer Vision and Pattern Recognition
(CVPR) , 2015. 2
[28] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. In-
triguing properties of neural networks. In International Con-
ference on Learning Representations , 2014. 1, 6, 7
[29] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. Learning spatiotemporal features with
3d convolutional networks. In Proceedings of the IEEE inter-
national conference on computer vision , pages 4489–4497,
2015. 2
[30] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann
LeCun, and Manohar Paluri. A closer look at spatiotemporal
convolutions for action recognition. In Proceedings of the
IEEE conference on Computer Vision and Pattern Recogni-
tion, pages 6450–6459, 2018. 2, 6
[31] G ¨ul Varol, Ivan Laptev, and Cordelia Schmid. Long-
term temporal convolutions for action recognition. IEEE
transactions on pattern analysis and machine intelligence ,
40(6):1510–1517, 2017. 2
[32] Lei Wang, Yangyang Xu, Jun Cheng, Haiying Xia, Jianqin
Yin, and Jiaji Wu. Human action recognition by learning
spatio-temporal features with deep neural networks. IEEE
Access , 6:17913–17922, 2018. 2
[33] Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and
Kaiming He. Non-local neural networks. 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 7794–7803, 2017. 1
[34] Xingxing Wei, Jun Zhu, Sha Yuan, and Hang Su. Sparse
adversarial perturbations for videos. In The Thirty-Third
AAAI Conference on Artiﬁcial Intelligence, AAAI 2019, The
Thirty-First Innovative Applications of Artiﬁcial Intelligence
Conference, IAAI 2019, The Ninth AAAI Symposium on Ed-
ucational Advances in Artiﬁcial Intelligence, EAAI 2019,
Honolulu, Hawaii, USA, January 27 - February 1, 2019 ,
pages 8973–8980, 2019. 1, 3
[35] Zhipeng Wei, Jingjing Chen, Xingxing Wei, Linxi Jiang, Tat-
Seng Chua, Fengfeng Zhou, and Yu-Gang Jiang. Heuristic
black-box adversarial attacks on video recognition models.
InAAAI , pages 12338–12345, 2020. 1, 3Appendices
A. Modiﬁed Adversarial loss function
For achieving a more stable convergence, we used a loss
mechanism similar to the loss presented by [1], with a small
modiﬁcation, which smoothly reaches the adversarial goal
only to the desired extent, leaving space for other regular-
ization terms. For untargeted attack:
`(y;t) = max
0;min1
m`m(y;t)2;`m(y;t)
(12)
`m(y;t) =yt max
i6=t(yi) +m: (13)
m > 0is the desired margin of the original class probabil-
ity below the adversarial class probability. When loss val-
ues are within the desired margin, the quadratic loss term
relaxes the relatively steep gradients and momentum of the
optimizer, and the difference between the ﬁrst and second
class probabilities approach the desired margin m. When
the loss starts rising, the quadratic term gently maintains
the desired difference between these two classes, therefore
preventing overshoot effects. In order to apply the sug-
gested mechanism on targeted attack, the loss term changed
to`m(y;t) = maxi6=t(yi) yt+m, while this time, tis the
targeted adversarial class.
In some cases it would be beneﬁcial to follow [1] and
use the logits instead of the probabilities for calculating the
loss. We suggest adapting this method partially by keeping
the desired margin in probability space, normalized at each
iteration accordingly, for margin deﬁned in logit space may
be less intuitive as a regularization term.
B. Implementation Details
B.1. Experiments on I3D
Experiment codes are implemented in TensorFlow6and
based on I3D source code7. The code is executed on a
server with four Nvidia Titan-X GPUs, Intel i7 processor
and 128GB RAM. For optimization we adopt the ADAM
[12] optimizer with learning rate of 1e-3 and with batch size
of 8 for the generalization section and 1 for a single video
attack. Except where explicitly stated 1=2= 0:5. For
single video attack and for generalization sections = 1.
B.2. Experiments on MC3, R3D, R(2+1)D
Experiments code are implemented in PyTorch8and
based on source code of computervision-recipes9and
6https://www.tensorflow.org/
7https://github.com/deepmind/kinetics-i3d
8https://pytorch.org/
9https://github.com/microsoft/computervision-
recipes
100101102103
iter (#)0.350.400.450.500.550.600.650.700.75probability
0.000.250.500.751.001.251.501.752.00
[%]max class
original class
roughness
thicknessFigure 4: Learning process of the modiﬁed loss mechanism.
Probabilities (green and red lines) corresponds to the left y-
scale. Roughness and thickness (blue lines) are in percents
from the full gray-level range of the image (right y-scale).
original class is the probability of the actual class of the
unperturbed video. max class is the probability of the most
probable class as the classiﬁer predicts.
torchvision10package. Hardware, optimizer, batch size,
1;2andare the same as previously introduced for the
I3D model.
C. Single Video Attack
C.1. Convergence Process
In order to demonstrate the convergence process we have
attacked a single video. As can be seen, several trends re-
garding the trends can be observed (Figure 4). At ﬁrst, the
adversarial perturbation rises in thickness and roughness.
At iteration 40the top-probability class switches from the
original to the adversarial class, which until now was not
plotted, for this adversarial attack is untargeted. At that it-
eration, the adversarial loss is m. When the difference be-
tween the probability of the adversarial and original class
is larger then mthe adversarial loss is zero and the regu-
larization starts to be prominent, causing the thickness and
roughness to decay. This change of trend occurs slightly
after the adversarial class change due to the momentum of
the Adam optimizer and remaining intrinsic gradients. At
iteration 600the difference between the probability of the
adversarial and original class is m= 0:05, the quadratic
loss term maintaining the desired difference between these
classes while diminishing the thickness and roughness. The
binary loss changes at the interface between adversarial suc-
cess and failure caused convergence issues, and the imple-
10https://github.com/pytorch/visionmentation of the quadratic term, as deﬁned in Equation (12)
handled this issue.
C.2. Thickness Vs. Roughness
In order to visualize the trade-off between 1and2we
plotted three graphs in Figure 7. In top and bottom graphs
we see the temporal amplitude of the adversarial perturba-
tion of each frame and for each color channel, respectively.
The extreme case (top) of minimizing only D1(given suc-
cess of the untargeted adversarial attack) and leaving D2
unconstrained ( 1= 1,2= 0). The signal of the RGB chan-
nels ﬂuctuates strongly with a thickness value of 0.87% and
a roughness of 1.24%. The other extreme case (bottom) is
whenD2is constrained and D1is not (1= 0,2= 1), lead-
ing to a thickness value of 1.66% and a roughness value of
0.6%. The central image displays all the gradual cases be-
tween the two extremities: 1goes from 1 to 0, and 2from
0 to 1 on the y-axis. The row denoted by 2= 0 corresponds
to the upper graph and the row denoted by 2= 1 corre-
sponds to the lower graph. Both D1andD2are very dom-
inant in the received perturbation, as desired. Visualization
of the path taken by our loss mechanisms at different 1and
2values can be found in the supplementary material.
Apart from the visualization experiments we showed, an-
other experiment have been conducted in order to visualize
the path taken by our loss mechanism at different 1and
2. We have plotted a 3D representation in probability-
thickness-roughness space for 10different experiments ( 10
different single video attack on the same video) with grad-
ual change of 1and2parameters. Figure 8 shows the
probability of the most probable class at 10 different scenar-
ios as described in the legend. One can see that at the begin-
ning the maximal probability (original class) drops from the
initial probability (upper section of the graph) on the same
path for all of the described cases, until the adversarial per-
turbation takes hold of the top class. From there, the ’s
parameters takes the lead. At this point, each different case
is converging along a different path to a different location
on the thickness-roughness plane. The user may choose the
desired ratios for each speciﬁc application.
D. Additional models, baseline comparison
and transferability
D.1. Baseline comparison
In addition to the table presented in the paper, we have
analyzed our experiments from the attacked model per-
spective. Each Sub-ﬁgure in Figure 9 shows the aver-
age fooling ratio of the attacked model (out of four) with
different perturbation as function of `1[%]. Each sub-
ﬁgure combine three (two in I3D)5main graph types, the
dashed graph represent the universal ﬂickering perturba-
tion developed upon the attacked model ( F), the dot-ted graphs represent the universal ﬂickering attack devel-
oped upon other models (except for I3D) and the contin-
ues graphs represent the random generated ﬂickering per-
turbation (F
U;F
MinMax;F
shuffle ) where the shaded ﬁlled
region isstandard deviation around the average fooling
ratio. Several consistent trends can be observed in each one
of the sub-ﬁgure and thus for each attacked model. For each
`1[%]we can see that the fooling ratio order (high to low)
is, ﬁrst universal ﬂickering attack, then the transferred uni-
versal ﬂickering attack developed upon other models and
ﬁnally, the random generated ﬂickering perturbations.
E. Over-the-Air Real world demonstration
Our goal is to produce an adversarial universal ﬂickering
attack, which will be implemented in the real world by an
RGB led light bulb in a room, causing miss-classiﬁcation.
The desired scenario for the demonstration of the attack in-
cludes a video camera streaming a video ﬁlmed in a room
with a Wiﬁ-controlled RGB led light bulb. A computer
sends over Wiﬁ the adversarial RGB pattern to the bulb. A
ﬁgure performs actions in front of the camera. The hard-
ware speciﬁcations are as follows:
•Video camera : We used 1:3MPixel RGB camera
streaming at 25 frames per second.
•RGB led light bulb : In order to applying the digitally
developed (univrsel or scene based) perturbation to the
scene, we use a RGB led light bulb11, controlled over
Wiﬁ via Python api12, allowing to set RGB value at
relatively high speed.
•Computer : We use a computer to run the I3D action
classiﬁer on the streaming video input. The model in-
put for prediction at time tare all consecutive frames
betweent 90tot(as described in I3D experiments
section). The model prediction frequency is set to 2Hz
(hardware performance limit). In addition, we use the
computer in order to control the smart led bulb.
•Acting ﬁgure : Performs the actions we would like to
classify and attack.
Figure 5 demonstrate our over-the-air attack setup, com-
bining the hardware mentioned above. Figure 5a demon-
strate the state when the attack is off (no adversarial pattern
is transmitted) and the video action recognition network
correctly classify the action, while Figure 5b demonstrate
the state when the attack is on (adversarial pattern is trans-
mitted) and the video action recognition network incorrectly
classify the action.
11https://www.mi.com/global/mi-led-smart-bulb-
essential/specs
12https://yeelight.readthedocs.io/en/latest/”juggling balls”(a) Without over-the-air attack, the action recognition network
classify the action correctly as ”juggling balls”.
”skydiving”(b) With over-the-air attack, the action recognition network
classify the action incorrectly as ”skydiving”.
Figure 5: Room sketched of our over-the-air attack setup.
(a) Frame example from “ironing” video used for training
over-the-air scene based attack.
(b) Frame example from “ironing” scene used for testing over-
the-air scene based attack.
Figure 6: Two frames from ”similar” scenes.
E.1. Over-the-Air Scene-based Flickering Attack
As described in the paper, in the scene-based approach
we assume a prior knowledge of the scene and the action.
In this approach we record a video without any adversarial
perturbation of the scene we would like to attack. Then we
develop a time-invariant digital attack for this recording as
described in the paper. Once we have the digital attack, we
transmit it to a ”similar” scene in order to apply the attack
in the real world as can be found here13. For illustrating
the meaning of ”similar” scene, we show in Figure 6 two
frames, where Figure 6a is a frame example from the video
(scene) which the attack was trained upon and Figure 6b
is a frame example from the scene on which the developed
attack was applied on. The relevant videos shows that even
though the positioning is different and the clothing are not
13https://bit.ly/Over\_the\_Air\_scene\_based\_videosthe same, the attack is still very effective even with a small
perturbation.
Acknowledgements
Cartoon in Figure 5 designed by ”brgfx / Freepik”.0 10 20 30 40 50 60 70 80 90−4−2024Amplitude [%]
β 1 =1 , β 2 =0
0 10 20 30 40 50 60 70 800.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0β 2 (= 1- β 1 )
0 10 20 30 40 50 60 70 80 90−4−2024Amplitude [%]
β 1 =0 , β 2 =1Figure 7: Top: The adversarial perturbation of the RGB channels (color represents relevant channel) as a function of the
frame number at the case that 1= 1 and2= 0 (D1minimization is preferred). Bottom: The adversarial perturbation of
the RGB channels as a function of the frame number at the case that 1= 0 and2= 1 (D2minimization is preferred). Top
and bottom graphs are presented in percents from the full scale of the image. Middle: The gradual change of the adversarial
pattern between the two extreme cases where 1= 0 corresponds to the top graph and 1= 1 corresponds to the bottom
graph. Color (stretched for visualization purposes) represents the RGB parameters of the adversarial pattern of each frame.
Roughness0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75Thickness0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6probability
0.450.500.550.600.650.70β 1 :0 . 0 β 2 :1 . 0
β 1 :0 . 1 β 2 :0 . 9
β 1 :0 . 2 β 2 :0 . 8
β 1 :0 . 3 β 2 :0 . 7
β 1 :0 . 4 β 2 :0 . 6
β 1 :0 . 5 β 2 :0 . 5
β 1 :0 . 6 β 2 :0 . 4
β 1 :0 . 7 β 2 :0 . 3
β 1 :0 . 8 β 2 :0 . 2
β 1 :0 . 9 β 2 :0 . 1
β 1 :1 . 0 β 2 :0 . 0
Figure 8: Convergence curve in probability-thickness-roughness space of an untargeted adversarial attack with different 1
and2parameters.(a) R3D
 (b) R(2+1)D
(c) MC3
 (d) I3D
Figure 9: Each one of the sub-ﬁgures shows the average fooling ratio of the attacked model (described in caption) with
different perturbations as a function of `1[%]. Each sub-ﬁgure combine three (two in I3D) main graph types, the dashed
graph represent the Universal ﬂickering perturbation developed on the attacked model ( F), the dotted graphs represent the
universal ﬂickering attack developed upon other models (except for I3D) and the continues graphs represent the random
generated ﬂickering perturbations ( F
U;F
MinMax;F
shuffle ) where the shaded ﬁlled region is standard deviation around the
average Fooling ratio.