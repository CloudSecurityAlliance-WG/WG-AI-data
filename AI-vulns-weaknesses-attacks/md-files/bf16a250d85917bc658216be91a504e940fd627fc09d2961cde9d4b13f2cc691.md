3/7/24, 2:41 PM CWE - CWE-1039: Automated Recognition Mechanism with Inadequate Detection or Handling of Adversaria…
https://cwe.mitre.org/data/deﬁnitions/1039.html 1/2
Common W eakness Enumeration
A community-developed list of SW & HW weaknesses that can become
vulnerabilities
Home Search
CWE-1039: Automated Recognition Mechanism with Inadequate Detection or
Handling of Adversarial Input Perturbations
Weakness ID: 1039
Vulnerability Mapping: (with careful review of mapping notes)
View customized information:
 Description
The product uses an automated mechanism such as machine learning to recognize complex data inputs (e.g. image or audio) as a
particular concept or category , but it does not properly detect or handle inputs that have been modified or constructed in a way that
causes the mechanism to detect a dif ferent, incorrect concept.
 Extended Description
When techniques such as machine learning are used to automatically classify input streams, and those classifications are used for
security-critical decisions, then any mistake in classification can introduce a vulnerability that allows attackers to cause the product to
make the wrong security decision. If the automated mechanism is not developed or "trained" with enough input data, then attackers
may be able to craft malicious input that intentionally triggers the incorrect classification.
Targeted technologies include, but are not necessarily limited to:
automated speech recognition
automated image recognition
For example, an attacker might modify road signs or road surface markings to trick autonomous vehicles into misreading the
sign/marking and performing a dangerous action.
 Relationships
 Relevant to the view "Research Concepts" (CWE-1000)
Nature Type ID Name
ChildOf 697 Incorrect Comparison
ChildOf 693 Protection Mechanism Failure
 Modes Of Introduction
Phase Note
Architecture and Design This issue can be introduced into the automated algorithm itself.
 Applicable Platforms
Languages
Class: Not Language-Specific (Undetermined Prevalence)
 Common Consequences
Scope Impact Likelihood
IntegrityTechnical Impact: Bypass Protection Mechanism
When the automated recognition is used in a protection mechanism, an attacker may be able to craft inputs
that are misinterpreted in a way that grants excess privileges.
 Weakness Ordinalities
Ordinality Description
Primary(where the weakness is a quality issue that might indirectly make it easier to introduce security-relevant weaknesses or make
them more difficult to detect)
This weakness does not depend on other weaknesses and is the result of choices made during optimization.
 Memberships
Nature Type ID Name
MemberOf 1413 Comprehensive Categorization: Protection Mechanism Failure
 Vulnerability Mapping NotesAbout ▼ CWE List ▼ Mapping ▼ Top-N Lists ▼ Community ▼ News ▼
ALLOWED
Abstraction: Class
Conceptual OperationalMapping
FriendlyComplete Custom
3/7/24, 2:41 PM CWE - CWE-1039: Automated Recognition Mechanism with Inadequate Detection or Handling of Adversaria…
https://cwe.mitre.org/data/deﬁnitions/1039.html 2/2Usage: ALLOWED-WITH-REVIEW
(this CWE ID could be used to map to real-world vulnerabilities in limited situations requiring careful review)
Reason: Abstraction
Rationale:
This CWE entry is a Class and might have Base-level children that would be more appropriate
Comments:
Examine children of this entry to see if there is a better fit
 Notes
Relationship
Further investigation is needed to determine if better relationships exist or if additional organizational entries need to be created. For
example, this issue might be better related to "recognition of input as an incorrect type," which might place it as a sibling of CWE-
704 (incorrect type conversion).
 References
[REF-16] Christian Szegedy , Wojciech Zaremba, Ilya Sutskever , Joan Bruna, Dumitru Erhan, Ian Goodfellow and Rob Fergus.
"Intriguing properties of neural networks". 2014-02-19. < https://arxiv .org/abs/1312.6199 >.
[REF-17] OpenAI. "Attacking Machine Learning with Adversarial Examples". 2017-02-24. < https://openai.com/research/attacking-
machine-learning-with-adversarial-examples >. URL validated: 2023-04-07 .
[REF-15] James V incent. "Magic AI: These are the Optical Illusions that Trick, Fool, and Flummox Computers". The V erge. 2017-
04-12. < https://www .theverge.com/2017/4/12/15271874/ai-adversarial-images-fooling-attacks-artificial-intelligence >.
[REF-13] Xuejing Yuan, Yuxuan Chen, Yue Zhao, Yunhui Long, Xiaokang Liu, Kai Chen, Shengzhi Zhang, Heqing Huang,
Xiaofeng W ang and Carl A. Gunter . "CommanderSong: A Systematic Approach for Practical Adversarial V oice Recognition". 2018-
01-24. < https://arxiv .org/pdf/1801.08535.pdf >.
[REF-14] Nicholas Carlini and David W agner . "Audio Adversarial Examples: Targeted Attacks on Speech-to-T ext". 2018-01-05.
.
 Content History
 Submissions
Submission Date Submitter Organization
2018-03-12
(CWE 3.1, 2018-03-29)CWE Content Team MITRE
 Modifications