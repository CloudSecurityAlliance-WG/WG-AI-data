Transferability in Machine Learning: from Phenomena to
Black-Box Attacks using Adversarial Samples
Nicolas Papernot and Patrick McDaniel
The Pennsylvania State University
University Park, PA
{ngp5056,mcdaniel}@cse.psu.eduIan Goodfellow
OpenAI
San Francisco, CA
ian@openai.com
ABSTRACT
Many machine learning models are vulnerable to adversarial
examples : inputs that are specially crafted to cause a ma-
chine learning model to produce an incorrect output. Ad-
versarial examples that aect one model often aect another
model, even if the two models have dierent architectures or
were trained on dierent training sets, so long as both mod-
els were trained to perform the same task. An attacker may
therefore train their own substitute model, craft adversar-
ial examples against the substitute, and transfer them to a
victim model, with very little information about the victim.
Recent work has further developed a technique that uses the
victim model as an oracle to label a synthetic training set
for the substitute, so the attacker need not even collect a
training set to mount the attack. We extend these recent
techniques using reservoir sampling to greatly enhance the
eciency of the training procedure for the substitute model.
We introduce new transferability attacks between previously
unexplored (substitute, victim) pairs of machine learning
model classes, most notably SVMs and decision trees. We
demonstrate our attacks on two commercial machine learn-
ing classication systems from Amazon (96.19% misclassi-
cation rate) and Google (88.94%) using only 800 queries
of the victim model, thereby showing that existing machine
learning approaches are in general vulnerable to systematic
black-box attacks regardless of their structure.
1. INTRODUCTION
Many classes of machine learning algorithms have been shown
to be vulnerable to adversarial samples [22, 12, 19]; adver-
saries subtly alter legitimate inputs (call input perturbation)
to induce the trained model to produce erroneous outputs.
Adversarial samples can be used to, for example, subvert
fraud detection, bypass content lters or malware detection,
or to mislead autonomous navigation systems [20]. These
attacks on input integrity exploit imperfections and approxi-
mations made by learning algorithms during training to con-
trol machine learning models outputs (see Figure 1).
14InputModel ActivationsOutputLegitimateAdversarialFigure 1: An adversarial sample (bottom row) is produced
by slightly altering a legitimate sample (top row) in a way
that forces the model to make a wrong prediction whereas
a human would still correctly classify the sample [19].
Adversarial sample transferability1is the property that some
adversarial samples produced to mislead a specic model
fcan mislead other models f0|even if their architectures
greatly dier [22, 12, 20]. A practical impact of this prop-
erty is that it leads to oracle -based black box attacks. In
one such attack, Papernot et al. trained a local deep neu-
ral network (DNN) using crafted inputs and output labels
generated by the target \victim" DNN [19]. Thereafter, the
local network was used to generate adversarial samples that
were highly eective on the original victim DNN. The key
here was that the adversary has very limited information|
they knew nothing about the architecture or parameters but
only knew that the victim was a DNN|and had only oracle
access that allowed it to obtain outputs for chosen inputs.
In this paper, we develop and validate a generalized algo-
rithm for black box attacks that exploit adversarial sample
transferability on broad classes of machine learning. In in-
vestigating these attacks, we explore transferability within
and between dierent classes of machine learning classier
algorithms. We explore neural networks (DNNs), logistic
regression (LR), support vector machines (SVM), decision
trees (DT), nearest neighbors (kNN), and ensembles (Ens.).
In this, we demonstrate that black-box attacks are gener-
ally applicable to machine learning and can eectively tar-
get classiers not built using deep neural networks. The
1Note that this is distinct from knowledge transfer , which
refers to techniques designed to transfer the generalization
knowledge learned by a model fduring training|and en-
coded in its parameters|to another model f0[13].arXiv:1605.07277v1 [cs.CR] 24 May 2016generalization is two-fold: we show that (1) the substitute
model can be trained with other techniques than deep learn-
ing, and (2) transferability-based black box attacks are not
restricted to deep learning targets and is in fact successful
with targeted models of many machine learning types. Our
contributions are summarized as follows:
We introduce adversarial sample crafting techniques
for support vector machine as well as decision trees|
which are non-dierentiable machine learning models.
We study adversarial sample transferability across the
machine learning space and nd that samples largely
transfer well across models trained with the same ma-
chine learning technique, and across models trained
with dierent techniques or ensembles taking collec-
tive decisions. For example, a support vector machine
and decision tree respectively misclassify 91 :43% and
87:42% of adversarial samples crafted for a logistic re-
gression model. Previous work on adversarial example
transferability has primarily studied the case where at
least one of the models involved in the transfer is a
neural network [22, 12, 24], while we aim to more gen-
erally characterize the transferability between a diverse
set of models chosen to capture most of the space of
popular machine learning algorithms.
We generalize the learning of substitute models from
deep learning to logistic regression and support vector
machines. Furthermore, we show that it is possible to
learn substitutes matching labels produced by many
machine learning models (DNN, LR, SVM, kNN) at
rates superior to 80%. We improve the accuracy and
computational cost of a previously proposed substi-
tute learning technique by introducing a new hyper-
parameter and the use of reservoir sampling.
We conduct black-box attacks against classiers hosted
by Amazon and Google. We show that despite our lack
of knowledge of the classier internals, we can force
them to respectively misclassify 96.19% and 88.94% of
their inputs using a logistic regression substitute model
trained by making only 800 queries to the target.
2. APPROACH OVERVIEW
In this section, we describe our approach, which is structured
around the evaluation of two hypotheses relevant to the de-
sign of black-box attacks against machine learning classiers.
Let us precisely dene adversarial sample transferability.
Consider an adversary interested in producing an adversar-
ial sample ~xmisclassied in any class dierent from the
class assigned by model fto legitimate input ~ x. This can
be done by solving2the following optimization problem [22]:
~x=~ x+~ xwhere~ x= arg min
~ zf(~ x+~ z)6=f(~ x) (1)
2Finding a closed form solution to this problem is not al-
ways possible, as some machine learning models fpreclude
the optimization problem from being linear or convex. Nev-
ertheless, several approaches have been proposed to nd ap-
proximative solutions to Equation 1. They yield adversar-
ial samples eectively misleading non-linear and non-convex
models like neural networks [22, 12, 19]. In addition, we in-
troduce new techniques to craft adversarial samples against
support vector machines and decision trees in Section 6.ML Dierentiable Linear Lazy
Technique Model Model Prediction
DNN Yes No No
LR Yes Log-linear No
SVM No No No
DT No No No
kNN No No Yes
Ens No No No
Table 1: Machine Learning Techniques studied in Section 3
Samples~xsolving Equation 1 are specically computed to
mislead model f. However, as stated previously, such adver-
sarial samples are in practice also frequently misclassied by
modelsf0dierent from f. To facilitate our discussion, we
formalize this adversarial sample transferability notion as:

X(f;f0) =
f0(~ x)6=f0(~ x+~ x) :~ x2X  (2)
where setXis representative of the expected input distri-
bution for the task solved by models fandf0. We partition
adversarial sample transferability in two variants character-
izing the pair of models ( f;f0). The rst, intra-technique
transferability , is dened across models trained with the
same machine learning technique but dierent parameter
initializations or datasets (e.g., fandf0are both neural net-
works or both decision trees). The second, cross-technique
transferability , considers models trained using two techniques
(e.g.,fis a neural network and f0a decision tree).
Hypothesis 1: Both intra-technique and cross-technique
adversarial sample transferabilities are consistently strong
phenomena across the space of machine learning techniques .
In this rst hypothesis, we explore how well both variants
of transferability hold across classes of machine learning al-
gorithms. The motivation behind this investigation is that
adversarial sample transferability constitutes a threat vector
against machine learning classiers in adversarial settings.
To identify the most vulnerable classes of models, we need
to generate an accurate comparison of the attack surface of
each class in constrained experimental settings.
To validate this hypothesis, we perform a large-scale study
in Section 3. Each of the study's two folds investigates
one of the adversarial sample transferability variants: intra-
technique and cross-technique. For completeness, we con-
sider a collection of models representatively spanning the
machine learning space, as demonstrated by Table 1. Mod-
els are trained on MNIST data [16] to solve the hand-written
digit recognition task. In the rst fold of the study, we mea-
sure intra-technique adversarial sample transferability rates

X(f;f0), for each machine learning technique, across mod-
els trained on dierent subsets of the data. In the second fold
of the study, we measure inter-technique adversarial sample
transferability rates 
 X(f;f0) across models corresponding
to all possible pairs of machine learning techniques.
Hypothesis 2: Black-box attacks are possible in practical
settings against any unknown machine learning classier.
Our motivation is to demonstrate that deployment of ma-
chine learning in settings where there are incentives for ad-versaries to have models misbehave must take into account
the practical threat vector of adversarial samples. Indeed, if
black-box attacks are realistic in practical settings, machine
learning algorithm inputs must be validated as being part of
the expected distribution of inputs. As is the case for SQL
injections, the existence of adversarial samples calls for input
validation in production systems using machine learning.
The verication of this second hypothesis is two-fold as well.
In Section 4, we show how to transfer the generalization
knowledge of any machine learning classiers into a substi-
tute model by querying the classier for labels on carefully
selected inputs. In Section 5, we perform black-box attacks
against commercial machine learning classiers hosted by
Amazon and Google. As we validate the hypothesis through-
out Sections 4 and 5, we operate under the specic threat
model of an oracle, described in [20], which characterizes re-
alistic adversarial settings. Instead of having full knowledge
of the model's architecture fand its parameters , as was
the case for the rst hypothesis validation in Section 3, we
now assume the adversary's only capability is to observe the
label predicted by the model fon inputs of its choice.
3. TRANSFERABILITY OF ADVERSARIAL
SAMPLES IN MACHINE LEARNING
In this section, our working hypothesis is that intra-technique
and cross-technique adversarial sample transferability are
strong phenomena across the machine learning space. Thus,
we empirically study these two phenomena across a range of
machine learning techniques: deep neural networks (DNNs),
logistic regression (LR), support vector machines (SVM), de-
cision trees (DT), nearest neighbors (kNN), and ensembles
(Ens.). All models are found vulnerable to intra-technique
adversarial sample transferability|misclassication of sam-
ples by dierent models trained using the same machine
learning technique, the phenomenon is stronger for dieren-
tiable models like DNNs and LR than for non-dierentiable
models like SVMs, DTs and kNNs. Then, we observe that
DNNs and kNNs boast resilience to cross-technique trans-
ferability, misclassications of adversarial samples by models
trained with distinct machine learning techniques. We nd
that all other models, including LR, SVMs, DTs, and an
ensemble of models collectively making predictions, are con-
siderably more vulnerable to cross-technique transferability.
3.1 Experimental Setup
We describe here the dataset and machine learning models
used in this section to study both types of transferability.
Dataset - We use the seminal MNIST dataset of handwrit-
ten digits [16]. This dataset has been well-studied in both
the machine learning and security communities. We chose it
because its dimensionality is suitable to the range of machine
learning techniques included in our study, which all perform
at least reasonably well on this dataset. The task associated
with the dataset is classication of images in one of the 10
classes corresponding to each possible digit ranging from 0
to 9. The dataset includes 50 ;000 training samples, 10 ;000
validation samples, and 10 ;000 test samples. Each 28x28
gray-scale pixel image is encoded as a vector of intensities
whose real values range from 0 (black) to 1 (white).Machine learning models - We selected ve machine
learning techniques: DNNs, LR, SVMs, DTs, and kNNs.
All of these machine learning techniques, as well as the al-
gorithms used to craft adversarial samples, are presented in
Section 6 of this paper. As outlined in Table 1, DNNs were
chosen for their state-of-the-art performance, LR for its sim-
plicity, SVMs for their potential robustness stemming from
the margin constraints when choosing decision boundaries
at training, DTs for their non-dierentiability, and kNNs
for being lazy-classication3models. To train DNN, LR,
and kNN models, we use Theano [3] and Lasagne [2]. The
DNN is made up of a hierarchy of 2 convolutional layers of
32 3x3 kernels, 2 convolutional layers of 64 3x3 kernels, 2
rectied linear layers of 100 units, and a softmax layer of 10
units. It is trained during 10 epochs with learning, momen-
tum, and dropout rates of respectively 10 2, 0:9, and 0:5
decayed by 0 :5 after 5 epochs. The LR is performed using
a softmax regression on the inputs. It is trained during 15
epochs at a learning rate of 10 2with a momentum rate of
0:9 both decayed by 0 :5 after 10 epochs. The linear SVM
and DT are trained with scikit-Learn.
3.2 Intra-technique Transferability
We show that dierentiable models like DNNs and LR are
more vulnerable to intra-technique transferability than non-
dierentiable models like SVMs, DTs, and kNNs. We mea-
sure intra-technique transferability between models iandj,
both learned using the same machine learning technique, as
the proportion of adversarial samples produced to be mis-
classied by model ithat are misclassied by model j.
To train dierent models using the same machine learn-
ing technique, we split the training set in disjoint subsets
A,B,C,D,E of 10 ;000 samples each, in order of increasing
indices. For each of the machine learning techniques (DNN,
LR, SVM, DT, kNN), we thus learn ve dierent models
referred to as A,B,C,D,E. Model accuracies, i.e. the pro-
portion of labels correctly predicted by the model for the
testing data, are reported in Figure 2a. For each of the 25
models, we apply the suitable adversarial sample algorithm
described in Section 3.2 and craft 10 ;000 samples from the
test set, which was unused during training. For adversar-
ial sample algorithms with parameters, we ne-tune them
to achieve a quasi-complete misclassication of the 10 ;000
adversarial samples by the model on which they are crafted.
Upon empirically exploring the input variation parameter
space, we set it to "= 0:3 for the fast gradient sign method
algorithm, and "= 1:5 for the SVM algorithm.
Figures 2b-2f report intra-technique transferability rates for
each of the ve machine learning techniques. Rates ( i;i) on
the diagonals indicate the proportion of adversarial samples
misclassied precisely by the same model ion which they
were crafted. O-diagonal rates ( i;j) indicate the propor-
tion of adversarial samples misclassied by a model jdier-
ent from the model ion which they were crafted. We rst
observe that all models are vulnerable to intra-technique
transferability in a non-negligible manner. LR models are
most vulnerable as adversarial samples transfer across mod-
els at rates larger than 94%. DNN models display similarly
3No model is learned during training. Predictions are made
by ndingkpoints closest to the sample in the training data,
and extrapolating its class from the class of these kpoints.A B C D E
Training SubsetDNN
LR
SVM
DT
kNNMachine Learning Technique97.72 97.91 97.91 97.6 97.62
82.57 83.45 84.07 83.16 82.98
88.9 89.07 89.29 88.84 88.9
80.64 81.57 80.94 81.78 81.55
94.42 94.92 94.83 94.91 94.44(a) Model Accuracies
A B C D E
Target DNNA
B
C
D
ESource DNN81 67 66 49 54
71 86 75 53 58
67 70 84 52 57
64 64 65 68 57
75 73 74 57 80 (b) DNN models
A B C D E
Target LRA
B
C
D
ESource LR98 95 95 95 95
95 98 95 95 94
94 94 98 95 95
94 95 95 98 95
95 95 95 95 98
(c) LR models
A B C D E
Target SVMA
B
C
D
ESource SVM99 41 38 40 41
34 99 32 46 34
36 41 99 38 45
37 43 37 99 38
39 37 47 37 99 (d) SVM models
A B C D E
Target DTA
B
C
D
ESource DT94 22 20 22 23
22 87 21 21 19
21 19 94 21 23
20 21 20 95 19
20 19 27 19 93
(e) DT models
A B C D E
Target kNNA
B
C
D
ESource kNN55 29 29 29 29
29 55 29 29 29
29 28 55 28 29
29 29 29 56 28
28 29 29 29 56 (f) kNN models
Figure 2: intra-technique transferability for 5 ML tech-
niques. Figure 2a reports the accuracy rates of the 25 models
used, computed on the MNIST test set. Figures 2b-2f are
such that cell ( i;j) reports the intra-technique transferabil-
ity between models iandj, i.e. the percentage of adversarial
samples produced using model imisclassied by model j.
important transferability, with rates of at least 49%. On the
SVM, DT, and kNN matrices, the diagonals stand out more,
indicating that these techniques are to some extent more ro-
bust to the phenomenon. In the case of SVMs, this could be
explained by the explicit constraint during training on the
choice of hyperplane decision boundaries that maximize the
margins (i.e. support vectors). The robustness of both DTs
and kNNs could simply stem from their non-dierentiability.
3.3 Cross-technique Transferability
We dene cross-technique transferability between models i
andj, trained using dierent machine learning techniques,
as the proportion of adversarial samples produced to be mis-classied by model ithat are also misclassied by model
j. Hence, this is a more complex phenomenon than intra-
technique transferability because it involves models learned
using possibly very dierent techniques like DNNs and DTs.
Yet, cross-technique transferability is surprisingly a strong
phenomenon to which techniques like LR, SVM, DT, and
ensembles are vulnerable, making it easy for adversaries to
craft adversarial samples misclassied by models trained us-
ing diverse machine learning techniques.
We study the cross-technique transferability phenomenon
across models trained using the ve machine learning tech-
niques already used in Section 3.2 and described in Sec-
tion 3.1 and 6. To these, we add a 6th model: an ensemble
f(~ x). The ensemble fis implemented using a collection of
5 experts, which are the 5 previously described models: the
DNN denoted f1, LR denoted f2, SVM denoted f3, DT de-
notedf4, and kNN denoted f5. Each expert makes a decision
and the ensemble outputs the most frequent choice (or the
class with the lowest index if they all disagree):
f(~ x) = arg max
i20::N 1X
j21::5fj;i(~ x) (3)
wherefj;i(~ x) = 1fj(~ x)==iindicates whether classier fjas-
signed class ito input~ x. Note that in this section, we only
train one model per machine learning technique on the full
MNIST training set of 50 ;000 samples, unlike in Section 3.2.
In this experiment, we are interested in transferability across
machine learning techniques. As such, to ensure our results
are comparable, we ne-tune the parameterizable crafting
algorithms to produce adversarial samples with similar per-
turbation magnitudes. To compare magnitudes across per-
turbation styles, we use the L1 norm: the sum of each per-
turbation component's absolute value. Perturbation added
to craft adversarial samples using the DNN, LR, and SVM
have an average L1 norm k~ xk1of 11:5%. To achieve this,
we use an input variation parameter of "= 0:25 with the
fast gradient sign method on the DNN, LR, and kNN. To
craft adversarial samples on the SVM, we use an input vari-
ation parameter of "= 5 with the crafting method intro-
duced in Section 6. Unfortunately, the attack on DT cannot
be parameterized to match the L1 norm of DNN, LR, kNN
and SVM attacks. Hence, perturbations selected have much
lower average L1 norms of respectively 1 :05%.
We build a cross-technique transferability matrix where each
cell (i;j) holds the percentage of adversarial samples pro-
duced for classier ithat are misclassied by classier j. In
other words, rows indicate the machine learning technique
that trained the model against which adversarial samples
were crafted. The row that would correspond to the ensem-
ble is not included because there is no crafting algorithm
designed to produce adversarial samples specically for an
ensemble, although we address this limitation in Section 4
using insight gained in this experiment. Columns indicate
the underlying technique of the classier making predictions
on adversarial samples. This matrix, plotted in Figure 3,
shows that cross-technique transferability is a strong but
heterogeneous phenomenon. The most vulnerable model is
the decision tree (DT) with misclassication rates ranging
from 47:20% to 89:29% while the most resilient is the deep
neural network (DNN) with misclassication rates betweenFigure 3: cross-technique Transferability matrix: cell ( i;j)
is the percentage of adversarial samples crafted to mislead
a classier learned using machine learning technique ithat
are misclassied by a classier trained with technique j.
0:82% and 38 :27%. Interestingly, the ensemble is not re-
silient to cross-technique transferability of adversarial sam-
ples with rates reaching 44 :14% for samples crafted using
the LR model. This is most likely due to the vulnerability
of each underlying expert to adversarial samples.
We showed that all machine learning techniques we studied
are vulnerable to two types of adversarial sample transfer-
ability. This most surprisingly results in adversarial sam-
ples being misclassied across multiple models learned with
dierent machine learning techniques. This cross-technique
transferability greatly reduces the minimum knowledge that
adversaries must possess of a machine learning classier in
order to force it to misclassify inputs that they crafted. We
leverage this observation, along with ndings from Section 4,
to justify design choices in the attack described in Section 5.
4. LEARNING CLASSIFIER SUBSTITUTES
BY KNOWLEDGE TRANSFER
In the previous section, we identied machine learning tech-
niques (e.g., DNNs and LR) yielding models adequate for
crafting samples misclassied across models trained with dif-
ferent techniques, i.e adversarial samples with strong cross-
technique transferability. Thus, in order to craft adversarial
samples misclassied by a classier whose underlying model
is unknown, adversaries can instead use a substitute model if
it solves the same classication problem and its parameters
are known. Therefore, eciently learning substitutes is key
to designing black-box attacks where adversaries target re-
mote classiers whose model, parameters, and training data
are unknown to them. This is precisely the attack scenario
evaluated against commercial machine learning platforms in
Section 5, while we focus in this section on the prerequisite
learning of substitutes for machine learning classiers.
We enhance an algorithm introduced in [20] to learn a sub-
stitute model for a given classier simply by querying it for
labels on carefully chosen inputs. More precisely, we in-
troduce two renements to the algorithm: one improves itsaccuracy and the second reduces its computational complex-
ity. We generalize the learning of substitutes to oracles using
a range of machine learning techniques: DNNs, LR, SVMs,
DTs, and kNNs. Furthermore, we show that both DNNs and
LR can be used as substitute models for all machine learning
techniques studied to the exception of decision trees.
4.1 Dataset Augmentation for Substitutes
The targeted classier is designated as an oracle because
adversaries have the minimal capability of querying it for
predictions on inputs of their choice. The oracle returns
thelabel (notthe probabilities) assigned to the sample. No
other knowledge of the classier (e.g., model type, parame-
ters, training data) is available. To circumvent this, we build
on a technique introduced in [20], which leverages a dataset
augmentation technique to train the substitute model.
Jacobian-based dataset augmentation - We use this
augmentation technique introduced in [20] to learn DNN
and LR substitutes for oracles. First, one collects an initial
substitute training set of limited size (representative of the
task solved by the oracle) and labels it by querying the ora-
cle. Using this labeled data, we train a rst substitute model
flikely to perform poorly as a source of adversarial samples
due to the small numbers of samples used for training. To
select additional training points, we use the following:
S+1=f~ x+sgn(Jf[~O(~ x)] :~ x2S)g[S (4)
whereSandS+1are the previous and new training sets, 
a parameter ne-tuning the augmentation step size, Jfthe
Jacobian matrix of substitute f, and ~O(~ x) the oracle's label
for sample ~ x. We train a new instance fof the substitute
with the augmented training set S+1, which we can label
simply by querying oracle ~O. By alternatively augmenting
the training set and training a new instance of the substitute
model for multiple iterations , Papernot et al. showed that
substitute DNNs can approximate another DNNs [20].
Periodical Step Size - When introducing the technique,
Papernot et al. used a xed step size parameter through-
out the substitute learning iterations . In this section,
we show that by having a step size periodically alternat-
ing between positive and negative values, one can improve
the quality of the oracle approximation made by the sub-
stitute, which we measure in terms of the number of labels
matched with the original classier oracle. More precisely,
we introduce an iteration period after which the step size
is multiplied by 1. Thus, the step size is dened as:
=( 1)b
c(5)
whereis set to be the number of epochs after which the
Jacobian-based dataset augmentation does not lead any sub-
stantial improvement in the substitute. A grid search can
also be performed to nd an optimal value for the period .
We also experimented with a decreasing grid step amplitude
, but did not nd that it yielded substantial improvements.
Reservoir Sampling - We also introduce the use of reser-
voir sampling [23] as a mean to reduce the number of queries
made to the oracle. This is useful when learning substitutes
in realistic environments where the number of label queries
an adversary can make without exceeding a quota or beingAlgorithm 1 Jacobian-based augmentation with Reservoir
Sampling: sets are considered as arrays for ease of notation.
Input:S 1,,Jf,
1:N jS 1j
2: Initialize Sas array of N+items
3:S[0 :N 1] S 1
4:fori20:: 1do
5:S[N+i] S 1[i] +sgn(Jf[~O(S 1[i])])
6:end for
7:fori2::N 1do
8:r random integer between 0 and i
9: ifr< then
10:S[N+r] S 1[i] +sgn(Jf[~O(S 1[i])])
11: end if
12:end for
13:returnS
detected by a defender is constrained. Reservoir sampling
is a class of algorithms that randomly select samples from
a list of samples. The total number of samples in the list
can be both very large and unknown. In our case, we use
reservoir sampling to select a limited number of new inputs
when performing a Jacobian-based dataset augmentation.
This prevents the exponential growth of queries made to the
oracle at each augmentation iteration. At iterations  > 
(the rstiterations are performed normally), when con-
sidering the previous set S 1of substitute training inputs,
we selectinputs from S 1to be augmented in S. These
inputs are selected using reservoir sampling, as described
in Algorithm 1. This technique ensures that each input in
S 1has an equal probability1
jS 1jto be augmented in
S. The number of queries made to the oracle is reduced
fromn2for the vanilla Jacobian-based augmentation to
n2+( ) for the Jacobian-based augmentation with
reservoir sampling. Our experiments show that the reduced
number of training points in the reservoir sampling variant
does not signicantly degrade the quality of the substitute.
4.2 Deep Neural Network Substitutes
In [20], the oracle classier approximated was always a DNN.
However, the authors concluded with preliminary results
suggesting applicability to a nearest neighbors classier. We
here show that in fact the technique is generalizable and ap-
plicable to many machine learning techniques by evaluating
its performance on 5 types of ML classiers: a DNN, LR,
SVM, DT, and kNN. This spectrum is representative of ma-
chine learning (cf. Section 3.1). Our experiments suggest
that one can accurately transfer the knowledge from many
machine learning classiers to a DNN and obtain a DNN
mimicking the decision boundaries of the original classier.
Using the Jacobian-based augmentation technique, we train
5 dierent substitute DNNs to match the labels produced
by 5 dierent oracles, one for each of the ML techniques
mentioned. These classiers serving as oracles are all trained
on the 50;000 sample MNIST training set using the models
described previously in Section 3.1. To approximate them,
we use the rst 100 samples from the MNIST test set (unseen
during training) as the initial substitute training set and
follow three variants of the procedure detailed in Section 4.1
with= 0:1: (1) vanilla Jacobian-based augmentation, (2)Substitute DNN LR SVM DT kNN
DNN 78.01 82.17 79.68 62.75 81.83
DNN+PSS 89.28 89.16 83.79 61.10 85.67
DNN+PSS+RS 82.90 83.33 77.22 48.62 82.46
LR 64.93 72.00 71.56 38.44 70.74
LR+PSS 69.20 84.01 82.19 34.14 71.02
LR+PSS+RS 67.85 78.94 79.20 41.93 70.92
Table 2: Impact of our renements, Periodic Step Size (PSS)
and Reservoir Sampling (RS), on the percentage of label
predictions matched between the substitutes and their target
classiers on test data after = 9 substitute iterations.
with= 3 periodic step size, (3) with both = 3 periodic
step size and reservoir sampling with parameters = 3 and
= 400. The substitute architecture is identical to the DNN
architecture from Section 3.1. We allow experiments to train
substitutes for 10 augmentation iterations, i.e. 9.
Figure 4a plots at each iteration the share of samples on
which the substitute DNNs agree with predictions made by
the classier oracle they are approximating. This proportion
is estimated by comparing the labels assigned to the MNIST
test set by the substitutes and oracles before each iteration
of the Jacobian-based dataset augmentation. The substi-
tutes used in this gure were all trained with both a periodic
step size and reservoir sampling, as described previously.
Generally speaking, all substitutes are able to successfully
approximate the corresponding oracle, after = 10 augmen-
tation iterations, the labels assigned match for about 77%
to 83% of the MNIST test set, except for the case of the DT
oracle, which is only matched for 48% of the samples. This
dierence could be explained by the non-dierentiability of
decisions trees. On the contrary, substitute DNNs are able
to approximate the nearest neighbors oracle although it uses
lazy classication: no model is learned at training time and
predictions are made by nding close training sample(s).
The rst three rows of Table 2 quantify the impact of the
two renements introduced above on the proportion of test
set labels produced by the oracle that were matched by DNN
substitutes. The rst renement, the periodic step size, al-
lows substitutes to approximate more accurately their target
oracle. For instance at = 9 iterations, the substitute DNN
trained with a periodic ste size for the DNN oracle matches
89:28% of the labels whereas the vanilla substitute DNN
only matched 78 :01%. Similarly, the substitute DNN trained
with a periodic ste size for the SVM oracle matches 83 :79%
of the labels whereas the vanilla substitute only matched
79:68%. The second renement, reservoir sampling allows us
to train substitutes for more augmentation iterations with-
out making too many queries to the oracle. For instance, 10
iterations with reservoir sampling (using = 3 and= 400)
make 10023+ 400(10 3) = 3;600 queries to the oracle
instead of 102 ;400 queries with the vanilla technique. The
reduced number of queries has an impact on the substitute
quality compared to the periodic step size substitutes but
it is still superior to the vanilla substitutes. For instance,
when approximating a DNN oracle, the vanilla substitute
matched 7;801 labels, the periodic step size one 8 ;928, and
the periodic step size with reservoir sampling one 8 ;290.(a) DNN substitutes
(b) LR substitutes
Figure 4: Label predictions matched between the DNN and
LR substitutes and their target classier oracles on test data.
4.3 Logistic Regression Substitutes
Having generalized substitute learning with a demonstration
of the capacity of DNNs to approximate any machine learn-
ing model, we now consider replacing the substitute itself by
another machine learning technique. Experiments in Sec-
tion 3.3 led us to conclude that cross-technique transferabil-
ity is not specic to adversarial samples crafted on DNNs,
but instead applies to many learning techniques. Looking
at Figure 3 again, a natural candidate is logistic regression,
as it displays large cross-technique transferability rates su-
perior to DNNs except when targeting DNNs themselves.
The Jacobian-based dataset augmentation's implementation
for DNNs is easily adapted to multi-class logistic regression.
Indeed, multi-class logistic regression is analog to the soft-
max layer frequently used by deep neural networks to pro-
duce class probability vectors. We can easily compute the
(i;j) component of the Jacobian of a multi-class LR model:
Jf(~ x)[i;j] =wje~ wj[i]~ x PN
l=1~ wl[i]ewl~ x
PN
l=1e~ wl[i]~ x2(6)
where notations are the ones used in Equation 9.Hence, we repeat the experiment from Section 4.2 but we
now train multi-class logistic regression substitute models
(instead of the DNN substitutes) to match the labels pro-
duced by the classier oracles. Everything else is unchanged
in the experimental setup. As illustrated in Figure 4b, the
change of model type for the substitute generally speaking
degrades the approximation quality: the proportion of la-
bels matched is reduced. Performances of LR substitutes
are competitive with those of DNN substitutes for LR and
SVM oracles. Here again, the substitutes perform poorly on
the decision tree oracle, with match rates barely above 40%.
The last three rows of Table 2 quantify the impact of the
two renements introduced above on the proportion of test
set labels produced by the oracle that were matched by LR
substitutes. The rst renement, the periodic step size, al-
lows LR substitutes to approximate more accurately their
target oracle, as was also the case for DNN substitutes. For
instance at = 9 iterations, the LRsubstitute trained with a
periodic ste size for the LR oracle matches 84 :01% of the la-
bels whereas the vanilla LR substitute only matched 72 :00%.
Similarly, the LR substitute trained with a periodic ste size
for the SVM oracle matches 82 :19% of the labels whereas
the vanilla substitute only matched 71 :56%. The second re-
nement, reservoir sampling allows us to reduce the number
of queries with a limited impact on the substitute quality:
less labels are match than the periodic step size substitutes
but more than the vanilla substitutes. For instance, when
approximating a SVM oracle, the vanilla substitute matched
71:56% of the labels, the periodic step size one 82 :19%, and
the periodic step size with reservoir sampling one 79 :20%.
The benet of vanilla LR substitutes compared to DNN sub-
stitutes is that they achieve their asymptotic match rate
faster, after only = 4 augmentation iterations, correspond-
ing to 1;600 oracle queries. Furthermore, LR models are
much lighter in terms of computational cost. These two fac-
tors could justify the use of LR (instead of DNN)substitutes
in some contexts. The reservoir sampling technique gives
good performances, especially on LR and SVM oracles.
4.4 Support Vector Machines Substitutes
Having observed that deep learning and logistic regression
were both relevant when approximating classier oracles, we
now turn to SVMs for substitute learning. This is motivated
by the strong cross-technique transferability of adversarial
sample crafted using an SVM observed in Section 3, making
SVMs good candidates for substitutes in a black-box attack.
SVM-based dataset augmentation - To train SVMs to
approximate oracles in a manner analogous to the Jacobian-
based dataset augmentation, we introduce a new augmenta-
tion technique. We replace the heuristic in Equation 4 by
the following, which is adapted to the specicities of SVMs:
S+1=f~ x ~ w[~O(~ x)]~ w[~O(~ x)]~ x:~ x2S)g[S (7)
where~ w[k] is the weight indicating the hyperplane direction
of subclassier kused to implement a multi-class SVM with
the one-vs-the-rest scheme as detailed in Equation 12. This
heuristic selects new points in the direction orthogonal to
the hyperplane acting as the decision boundary for the bi-nary SVM subclassier kcorresponding to the input's label.
This is precisely the direction used in Equation 13 to nd
adversarial samples but parameter is here generally set to
lower values so as to nd samples near the decision bound-
ary instead of on the other side of the decision boundary.
Experimental Validation - We repeat the experiments
from Sections 4.2 and 4.3 but we now train 18 dierent SVM
models to match labels produced by the classiers|instead
of training DNN or LR substitutes. Unfortunately, our re-
sults suggest that SVMs are unable to perform knowledge
transfer from oracles that are not SVMs themselves using the
dataset augmentation technique introduced in Equation 7,
as well as the renements introduced previously: the pe-
riodic step size and reservoir sampling. Indeed, the SVM
substitute matches 79 :80% of the SVM oracle labels, but
only 11:98% and 11 :97% of the DNN and LR oracle labels.
These numbers are not improved by the use of a periodic
step size and/or reservoir sampling. This could be due to
the specicity of SVM training and the decision boundaries
they learn. Future work should investigate the use of alter-
native augmentation techniques to conrm our ndings.
In this section, we evaluated the capacity of DNN, LR, and
SVM substitutes to approximate a classier oracle by query-
ing it for labels on inputs selected using a heuristic relying
on the substitute's Jacobian. We observed that predictions
made by DNN and LR substitutes more accurately matched
the targeted oracles than SVM substitute predictions. We
emphasize that all experiments only required knowledge of
100 samples from the MNIST test set. In other words, learn-
ing substitutes does not require knowledge of the targeted
classier's type, parameters, or training data, and can thus
be performed under realistic adversarial threat models.
5. BLACK-BOX ATTACKS OF REMOTE
MACHINE LEARNING CLASSIFIERS
Intra-technique and cross-technique transferability of adver-
sarial samples, together with the learning of substitutes for
classier oracles, enable a range of attacks targeting re-
mote machine learning based systems whose internals are
unknown to adversaries. To illustrate the feasibility of black-
box attacks on such remote systems, we target in an exper-
iment two machine learning classiers respectively trained
and hosted by Amazon and Google. We nd it is possible
to craft samples misclassied by these commerical oracles
at respective rates of 96 :19% and 88 :94% after making 800
queries to learn substitute models approximating them.
5.1 The Oracle Attack Method
This section's adversarial threat model is identical to the
one used when learning substitutes in Section 4: adversaries
have an oracle access to the remote classier. Its type, pa-
rameters, or training set are all unknown to the adversary.
The attack method leverages Sections 3 and 4 of this paper,
and is a generalization of the approach introduced in [20].
The adversary rst locally trains a substitute model to ap-
proximate the remotely hosted classier, using queries to the
oracle as described in Section 4. We consider the use of deep
learning and logistic regression to learn substitutes for classi-
ers. We apply the two renements introduced in this paper:a periodic step size and reservoir sampling. Since substitute
models are locally trained, the adversary has full knowledge
of their model parameters. Thus, one of the adversarial
sample crafting algorithms introduced in Section 6 corre-
sponding to the machine learning technique used to learn
the substitute are employed to craft adversarial samples
misclassied by the substitute model. The adversary than
leverages either intra-technique or cross-technique transfer-
ability of adversarial samples|depending on the techniques
with which the substitute and oracle were learned: the in-
puts misleading the locally trained substitute model are very
likely to also deceive the targeted remotely hosted oracle.
Previous work conducted such an attack using a substi-
tute and targeted classier both trained using deep learning,
demonstrating that the attack was realistic using the Meta-
Mind API providing Deep Learning as a Service [20]. We
generalize these results by performing the attack on Machine
Learning as a Service platforms that employ techniques that
are unknown to us: Amazon Web Services and Google Cloud
Prediction. Both platforms automate the process of learn-
ing classiers using a labeled dataset uploaded by the user.
Unlike MetaMind, neither of these platforms claim to exclu-
sively use deep learning to build classiers. When analyzing
our results, we found that Amazon uses logistic regression
(cf. below) but to the best of our knowledge Google has
never disclosed the technique they use to train classiers,
ensuring that our experiment is properly blind-folded.
5.2 Amazon Web Services Oracle
Amazon oers a machine learning service, Amazon Machine
Learning ,4as part of their Amazon Web Services platform.
We used this service to train and host a ML classier oracle.
First, we uploaded a CSV encoded version of the MNIST
training set to an S3 bucket on Amazon Web Services. We
truncated the pixel values in the CSV le to 8 decimal places.
We then started the ML model training process on the Ma-
chine Learning service: we loaded the CSV training data
from our S3 bucket, selected the multi-class model type, pro-
vided the target column in the CSV le, and kept the default
conguration settings. Note that Amazon oers limited cus-
tomization options: the settings allow one to customize the
recipe (data transformations), specify a maximum model
size and number of training epochs, disable training data
shue, and change the regularization type between L1 and
L2 or simply disable regularization. The training process
takes a few minutes and outputs a classier model achiev-
ing a 92:17% accuracy on the MNIST test set. We have no
way to improve that performance beyond the limited cus-
tomizing options as the intent of the service is to automate
model training. Finally, we activate real-time predictions to
be able to query the model for labels from our local machine.
We then use the Python API provided with the Amazon
Machine Learning service to submit prediction queries to
our trained oracle model and retrieve the output label. Al-
though condence values are available for predictions, we
only consider the label to ensure our threat model for ad-
versarial capabilities remains realistic. We incorporate this
oracle in our experimental setup and train two substitute
models to approximate the labels produced by this oracle,
4https://aws.amazon.com/machine-learningSubstitute type DNN LR
= 3 (800 queries) 87.44% 96.19%
= 6 (6,400 queries) 96.78 % 96.43%
= 6 (PSS + RS) (2,000 queries) 95.68% 95.83%
Table 3: Misclassication rates of the Amazon oracle on
adversarial samples ( "= 0:3) produced with DNN and LR
substitutes after =f3;6gaugmentation iterations. Sub-
stitutes are trained without and with renements from Sec-
tion 4: periodic step size (PSS) and reservoir sampling (RS).
a DNN and LR, as SVM substitutes were dismissed by the
conclusions of Section 4. We train two variants of the DNN
and LR substitutes. The rst variant is trained with the
vanilla dataset augmentation and the second variant with
the enhanced dataset augmentation introduced in this pa-
per, which uses both a periodic step size and reservoir sam-
pling. Learning is initialized with a substitute training set
of 100 samples from the MNIST test set. For all substitutes,
we measure the attack success as the proportion among the
10;000 adversarial samples, produced using the fast gradi-
ent sign method with parameter "= 0:3 (cf. Section 6) and
the MNIST test set, misclassied by the Amazon oracle.
Misclassication rates of the Amazon Machine Learning or-
acle on adversarial samples crafted using both the DNN and
LR substitutes after 2f3;6gdataset augmentation iter-
ations are reported in Table 3. Results are given for mod-
els learned without and with the two renements|periodic
step size (PSS) and reservoir sampling (RS)|introduced in
Section 4. With a misclassication rate of 96 :19% for an ad-
versarial perturbation "= 0:3 using a LR substitute trained
with 800 queries ( = 3) to the oracle, the model trained
by Amazon is easily misled. To understand why, we care-
fully read the online documentation and eventually found
one page indicating that the type of model trained by the
Amazon Machine Learning service is an \industry-standard"
multinomial logistic regression.5As seen in Section 3, LR is
extremely vulnerable to intra-technique and to a lesser ex-
tend vulnerable to cross-technique transferability. In fact, as
pointed out by Goodfellow et al. [12], shallow models like lo-
gistic regression are unable to cope with adversarial samples
and learn a classier resistant to them. This explains why
(1) the attack is very successful and (2) the LR substitute
performs better than the DNN substitute.
Additionally, Table 3 shows how the use of a periodic step
size (PSS) together with reservoir sampling (RS) allows us
to reduce the number of queries made to the Amazon ora-
cle while learning a DNN substitute producing adversarial
samples with higher transferability to the targeted classier.
Indeed, we reduce by a factor of more than 3 the number
of queries made from 6 ;400 to 2;000, while only degrad-
ing the misclassication rate from 96 :78% to 95:68%|still
larger than the rate of 87 :44% achieved after 800 queries
by the substitute learned without PSS and RS. For the LR
substitutes, we do not see any positive impact from the use
of PSS and RS, which is most likely to the fast convergence
of LR substitute learning, as observed in Section 4.
5http://docs.aws.amazon.com/machine-learning/
latest/dg/types-of-ml-models.htmlSubstitute type DNN LR
= 3 (800 queries) 84.50% 88.94%
= 6 (6,400 queries) 97.17% 92.05%
= 6 (PSS + RS) (2,000 queries) 91.57% 97.72%
Table 4: Misclassication rates of the Google oracle on ad-
versarial samples ( "= 0:3) produced with DNN and LR
substitutes after =f3;6gaugmentation iterations.. Sub-
stitutes are trained without and with renements from Sec-
tion 4: periodic step size (PSS) and reservoir sampling (RS).
5.3 Google Cloud Prediction Oracle
To test whether this poor performance is limited to the
Amazon Web Services platform, we now target the Google
Cloud Prediction API service6. The procedure to train a
classier on Google's platform is similar to Amazon's. We
rst upload to Google's Cloud Storage service the CSV en-
coded le of the MNIST training data identical to the one
used to train the oracle on Amazon Machine Learning. We
then activate the Prediction API on Google's Cloud Plat-
form and train a model using the API's method named pre-
diction.trainedmodels.insert . The only property we are
able to specify is the expected multi-class nature of our clas-
sier model as well as the column in the CSV indicating tar-
get labels. We then evaluate the resulting model using the
API method prediction.trainedmodels.predict and an
uploaded CSV le of the MNIST test set. The API reports
an accuracy of 92% on this test set for the model trained.
We now use the Google Cloud Python API to connect our
experimental setup to the Prediction API, thus allowing our
algorithms to make queries to the Google classier oracle.
As we did for Amazon, we train two substitute models (DNN
and LR) using an initial substitute training set of 100 sam-
ples from the MNIST test set. For each substitute type, we
train two model variants: the rst one without periodic step
size (PSS) or reservoir sampling (RS), the second one with
both PSS and RS. Table 4 reports the rate of adversarial
samples produced by each of the four resulting substitutes
and misclassied by the Google Prediction API oracle.
The model trained using Google's machine learning service
is a little more robust to adversarial samples than the one
trained using Amazon's service, but is still vulnerable to a
large proportion of samples: 88 :94% of adversarial samples
produced with a perturbation "= 0:3 using a LR substitute
trained with 800 queries to the oracle are misclassied. This
conrms the above demonstration of the feasibility of black-
box attacks against the classier hosted by Amazon. Fur-
thermore, if we use PSS and RS, the misclassication rate
is 91:57% for the DNN substitute and 97 :72% for the LR
substitute, which again demonstrates that combining PSS
and RS increases misclassication compared to the original
method for = 3, and reduces by a factor of 3 the number of
queries (2;000) compared to the original method for = 6.
A brief discussion of defenses - In an eort to evaluate
possible defenses against such attacks, we now add these ad-
versarial samples to the MNIST training dataset and train
a new instance of the classier oracle with the same pro-
cedure. The new oracle has an accuracy of 91 :25% on the
6https://cloud.google.com/prediction/MNIST test set. Adversarial samples crafted by training a
new DNN substitute, even without PSS and RS, are still
misclassied at a rate of 94 :2% after= 3 iterations and
100% after = 6. This defense is thus not eective to pro-
tect the oracle from adversaries manipulating inputs. This
is most likely due to the fact that the Google Prediction
API uses shallow techniques to train its machine learning
models, but we have no means to verify this. One could also
try to deploy other defense mechanisms like defensive dis-
tillation [21]. Unfortunately, as we do not have any control
on the training procedure used by Google Cloud, we can-
not do so. To the best of our knowledge, Google has not
disclosed the machine learning technique they use to train
models served by their Google Cloud Prediction API service.
As such, we cannot make any further recommendations on
how to better secure models trained using this service.
6. ADVERSARIAL SAMPLE CRAFTING
This section describes machine learning techniques used in
this paper, along with methods used to craft adversarial
samples against classiers learned using these techniques.
Building on previous work [22, 12, 19] describing how adver-
saries can eciently select perturbations leading deep neu-
ral networks to misclassify their inputs, we introduce new
crafting algorithms for adversaries targeting Support Vector
Machines (SVMs) and Decision Trees (DTs).
6.1 Deep Neural Networks
Deep Neural Networks (DNNs) learn hierarchical represen-
tations of high dimensional inputs used to solve ML tasks [11],
including classication. Each representation is modeled by
a layer of neurons|elementary parameterized computing
units|behaving like a multi-dimensional function. The in-
put of each layer fiis the output of the previous layer fi 1
multiplied by a set of weights, which are part of the layer's
parameteri. Thus, a DNN fcan be viewed as a composi-
tion of parameterized functions
f:~ x7!fn(n;:::f 2(2;f1(1;~ x)):::)
whose parameters =figiare learned during training. For
instance, in the case of classication, the network is given a
large collection of known input-label pairs ( ~ x;y) and adjusts
its parameters to reduce the label prediction error f(~ x) y
on these inputs. At test time, the model extrapolates from
its training data to make predictions f(~ x) on unseen inputs.
To craft adversarial samples misclassied by DNNs, an ad-
versary with knowledge of the model fand its parameters
can use the fast gradient sign method introduced in [12]
or the Jacobian-based iterative approach proposed in [19].
We only provide here a brief description of the fast gradi-
ent sign method, which is the one we use in this work. To
nd an adversarial sample ~xapproximatively solving the
optimization problem stated in Equation 1, Goodfellow et
al. [12] proposed to compute the following perturbation:
~ x="sgn(r~ xc(f;~ x;y )) (8)
wherefis the targeted DNN, cits associated cost, and ythe
correct label of input ~ x. In other words, perturbations are
evaluated as the sign of the model's cost function gradient
with respect to inputs. An adversarial sample ~x=~ x+~ xis
successfully crafted when misclassied by model f|it sat-
isesf(~x)6=f(~ x)|while its perturbation ~ xremains in-distinguishable to humans. The input variation "sets the
perturbation magnitude: higher input variations yield sam-
ples more likely to be misclassied by the DNN model but
introduce more perturbation, which can be easier to detect.
6.2 Multi-class Logistic Regression
Multi-class logistic regression is the generalization of logistic
regression to classication problems with N > 2 classes [18].
Logistic regression seeks to nd the hypothesis best match-
ing the data among the class of hypothesis that are a compo-
sition of a sigmoid function over the class of linear functions.
A multi-class logistic regression model fcan be written as:
f:~ x7!"
e~ wj~ x
PN
l=1e~ wl~ x#
j21::N(9)
where=fw1;:::;wNgis the set of parameters learned dur-
ing training, e.g., by gradient descent or Newton's method.
Adversaries can also craft adversarial samples misclassied
by multi-class logistic regression models using the fast gra-
dient sign method [12]. In the case of logistic regression, the
method nds the most damaging perturbation ~ x(according
to the max norm) by evaluating Equation 8, unlike the case
of deep neural networks where it found an approximation.
6.3 Nearest Neighbors
The k nearest neighbor (kNN) algorithm is a lazy-learning
non-parametric classier [18]: it does not require a training
phase. Predictions are made on unseen inputs by considering
thekpoints in the training sets that are closest according to
some distance. The estimated class of the input is the one
most frequently observed among these kpoints. When kis
set to 1, as is the case in this paper, the classier is:
f:~ x7!Y
arg min
~ z2Xk~ z ~ xk2
2
(10)
which outputs one row of Y, the matrix of indicator vectors
encoding labels for the training data X.
Although the kNN algorithm is non-parametric, it is still
vulnerable to adversarial samples as pointed out in [20, 24].
In this paper, we used the fast gradient sign method to craft
adversarial samples misclassied by nearest neighbors. To
be able to dierentiate the models, we use a smoothed vari-
ant of the nearest neighbor classiers, which replaces the
argmin operation in Equation 11 by a soft-min, as follows:
f:~ x7!h
e k~ z ~ xk2
2i
~ z2XP
~ z2Xe k~ z ~ xk2
2Y (11)
6.4 Multi-class Support Vector Machines
One possible implementation of a multiclass linear Support
Vector Machine classier fis the one-vs-the-rest scheme.
For each class kof the machine learning task, a binary Sup-
port Vector Machine classier fkis trained with samples of
classklabeled as positive and samples from other classes la-
beled as negative [8]. To classify a sample, each binary linear
SVM classier fkmakes a prediction and the overall mul-
ticlass classier foutputs the class assigned the strongest
condence. Each of these underlying linear SVMs is a model
fkclassifying unseen samples ~ xusing the following:
fk:~ x7!sgn(~ w[k]~ x+bk) (12)~wk~x~x⇤"NegativeOutput Class
PositiveOutput ClassFigure 5: SVM Adversarial Samples: to move a sample ~ x
away from its legitimate class in a binary SVM classier fk,
we perturb it by "along the direction orthogonal to~w[k].
We now introduce an algorithm to nd adversarial samples
misclassied by a multi-class linear SVM f. To the best
of our knowledge, this method is more computationally e-
cient than previous [4]: it does not require any optimization.
To craft adversarial samples, we perturb a given input in a
direction orthogonal to the decision boundary hyperplane.
More precisely, we perturb legitimate samples correctly clas-
sied by model fin the direction orthogonal to the weight
vector~ w[k] corresponding to the binary SVM classier fk
that assigned the correct class koutput by the multiclass
modelf. The intuition, illustrated in Figure 5 with a binary
SVM classier, can be formalized as follows: for a sample ~ x
belonging to class k, an adversarial sample misclassied by
the multiclass SVM model fcan be computed by evaluating:
~x=~ x "~w[k]
k~ wkk(13)
wherekkis the Frobenius norm,~w[k] the weight vector of
binary SVM k, and"theinput variation parameter. The
input variation parameter controls the amount of distortion
introduced as is the case in the fast gradient sign method.
6.5 Decision Trees
Decision trees are dened by recursively partitioning the in-
put domain [18]. Partitioning is performed by selecting a
feature and a corresponding condition threshold that best
minimize some cost function over the training data. Each
node is a if-else statement with a threshold condition corre-
sponding to one of the sample's features. A sample is clas-
sied by traversing the decision tree from its root to one of
its leaves accordingly to conditions specied in intermediate
tree nodes. The leaf reached indicates the class assigned.
Adversaries can also craft adversarial inputs misclassied by
decision trees. To the best of our knowledge, this is the rst
adversarial sample crafting algorithm proposed for decision
trees. The intuition exploits the underlying tree structure of
the classier model. To nd an adversarial sample, given a
sample and a tree, we simply search for leaves with dierent
classes in the neighborhood of the leaf corresponding to the
decision tree's original prediction for the sample. We then
nd the path from the original leaf to the adversarial leaf
and modify the sample accordingly to the conditions on this
path so as to force the decision tree to misclassify the sample
in the adversarial class specied by the newly identied leaf.
afg3h3i13213de2bc12Figure 6: Decision Tree Adversarial Samples: leaves indi-
cate output classes (here the problem has 3 output classes)
whereas intermediate nodes with letters indicate binary con-
ditions (if condition do else do). To misclassify the sample
from class 3 denoted by the green leaf, the adversary modi-
es it such that conditions gandievaluate accordingly for
the sample to be classied in class 2 denoted by the red leaf.
Algorithm 2 Crafting Decision Tree Adversarial Samples
Input:T,~ x,legitimate\_class
1:~x ~ x
2:legit\_leaf nd leaf in Tcorresponding to ~ x
3:ancestor legitimate\_leaf
4:components []
5:while predict (T;~x) == legitimate\_class do
6: ifancestor == ancestor.parent.left then
7: advers\_leaf nd leaf under ancestor.right
8: else ancestor == ancestor.parent.right
9: advers\_leaf nd leaf under ancestor.left
10: end if
11: components nodes from legit\_leaf toadvers\_leaf
12: ancestor ancestor.parent
13:end while
14:fori2components do
15: perturb ~x[i] to change node's condition output
16:end for
17:return~x
This intuition, depicted in Figure 6, is formalized by Algo-
rithm 2. The algorithm takes a decision tree T, a sample
~ x, the legitimate\_class for sample ~ x, and outputs an ad-
versarial sample ~xmisclassied by decision tree T. The
algorithm does not explicitly minimize the amount of per-
turbation introduced to craft adversarial samples, but as
shown in Section 3.3, we found in practice that perturba-
tions found involve a minuscule proportion of features.
7. DISCUSSION AND RELATED WORK
Upon completion of their training on collections of known
input-label pairs ( ~ x;~ y), classiers fmake label predictions
f(x) on unseen inputs ~ x[18]. Models extrapolate from
knowledge extracted by processing input-label pairs during
training to make label predictions. Several factors, including
(1) imperfections in the training algorithms, (2) the linearity
of many underlying components used to built machine learn-
ing models, and (3) the limited amount of training points not
always representative of the entire plausible input domain,
leave numerous machine learning models exposed to adver-
sarial manipulations of their inputs despite having excellent
performances on legitimate|expected|inputs.Our work builds on a practical method for attacks against
black-box deep learning classiers [20]. Learning substitute
models approximating the decision boundaries of targeted
classiers alleviates the need of previous attacks [22, 12,
19] for knowledge of the target architecture and parame-
ters. We generalized this method and showed that it can
target any machine learning classier. We also reduced
its computational cost by (1) introducing substitute mod-
els trained using logistic regression instead of deep learning
and (2) decreasing the number of queries made with reservoir
sampling. Learning substitutes is an instance of knowledge
transfer, a set of techniques to transfer the generalization
knowledge learned by a model into another model [9, 10].
This paper demonstrates that adversaries can reliably tar-
get classiers whose characteristics are unknown, deployed
remotely, e.g., by machine learning as a service platforms.
The existence of such a threat vector calls for the design
of defensive mechanisms [17]. Unfortunately, we found that
defenses proposed in the literature|such as training with
adversarial samples [12]|were noneective, or we were un-
able to deploy them because of our lack of access to the ma-
chine learning model targeted|for instance distillation [21].
This failure is most likely due to the shallowness of models
like logistic regression, which support the services oered by
Amazon and Google, although we are unable to conrm that
statement in Google's case using available documentation.
This work is part of a series of security evaluations of ma-
chine learning algorithms [1, 5]. Unlike us, previous work in
this eld assumed knowledge of the model architecture and
parameters [6, 14]. Our threat model considered adversaries
interested in misclassication at test time, once the model
has been deployed. Other largely unexplored threat mod-
els exist. For instance poisoning the training data used to
learn models was only considered in the context of binary
SVMs whose training data is known [7] or anomaly detection
systems whose underlying model is known [15].
8. CONCLUSIONS
Our work rst exposed the strong phenomenon of adversar-
ial sample transferability across the machine learning space.
Not only do we nd that adversarial samples are misclas-
sied across models trained using the same machine learn-
ing technique, but also across models trained by dierent
techniques. We then improved the accuracy and reduced
the computational complexity of an existing algorithm for
learning models substitutes of machine learning classiers.
We showed that DNNs and LR could both eectively be used
to learn a substitute model for many classiers trained with
a deep neural network, logistic regression, support vector
machine, decision tree, and nearest neighbors. In a nal ex-
periment, we demonstrated how all of these ndings could
be used to target online classiers trained and hosted by
Amazon and Google, without any knowledge of the model
design or parameters, but instead simply by making label
queries for 800 inputs. The attack successfully forces these
classiers to misclassify 96 :19% and 88 :94% of their inputs.
These ndings call for some validation of inputs used by ma-
chine learning algorithms. This remains an open problem.
Future work should continue to improve the learning of sub-
stitutes to maximize their accuracy and the transferability
of adversarial samples crafted to targeted models. Further-more, poisoning attacks at training time remain largely to
be investigated, leaving room for contributions to the eld.
9. REFERENCES
[1] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D.
Tygar. Can machine learning be secure? In Proceedings of the
2006 ACM Symposium on Information, computer and
communications security , pages 16{25. ACM, 2006.
[2] E. Battenberg, S. Dieleman, and al. Lasagne: Lightweight
library to build and train neural networks in theano, 2015.
[3] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, and al.
Theano: a cpu and gpu math expression compiler. In
Proceedings of the Python for scientic computing conference
(SciPy) , volume 4, page 3. Austin, TX, 2010.
[4] B. Biggio, I. Corona, and al. Evasion attacks against machine
learning at test time. In Machine Learning and Knowledge
Discovery in Databases , pages 387{402. Springer, 2013.
[5] B. Biggio, G. Fumera, and F. Roli. Security evaluation of
pattern classiers under attack. Knowledge and Data
Engineering, IEEE Transactions on , 26(4):984{996, 2014.
[6] B. Biggio, B. Nelson, and P. Laskov. Support vector machines
under adversarial label noise. In ACML , pages 97{112, 2011.
[7] B. Biggio, B. Nelson, and L. Pavel. Poisoning attacks against
support vector machines. In Proceedings of the 29th
International Conference on Machine Learning , 2012.
[8] C. M. Bishop. Pattern recognition. Machine Learning , 2006.
[9] C. Bucila, R. Caruana, and A. Niculescu-Mizil. Model
compression. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and data
mining , pages 535{541. ACM, 2006.
[10] T. Chen, I. Goodfellow, and J. Shlens. Net2net: Accelerating
learning via knowledge transfer. In Proceedings of the 2016
International Conference on Learning Representations .
Computational and Biological Learning Society, 2016.
[11] I. Goodfellow, Y. Bengio, and A. Courville. Deep learning.
Book in preparation for MIT Press, 2016.
[12] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and
harnessing adversarial examples. In Proceedings of the 2015
International Conference on Learning Representations .
Computational and Biological Learning Society, 2015.
[13] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in
a neural network. In Deep Learning and Representation
Learning Workshop at NIPS 2014 . arXiv preprint
arXiv:1503.02531, 2014.
[14] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and
J. Tygar. Adversarial machine learning. In Proceedings of the
4th ACM workshop on Security and articial intelligence ,
pages 43{58. ACM, 2011.
[15] M. Kloft and P. Laskov. Online anomaly detection under
adversarial impact. In International Conference on Articial
Intelligence and Statistics , pages 405{412, 2010.
[16] Y. LeCun and C. Cortes. The mnist database of handwritten
digits, 1998.
[17] P. McDaniel, N. Papernot, and Z. B. Celik. Machine Learning
in Adversarial Settings. IEEE Security & Privacy Magazine ,
14(3), May/June 2016.
[18] K. P. Murphy. Machine learning: a probabilistic perspective .
MIT press, 2012.
[19] N. Papernot, P. McDaniel, and al. The limitations of deep
learning in adversarial settings. In Proceedings of the 1st IEEE
European Symposium on Security and Privacy . IEEE, 2016.
[20] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, and al.
Practical black-box attacks against deep learning systems using
adversarial examples. arXiv preprint arXiv:1602.02697 , 2016.
[21] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami.
Distillation as a defense to adversarial perturbations against
deep neural networks. In Proceedings of the 37th IEEE
Symposium on Security and Privacy . IEEE, 2016.
[22] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, , et al.
Intriguing properties of neural networks. In Proceedings of the
2014 International Conference on Learning Representations .
Computational and Biological Learning Society, 2014.
[23] J. S. Vitter. Random sampling with a reservoir. ACM
Transactions on Mathematical Software (TOMS) ,
11(1):37{57, 1985.
[24] D. Warde-Farley and I. Goodfellow. Adversarial perturbations
of deep neural networks. In T. Hazan, G. Papandreou, and
D. Tarlow, editors, Advanced Structured Prediction . 2016.10. ACKNOWLEDGMENTS
Research was sponsored by the Army Research Laboratory
and was accomplished under Cooperative Agreement Num-
ber W911NF-13-2-0045 (ARL Cyber Security CRA). The
views and conclusions contained in this document are those
of the authors and should not be interpreted as represent-
ing the ocial policies, either expressed or implied, of the
Army Research Laboratory or the U.S. Government. The
U.S. Government is authorized to reproduce and distribute
reprints for Government purposes notwithstanding any copy-
right notation here on.