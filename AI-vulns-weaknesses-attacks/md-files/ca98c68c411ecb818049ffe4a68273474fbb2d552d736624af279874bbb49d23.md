GeoDA: a geometric framework for black-box adversarial attacks
Ali Rahmati, Seyed-Mohsen Moosavi-Dezfooliy, Pascal Frossardz, and Huaiyu Dai
Department of ECE, North Carolina State University
yInstitue for Machine Learning, ETH Zurich
zEcole Polytechnique Federale de Lausanne
arahmat@ncsu.edu, seyed.moosavi@inf.ethz.ch, pascal.frossard@epfl.ch, hdai@ncsu.edu
Abstract
Adversarial examples are known as carefully perturbed
images fooling image classiÔ¨Åers. We propose a geometric
framework to generate adversarial examples in one of the
most challenging black-box settings where the adversary
can only generate a small number of queries, each of them
returning the top- 1label of the classiÔ¨Åer. Our framework
is based on the observation that the decision boundary of
deep networks usually has a small mean curvature in the
vicinity of data samples. We propose an effective iterative
algorithm to generate query-efÔ¨Åcient black-box perturba-
tions with small `pnorms forp1, which is conÔ¨Årmed
via experimental evaluations on state-of-the-art natural im-
age classiÔ¨Åers. Moreover, for p= 2, we theoretically show
that our algorithm actually converges to the minimal `2-
perturbation when the curvature of the decision boundary
is bounded. We also obtain the optimal distribution of the
queries over the iterations of the algorithm. Finally, exper-
imental results conÔ¨Årm that our principled black-box attack
algorithm performs better than state-of-the-art algorithms
as it generates smaller perturbations with a reduced num-
ber of queries.1
1. Introduction
It has become well known that deep neural networks
are vulnerable to small adversarial perturbations, which are
carefully designed to cause miss-classiÔ¨Åcation in state-of-
the-art image classiÔ¨Åers [29]. Many methods have been
proposed to evaluate adversarial robustness of classiÔ¨Åers in
the white-box setting, where the adversary has full access
to the target model [15, 27, 3]. However, the robustness of
classiÔ¨Åers in black-box settings ‚Äì where the adversary has
only access to the output of the classiÔ¨Åer ‚Äì is of high rel-
evance in many real-world applications of deep neural net-
1The code of GeoDA is available at https://github.com/
thisisalirah/GeoDA .
Normal vector
Boundary
HyperplaneFigure 1: Linearization of the decision boundary.
works such as autonomous systems and healthcare, where it
poses serious security threats. Several black-box evaluation
methods have been proposed in the literature. Depending
on what the classiÔ¨Åer gives as an output, black-box evalua-
tion methods are either score-based [28, 6, 20] or decision-
based [4, 2, 22].
In this paper, we propose a novel geometric framework
for decision-based black-box attacks in which the adversary
only has access to the top-1label of the target model. In-
tuitively small adversarial perturbations should be searched
in directions where the classiÔ¨Åer decision boundary comes
close to data samples. We exploit the low mean curvature
of the decision boundary in the vicinity of the data sam-
ples to effectively estimate the normal vector to the decision
boundary. This key prior permits to considerably reduces
the number of queries that are necessary to fool the black-
box classiÔ¨Åer. Experimental results conÔ¨Årm that our Geo-
metric Decision-based Attack (GeoDA) outperforms state-
of-the-art black-box attacks, in terms of required number
of queries to fool the classiÔ¨Åer. Our main contributions are
summarized as follows:
We propose a novel geometric framework based on lin-
earizing the decision boundary of deep networks in the
vicinity of samples. The error for the estimation of
the normal vector to the decision boundary of clas-
siÔ¨Åers with Ô¨Çat decision boundaries, including linear
classiÔ¨Åers, is shown to be bounded in a non-asymptotic
1arXiv:2003.06468v1 [cs.CV] 13 Mar 2020regime. The proposed framework is general enough to
be deployed for any classiÔ¨Åer with low curvature deci-
sion boundary.
We demonstrate how our proposed framework can be
used to generate query-efÔ¨Åcient `pblack-box perturba-
tions. In particular, we provide algorithms to generate
perturbations for p1, and show their effectiveness
via experimental evaluations on state-of-the-art natu-
ral image classiÔ¨Åers. In the case of p= 2, we also
prove that our algorithm converges to the minimal `2-
perturbation. We further derive the optimal number of
queries for each step of the iterative search strategy.
Finally, we show that our framework can incorporate
different prior information, particularly transferability
and subspace constraints on the adversarial perturba-
tions. We show theoretically that having prior infor-
mation can bias the normal vector estimation search
space towards a more accurate estimation.
2. Related work
Adversarial examples can be crafted in white-box set-
ting [15, 27, 3], score-based black-box setting [28, 6, 20] or
decision-based black-box scenario [4, 2, 22]. The latter set-
tings are obviously the most challenging as little is known
about the target classiÔ¨Åcation settings. Yet, there are several
recent works on the black-box attacks on image classiÔ¨Åers
[20, 21, 32]. However, they assume that the loss function,
the prediction probabilities, or several top sorted labels are
available, which may be unrealistic in many real-world sce-
narios. In the most challenging settings, there are a few at-
tacks that exploit only the top- 1label information returned
by the classiÔ¨Åer, including the Boundary Attack (BA) [2],
the HopSkipJump Attack (HSJA) [5], the OPT attack [8],
and qFool [22]. In [2], by starting from a large adversar-
ial perturbation, BA can iteratively reduce the norm of the
perturbation. In [5], the authors provided an attack based
on [2] that improves the BA taking the advantage of an esti-
mated gradient. This attack is quite query efÔ¨Åcient and can
be assumed as the state-of-the-art baseline in the black-box
setting. In [8], an optimization-based hard-label black-box
attack algorithm is introduced with guaranteed convergence
rate in the hard-label black-box setting which outperforms
the BA in terms of number of queries. Closer to our work,
in [22], a heuristic algorithm based on the estimation of the
normal vector to decision boundary is proposed for the case
of`2-norm perturbations.
Most of the aforementioned attacks are however speciÔ¨Å-
cally designed for minimizing perturbation metrics such `2
and`1norms, and mostly use heuristics. In contrast, we
introduce a powerful and generic framework grounded on
the geometric properties of the decision boundary of deepnetworks, and propose a principled approach to design efÔ¨Å-
cient algorithms to generate general `p-norm perturbations,
in which [22] can be seen as a special case. We also provide
convergence guarantees for the `2-norm perturbations. We
obtained the optimal distribution of queries over iterations
theoretically as well which permits to use the queries in a
more efÔ¨Åcient manner. Moreover, the parameters of our al-
gorithm are further determined via empirical and theoretical
analysis, not merely based on heuristics as done in [22].
3. Problem statement
Let us assume that we have a pre-trained L-class classi-
Ô¨Åer with parameters represented as f:Rd!RL, where
x2Rdis the input image and ^k(x) = argmaxkfk(x)is
the top- 1classiÔ¨Åcation label where fk(x)is thekthcompo-
nent off(x)corresponds to the kthclass. We consider the
non-targeted black-box attack, where an adversary without
any knowledge on computes an adversarial perturbation
vto change the estimated label of an image xto any in-
correct label, i.e., ^k(x+v)6=^k(x). The distance metric
D(x;x+v)can be any function including the `pnorms.
We assume a general form optimization problem in which
the goal is to fool the classiÔ¨Åer while D(x;x+v)is mini-
mized as:min
vD(x;x+v)
s.t. ^k(x+v)6=^k(x):(1)
Finding a solution for (1) is a hard problem in general. To
obtain an efÔ¨Åcient approximate solution, one can try to es-
timate the point of the classiÔ¨Åer decision boundary that is
the closest to the data point x. Crafting an small adver-
sarial perturbation then consists in pushing the data point
beyond the decision boundary in the direction of its nor-
mal. The normal to the decision boundary is thus critical
in a geometry-based attack. While it can be obtained us-
ing back-propagation in white box settings (e.g., [27]), its
estimation in black-box settings becomes challenging.
The key idea here is to exploit the geometric proper-
ties of the decision boundary in deep networks for effec-
tive estimation in black-box settings. In particular, it has
been shown that the decision boundaries of the state-of-the-
art deep networks have a quite low mean curvature in the
neighborhood of data samples [12]. SpeciÔ¨Åcally, the deci-
sion boundary at the vicinity of a data point xcan be locally
approximated by a hyperplane passing through a boundary
pointxBclose tox, with a normal vector w[14, 13]. Thus,
by exploiting this property, the optimization problem in (1)
can be locally linearized as:
min
vD(x;x+v)
s.t.wT(x+v) wTxB= 0(2)
Typically,xBis a point on the boundary, which can be
found by binary search with a small number of queries.
2However, solving the problem (2) is quite challenging in
black-box settings as one does not have any knowledge
about the parameters and can only access the top-1 la-
bel^k(x)of the image classiÔ¨Åer. A query is a request that
results in the top-1 label of an image classiÔ¨Åer for a given
input, which prevents the use of zero-order black box op-
timization methods [34, 33] that need more information to
compute adversarial perturbations. The goal of our method
is to estimate the normal vector to the decision boundary
wresorting to geometric priors with a minimal number of
queries to the classiÔ¨Åer.
4. The estimator
We introduce an estimation method for the normal vec-
tor of classiÔ¨Åers with Ô¨Çat decision boundaries. It is worth
noting that the proposed estimation is not limited to deep
networks and applies to any classiÔ¨Åer with low mean cur-
vature boundary. We denote the estimate of the vector w
normal to the Ô¨Çat decision boundary in (2) with ^wNwhen
Nqueries are used. Without loss of generality, we assume
that the boundary point xBis located at the origin. Thus,
according to (2), the decision boundary hyperplane passes
through the origin and we have wTx= 0for any vector x
on the decision boundary hyperplane. In order to estimate
the normal vector to the decision boundary, the key idea is
to generateNsamplesi; i2f1;:::;Ngfrom a multi-
variate normal distribution iN(0;). Then, we query
the image classiÔ¨Åer Ntimes to obtain the top-1 label output
for eachxB+i;8i2N. For a given data point x, if
wTx0, the label is correct; if wTx0, the classiÔ¨Åer is
fooled. Hence, if the generated perturbations are adversar-
ial, they belong to the set
Sadv=fij^k(xB+i)6=^k(x)g
=fijwTi0g: (3)
Similarly, the perturbations on the other side of the hyper-
plane, which lead to correct classiÔ¨Åcation, belong to the set
Sclean=fij^k(xB+i) =^k(x)g
=fijwTi0g: (4)
The samples in each of the sets SadvandScleancan be as-
sumed as samples drawn from a hyperplane ( wTx= 0)
truncated multivariate normal distribution with mean 0and
covariance matrix . We deÔ¨Åne the PDF of the ddimen-
sional zero mean multivariate normal distribution with co-
variance matrix asd(j). We deÔ¨Åne d(bj) =R1
bd(j)das cumulative distribution function of the
univariate normal distribution.
Lemma 1. Given a multivariate Gaussian distribution
N(0;)truncated by the hyperplane wTx0, the meanand covariance matrix Rof the hyperplane truncated dis-
tribution are given by:
=c1w (5)
wherec1= (d(0)) 1d(0)and the covariance matrix
R= wwT(d(0)22) 1d(0))d2(0)in which
= (wTw)1
2[30].
As it can be seen in (5), the mean is a function of both
the covariance matrix andw. Our ultimate goal is to es-
timate the normal vector to the decision boundary. In order
to recoverwfrom, a sufÔ¨Åcient condition is to choose 
to be a full rank matrix.
General case We Ô¨Årst consider the case where no prior
information on the search space is available. The matrix
=Ican be a simple choice to avoid unnecessary com-
putations. The direction of the mean of the truncated dis-
tribution is an estimation for the direction of hyperplane
normal vector as =c1w. The covariance matrix of
the truncated distribution is R=I+c2wwTwhere
c2= 2(d(0)) 22
d(0). As the samples in both of the
setsSadvandScleanare hyperplane truncated Gaussian distri-
butions, the same estimation can be applied for the samples
in the setScleanas well. Thus, by multiplying the samples
inScleanby 1and we can use them to approximate the de-
sired gradient to have a more efÔ¨Åcient estimation. Hence,
the problem is reduced to the estimation of the mean of the
Nsamples drawn from the hyperplane truncated distribu-
tion with mean and covariance matrix R. As a result, the
estimator NofwithNsamples is N=1
NPN
i=1ii,
where
i=1^k(xB+i)6=^k(x)
 1^k(xB+i) =^k(x):(6)
The normalized direction of the normal vector of the bound-
ary can be obtained as:
^wN=N
kNk2(7)
Perturbation priors We now consider the case where pri-
ors on the perturbations are available. In black-box settings,
having prior information can signiÔ¨Åcantly improve the per-
formance of the attack. Although the attacker does not have
access to the weights of the classiÔ¨Åer, it may have some
prior information about the data, classiÔ¨Åer, etc. [21]. Here,
using , we can capture the prior knowledge for the esti-
mation of the normal vector to the decision boundary. In
the following, we unify the two common priors in our pro-
posed estimator. In the Ô¨Årst case, we have some prior in-
formation about the subspace in which we search for nor-
mal vectors, we can incorporate such information into to
have a more efÔ¨Åcient estimation. For instance, deploying
3low frequency sub-space Rmin whichmd, we can
generate a rank mcovariance matrix . Let us assume
thatS=fs1;s2;:::;smgis an orthonormal Discrete Co-
sine Transform (DCT) basis in the m-dimensional subspace
of the input space [16]. In order to generate the samples
from this low dimensional subspace, we use the following
covariance matrix:
=1
mmX
i=1sisT
i: (8)
The normal vector of the boundary can be obtained by
plugging the modiÔ¨Åed in (5). Second, we consider
transferability priors. It has been observed that adversar-
ial perturbations well transfer across different trained mod-
els [31, 26, 9]. Now, if the adversary further has full access
to another modelT0, yet different than the target black-box
modelT, it can take advantage of the transferability proper-
ties of adversarial perturbations. For a given datapoint, one
can obtain the normal vector to the decision boundary in the
vicinity of the datapoint for T0, and bias the normal vector
search space for the black-box classiÔ¨Åer. Let us denote the
transferred direction with unit-norm vector g. By incorpo-
rating this vector into , we can bias the search space as:
=I+ (1 )ggT(9)
where2[0;1]adjusts the trade-off between exploitation
and exploration. Depending on how conÔ¨Ådent we are about
the utility of the transferred direction, we can adjust its con-
tribution by tuning the value of . Substituting (9) into (5),
after normalization to c1, one can get
=w+ (1 )ggTw; (10)
where the Ô¨Årst term is the estimated normal vector to the
boundary and the second term is the projection of the esti-
mated normal vector on the transferred direction g. Having
incorporated the prior information into , one can gener-
ate perturbations iN(0;)with the modiÔ¨Åed in an
effective search space, which leads to a more accurate esti-
mation of normal to the decision boundary.
Estimator bound Finally, we are interested in quantify-
ing the number of samples that are necessary for estimat-
ing the normal vectors in our geometry inspired framework.
Given a real i.i.d. sequence, using the central limit theorem,
if the samples have a Ô¨Ånite variance, an asymptotic bound
can be provided for the estimate. However, this bound is not
of our interest as it is only asymptotically correct. We are
interested in bounds of similar form with non-asymptotic
inequalities as the number of queries is limited [23, 17].
Lemma 2. The mean estimation Ndeployed in (9)ob-
tained fromNmultivariate hyperplane truncated GaussianAlgorithm 1: `pGeoDA (with optimal query distribu-
tion) forp>1
1Inputs: Original image x, query budget N,, number
of iterations T.
2Output: Adversarial example xT.
3Obtain the optimal query distribution N
t;8tby (19).
4Find a starting point on the boundary x0.
5fort= 1 :Tdo
6 Estimate normal ^wN
tatxt 1byN
tqueries.
7 Obtainvtaccording to (13).
8 ^rt minfr0>0 :^k(x+r0vt)6=^k(x)g
9xt x+ ^rt^wN
t
queries satisÔ¨Åes the probability
P 
kN kr
Tr(R)
N+r
2maxlog(1=)
N!
1 
(11)
where Tr (R)andmaxdenote the trace and largest eigen-
value of the covariance matrix R, respectively.
Proof. The proof can be found in Appendix A.
This bound will be deployed in sub-section 5.1 to com-
pute the optimal distribution of queries over iterations.
5. Geometric decision-based attacks (GeoDA)
Based on the estimator provided in Section 4, one can
design efÔ¨Åcient black-box evaluation methods. In this pa-
per, we focus on the minimal `p-norm perturbations, i.e.,
D(x;x+v) =kvkp. We Ô¨Årst describe the general algo-
rithm for`pperturbations, and then provide algorithms to
Ô¨Ånd black-box perturbations for p= 1;2;1. Furthermore,
forp= 2, we prove the convergence of our method. The
linearized optimization problem in (2) can be re-written as
min
vkvkp
s.t.wT(x+v) wTxB= 0: (12)
In the black-box setting, one needs to estimate xBandw
in order to solve this optimization problem. The boundary
pointxBcan be found using a similar approach as [22].
HavingxB, one then use the process described in Section 4
to compute the estimator of w‚Äì i.e., ^wN1‚Äì by making N1
queries to the classiÔ¨Åer. In the case of p= 2, the estimated
direction ^wNis indeed the direction of the minimal pertur-
bation. This process is depicted in Fig. 1.
If the curvature of the decision boundary is exactly zero,
the solution of this problem gives the direction of the min-
imal`pperturbation. However, for deep neural networks,
even ifN!1 , the obtained direction is not completely
4aligned with the minimal perturbation as these networks
still have a small yet non-zero curvature (see Fig. 4c). Nev-
ertheless, to overcome this issue, the solution vof (12)
can be used to obtain a boundary point x1=x+ ^r1vto
the original image xthanx0, for an appropriate value of
^r1>0. For notation consistency, we deÔ¨Åne x0=xB.
Now, we can again solve (12) for the new boundary point
x1. Repeating this process results in an iterative algorithm
to Ô¨Ånd the minimal `pperturbation, where each iteration
corresponds to solving (12) once. Formally, for a given im-
agex, letxtbe the boundary point estimated in the iteration
t 1. Also, letNtbe the number of queries used to estimate
the normal to the decision boundary ^wNtat the iteration t.
Hence, the (normalized) solution to (12) in the t-th iteration,
vt, can be written in closed-form as:
vt=1
k^wNtkp
p 1sign(^wNt); (13)
forp2[1;1), whereis the point-wise product. For the
particular case of p=1, the solution of (13) is simply
reduced to:
vt=sign(^wNt): (14)
The cases of the p= 1;2are presented later. In all cases,
xtis then updated according to the following update rule:
xt=x+ ^rtvt (15)
where ^rtcan be found using an efÔ¨Åcient line search along
vt. The general algorithm is summarized in Alg. 1.
5.1.`2perturbation
In the`2case, the update rule of (15) is reduced to
xt=x+ ^rt^wNtwhere ^rtis the`2distance ofxto the
decision boundary at iteration t. We propose convergence
guarantees and optimal distribution of queries over the suc-
cessive iterations for this case.
Convergence guarantees We prove that GeoDA con-
verges to the minimal `2perturbation given that the cur-
vature of the decision boundary is bounded. We deÔ¨Åne the
curvature of the decision boundary as =1
R, whereRis
the radius of the largest open ball included in the region that
intersects with the boundary B[12]. In case N!1 , then
^rt!rtwherertis assumed as exact distance required to
push the image xtowards the boundary at iteration twith
directionvt. The following Theorem holds:
Theorem 1. Given a classiÔ¨Åer with decision boundary of
bounded curvature with r < 1, the sequencef^rtggener-
ated by Algorithm 1 converges linearly to the minimum `2
distancersince we have:
lim
t!1^rt+1 r
^rt r= (16)
where<1is the convergence rate.
Proof. The proof can be found in Appendix B.Optimal query distribution In practice, however, the
number of queries Nis limited. One natural question is how
should one choose the number of queries in each iteration
of GeoDA. It can be seen in the experiments that allocating
a smaller number of queries for the Ô¨Årst iterations and then
increasing it in each iteration can improve the convergence
rate of the GeoDA. At early iterations, noisy normal vector
estimates are Ô¨Åne because the noise is smaller relative to the
potential improvement, whereas in later iterations noise has
a bigger impact. This makes the earlier iterations cheaper in
terms of queries, potentially speeding up convergence [11].
We assume a practical setting in which we have a limited
budgetNfor the number of queries as the target system may
block if the number of queries increases beyond a certain
threshold [7]. The goal is to obtain the optimal distribution
of the queries over the iterations.
Theorem 2. Given a limited query budget N, the bounds
for the GeoDA `2perturbation error for total number of
iterationsTcan be obtained as:
T(r0 r) e(N)^rt rT(r0 r) +e(N)(17)
wheree(N) =PT
i=1T iripNiis the error due to limited
number of queries, =p
Tr(R) +p
2maxlog(1=)and
Ntis the number of queries to estimate the normal vector
to the boundary at point xt 1, andr0=kx x0k.
Proof. The proof can be found in Appendix C.
As in (17), the error in the convergence is due to two
factors: ( i) curvature of the decision boundary ( ii) limited
number of queries. If the number of iterations increases,
the effect of the curvature can vanish. However, the term
ripNtis not small enough as the number of queries is Ô¨Ånite.
Having unlimited number of the queries, the error term due
to queries can vanish as well. However, given a limited
number of queries, what should be the distribution of the
queries to alleviate such an error? We deÔ¨Åne the following
optimization problem:
min
N1;:::;N TTX
i=1 iripNi
s.t.TX
i=1NiN (18)
where the objective is to minimize the error e(N)while the
query budget constraint is met over all iterations.
Theorem 3. The optimal numbers of queries for (18) in
each iteration form geometric sequence with the common
ratioN
t+1
N
t 2
3, where 01. Moreover, we have
N
t 2
3t
PT
i=1 2
3iN: (19)
Proof. The proof can be found in Appendix D.
5103104
Number of queries01020304050‚Ñì2 distanceBA
HSJA
GeoDA(a)
2500 5000 7500 10000 12500 15000 17500 20000
Number of queries100101102103Number of iterationsBA
HSJA
GeoDA (b)
0% 1% 2% 3% 4% 5% 6%
Median percentage of number of perturbed coordinates0.20.40.60.81.0Fooling rate
Query = 10000
Query = 2000
Query = 500 (c)
Figure 2: Performance evaluation of GeoDA for `pwhenp= 1;2(a) Comparison for the performance of GeoDA, BA, and
HSJA for`2norm. (b) Comparison for the number of required iterations in GeoDA, BA, and HSJA. (c) Fooling rate vs.
sparsity for different numbers of queries in sparse GeoDA.
5.2.`1perturbation (sparse case)
The framework proposed by GeoDA is general enough
to Ô¨Ånd sparse adversarial perturbations in the black-box set-
ting as well. The sparse adversarial perturbations can be
computed using the following optimization problem with
box constraints as:
min
vkvk1
s.t.wT(x+v) wTxB= 0
lx+vu (20)
In the box constraint lx+vu,landudenote
the lower and upper bounds of the values of x+v. We
can estimate the normal vector ^wNand the boundary point
xBsimilarly to the `2case withNqueries. Now, the
decision boundary Bis approximated with the hyperplane
fx:^wT
N(x xB) = 0g. The goal is to Ô¨Ånd the top- kcoor-
dinates of the normal vector ^wNwith minimum kand push-
ing them to extreme values of the valid range depending on
the sign of the coordinate until it hits the approximated hy-
perplane. In order to Ô¨Ånd the minimum k, we deploy binary
search for a d-dimensional image. Here, we just consider
one iteration for the sparse attack., while the initial point
of the sparse case is obtained using the GeoDA for `2case.
The detailed Algorithm for the sparse version of GeoDA is
given in Algorithm 2.
6. Experiments
6.1. Settings
We evaluate our algorithms on a pre-trained ResNet-
50 [18] with a set Xof 350 correctly classiÔ¨Åed and ran-
domly selected images from the ILSVRC2012‚Äôs validation
set [10]. All the images are resized to 2242243.
To evaluate the performance of the attack we deploy the
median of the `pnorm forp= 2;1distance over all tested
samples, deÔ¨Åned by median
x2X
kx xadvkp
. For sparse per-Algorithm 2: Sparse GeoDA
1Inputs: Original image x, query budget N,,
projection operator Q.
2Output: Sparsely perturbed xadv, sparsitys.
3ObtainxB,^wNwithNqueries by`2GeoDA
algorithm.
4ml= 0,mu=d,J=round (log2(d)) + 1
5forj= 1 :Jdo
6k= round(mu+ml
2)
7 Obtain topkabsolute values of coordinates of ^wN
as^wsp.
8xj Q(x+sign(^wsp))
9 if^wT
N(xj xB)>0then
10mu=k
11 else
12ml=k
13xadv xj,s mu
turbations, we measure the performance by fooling rate de-
Ô¨Åned asjx2X :^k(x)6=^k(xadv)j=jXj. In evaluation of
the sparse GeoDA, we deÔ¨Åne sparsity as the percentage of
the perturbed coordinates of the given image
6.2. Performance analysis
Black-box attacks for `pnorms. We compare the per-
formance of the GeoDA with state of the art attacks for `p
norms. There are several attacks in the literature includ-
ing Boundary attack [2], HopSkipJump attack [5], qFool
[22], and OPT attack [8]. In our experiments, we compare
GeoDA with Boundary attack, qFool and HopSkipJump at-
tack. We do not compare our algorithm with OPT attack
as HopSkipJump already outperforms it considerably [5].
In our algorithm, the optimal distribution of the queries is
obtained for any given number of queries for `2case. The
results for`2and`1for different numbers of queries is
6Queries `2`1 Iterations Gradients
1000 47.92 0.297 40 -
Boundary attack [2] 5000 24.67 0.185 200 -
20000 5.13 0.052 800 -
1000 16.05 - 3 -
qFool [5] 5000 7.52 - 3 -
20000 1.12 - 3 -
1000 14.56 0.062 6 -
HopSkipJump attack [5] 5000 4.01 0.031 17 -
20000 1.85 0.012 42 -
1000 11.76 0.053 6 -
GeoDA-fullspace 5000 3.35 0.022 10 -
20000 1.06 0.009 14 -
1000 8.16 0.022 6 -
GeoDA-subspace 5000 2.51 0.008 10 -
20000 1.01 0.003 14 -
DeepFool (white-box) [27] - 0.026 - 2 20
C&W (white-box) [3] - 0.034 - 10000 10000
Table 1: The performance comparison of GeoDA with BA and HSJA for median `2and`1on ImageNet dataset.
Queries Fooling rate Perturbation
500 88.44% 4.29%
GeoDA 2000 90.25% 3.04%
10000 91.17% 2.36%
SparseFool [2] - 100% 0.23%
Table 2: The performance comparison of black-box sparse
GeoDA for median sparsity compared to white box attack
SparseFool [2] on ImageNet dataset.
depicted in Table 1. GeoDA can outperform the-state-of-
the-art both in terms of smaller perturbations and number
of iterations, which has the beneÔ¨Åt of parallelization. In
particular, the images can be fed into multiple GPUs with
larger batch size. In Fig. 2a, the `2norm of GeoDA, Bound-
ary attack and HopSkipJump are compared. As shown,
GeoDA can outperform the HopSkipJump attack especially
when the number of queries is small. By increasing the
number of queries, the performance of GeoDA and Hop-
SkipJump are getting closer. In Fig. 2b, the number of iter-
ations versus the number of queries for different algorithms
are compared. As depicted, GeoDA needs fewer iterations
compared to HopSkipJump and BA when the number of
queries increases. Thus, on the one hand GeoDA generates
smaller`2perturbations compared to the HopSkipJump at-
tack when the number of queries is small, on the other hand,it saves signiÔ¨Åcant computation time due to parallelization
of queries fed into the GPU.
Now, we evaluate the performance of GeoDA for gen-
erating sparse perturbations. In Fig. 2c, the fooling rate
versus sparsity is depicted. In experiments, we observed
that instead of using the boundary point xBin the sparse
GeoDA, the performance of the algorithm can be improved
by further moving towards the other side of the hyperplane
boundary. Thus, we use xB+(xB x), where0.
The parameter can adjust the trade-off between the fool-
ing rate and the sparsity. It is observed that the higher the
value for, the higher the fooling rate and the sparsity and
vice versa. In other words, choosing small values for pro-
duces sparser adversarial examples; however, it decreases
the chance that it is an adversarial example for the actual
boundary. In Fig. 2c, we depicted the trade-off between
fooling rate and sparsity by increasing the value for for
different query budgets. The larger the number of queries,
the closer the initial point to the original image, and also the
better our algorithm performs in generating sparse adver-
sarial examples. In Table 2, the sparse GeoDA is compared
with the white-box attack SparseFool. We show that with
a limited number of queries, GeoDA can generate sparse
perturbations with acceptable fooling rate with sparsity of
about 3percent with respect to the white-box attack Sparse-
Fool. The adversarial perturbations generated by GeoDA
for`pnorms are shown in Fig. 3 and the effect of different
norms can be observed.
7Figure 3: Original images and adversarial perturbations generated by GeoDA for `2fullspace,`2subspace,`1fullspace,`1
subspace, and `1sparse withN= 10000 queries. (Perturbations are magniÔ¨Åed 10for better visibility.)
10‚àí710‚àí610‚àí510‚àí410‚àí310‚àí210‚àí1100
œÉ0.00.10.20.30.40.5C/N
(a)
0.0 0.2 0.4 0.6 0.8 1.0
Œª12.515.017.520.022.525.027.5l2 distanceQuery = 1000
Query = 2000 (b)
103104
Number of queries0510152025303540l2 distanceSingle iteration
Uniform distribution
Optimal distribution (c)
Figure 4: (a) The effect of the variance on the ratio of correctly classiÔ¨Åed queries Cto the total number of queries Nat
boundary point xB. (b) Effect of on the performance of the algorithm. (c) Comparison of two extreme cases of query
distributions, i.e., single iteration ( !0) and uniform distribution ( = 1) with optimal distribution ( = 0:6).
Incorporating prior information. Here, we evaluate the
methods proposed in Section 4 to incorporate prior infor-
mation in order to improve the estimation of the normal
vector to the decision boundary. As sub-space priors, we
deploy the DCT basis functions in which mlow frequency
subspace directions are chosen [25]. As shown in Fig. 5, bi-
asing the search space to the DCT sub-space can reduce the
`2norm of the perturbations by approximately 27% com-
pared to the full-space case. For transferrability, we obtain
the normal vector of the given image using the white box
attack DeepFool [27] on a ResNet-34 classiÔ¨Åer. We bias the
search space for normal vector estimation as described in
Section 4. As it can be seen in Fig. 5, prior information can
improve the normal vector estimation signiÔ¨Åcantly.
6.3. Effect of hyper-parameters on the performance
Instead of throwing out the gradient obtained from the
previous iterations, we can take advantage of them in next
iterations as well. To do this, we can bias the covariance
matrix towards the gradient obtained from the previous
iteration. The other way is to simply have a weighted av-
erage of the estimated gradient and previous gradients. Asa general rule, given in (10) should be chosen in such a
way that the estimated gradient in recent iterations get more
weights compared to the Ô¨Årst iterations.
103104
Number of queries05101520253035‚Ñì2 distanceGeoDA-fullspace
GeoDA-subspace
GeoDA with transfer direction
Figure 5: Effect of prior information, i.e., DCT sub-space
and transferability on the performance of `2perturbation.
In practice, we need to choose such that the locally Ô¨Çat
assumption of the boundary is preserved. Upon generating
the queries at boundary point xBto estimate the direction of
the normal vector as in (7), the value for is chosen in such
a way that the number of correctly classiÔ¨Åed images and
adversarial images on the boundary are almost the same. In
8Fig. 4a, the effect of variance of added Gaussian pertur-
bation on the number of correctly classiÔ¨Åed queries on the
boundary point is illustrated. We obtained a random point
xBon the decision boundary of the image classiÔ¨Åer and
query the image classiÔ¨Åer 1000 times. As it can be seen, the
varianceis too small, none of the queries is correctly clas-
siÔ¨Åed as the point xBis not exactly on the boundary. It is
worth mentioning that in binary search we choose the point
on the adversarial side as a boundary point. On the other
hand if the variance is too high, all the images are classiÔ¨Åed
as adversarial since they are highly perturbed.
In order to obtain the optimal query distribution for a
given limited budget N, the values for andTshould be
given. Having Ô¨Åxed , ifTis large, the number of queries
allocated to the Ô¨Årst iteration may be too small. To address
this, we consider a Ô¨Åxed number of queries for the Ô¨Årst it-
eration asN
1= 70 . Thus, having Ô¨Åxed , a reasonable
choice forTcan be obtained by solving (19) for T. Based
on (19), if!0, all the queries are allocated to the last
iteration and when = 1, the query distribution is uniform.
A value between these two extremes is desirable for our al-
gorithm. To obtain this value, we run our algorithm for dif-
ferentfor only 10 images different from X. As it can be
seen in Fig. 4b, the algorithm has its worst performance
whenis close to the two extreme cases: single iteration
(!0) and uniform distribution ( = 1). We thus choose
the value= 0:6for our experiments. Finally, in Fig. 4c,
the comparison between three different query distributions
is shown. The optimal query distribution achieves the best
performance while the single iteration preforms worst. Ac-
tually, the fact that the single iteration performs worst is
reÔ¨Çected in our proposed bound in (17) as even with inÔ¨Ånite
number of queries it can not do better than (r0 r). Indeed
the effect of curvature can be addressed only by increasing
the number of iterations.
7. Conclusion
In this work, we propose a new geometric framework for
designing query-efÔ¨Åcient decision-based black-box attacks,
in which the attacker only has access to the top-1 label of
the classiÔ¨Åer. Our method relies on the key observation that
the curvature of the decision boundary of deep networks is
small in the vicinity of data samples. This permits to es-
timate the normals to the decision boundary with a small
number of queries to the classiÔ¨Åer, hence to eventually de-
sign query-efÔ¨Åcient `p-norm attacks. In the particular case
of`2-norm attacks, we show theoretically that our algorithm
converges to the minimal adversarial perturbations, and that
the number of queries at each step of the iterative search
can be optimized mathematically. We Ô¨Ånally study GeoDA
through extensive experiments that conÔ¨Årm its superior per-
formance compared to state-of-the-art black-box attacks.Acknowledgements
This work was supported in part by the US National
Science Foundation under grants ECCS-1444009 and CNS-
1824518. S. M. is supported by a Google Postdoctoral Fel-
lowship.
References
[1] Stephen Boyd and Lieven Vandenberghe. Convex optimiza-
tion. Cambridge university press, 2004. 13
[2] Wieland Brendel, Jonas Rauber, and Matthias Bethge.
Decision-based adversarial attacks: Reliable attacks against
black-box machine learning models. arXiv preprint
arXiv:1712.04248 , 2017. 1, 2, 6, 7
[3] Nicholas Carlini and David Wagner. Towards evaluating the
robustness of neural networks. In 2017 IEEE Symposium on
Security and Privacy (SP) , pages 39‚Äì57, 2017. 1, 2, 7
[4] Jianbo Chen and Michael I Jordan. Boundary attack++:
Query-efÔ¨Åcient decision-based adversarial attack. arXiv
preprint arXiv:1904.02144 , 2019. 1, 2
[5] Jianbo Chen, Michael I Jordan, and Martin J Wainwright.
Hopskipjumpattack: A query-efÔ¨Åcient decision-based attack.
arXiv preprint arXiv:1904.02144 , 2019. 2, 6, 7
[6] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and
Cho-Jui Hsieh. Zoo: Zeroth order optimization based black-
box attacks to deep neural networks without training sub-
stitute models. In Proceedings of the 10th ACM Workshop
on ArtiÔ¨Åcial Intelligence and Security , pages 15‚Äì26. ACM,
2017. 1, 2
[7] Steven Chen, Nicholas Carlini, and David Wagner. Stateful
detection of black-box adversarial attacks. arXiv preprint
arXiv:1907.05587 , 2019. 5
[8] Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan
Zhang, and Cho-Jui Hsieh. Query-efÔ¨Åcient hard-label black-
box attack: An optimization-based approach. arXiv preprint
arXiv:1807.04457 , 2018. 2, 6
[9] Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, and
Jun Zhu. Improving black-box adversarial attacks with a
transfer-based prior. arXiv preprint arXiv:1906.06919 , 2019.
4
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248‚Äì255, 2009. 6
[11] Aditya Devarakonda, Maxim Naumov, and Michael Garland.
Adabatch: adaptive batch sizes for training deep neural net-
works. arXiv preprint arXiv:1712.02029 , 2017. 5
[12] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and
Pascal Frossard. Robustness of classiÔ¨Åers: from adversarial
to random noise. In Advances in Neural Information Pro-
cessing Systems , pages 1632‚Äì1640, 2016. 2, 5, 11, 13
[13] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and
Pascal Frossard. The robustness of deep networks: A ge-
ometrical perspective. IEEE Signal Processing Magazine ,
34(6):50‚Äì62, 2017. 2
9[14] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal
Frossard, and Stefano Soatto. Empirical study of the topol-
ogy and geometry of deep networks. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3762‚Äì3770, 2018. 2
[15] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. arXiv
preprint arXiv:1412.6572 , 2014. 1, 2
[16] Chuan Guo, Jacob R Gardner, Yurong You, Andrew Gordon
Wilson, and Kilian Q Weinberger. Simple black-box adver-
sarial attacks. arXiv preprint arXiv:1905.07121 , 2019. 4
[17] David Lee Hanson and Farroll Tim Wright. A bound on
tail probabilities for quadratic forms in independent ran-
dom variables. The Annals of Mathematical Statistics ,
42(3):1079‚Äì1083, 1971. 4, 11
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770‚Äì778, 2016. 6
[19] Nicholas J Higham. Analysis of the Cholesky decomposition
of a semi-deÔ¨Ånite matrix . Oxford University Press, 1990. 11
[20] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy
Lin. Black-box adversarial attacks with limited queries and
information. arXiv preprint arXiv:1804.08598 , 2018. 1, 2
[21] Andrew Ilyas, Logan Engstrom, and Aleksander Madry.
Prior convictions: Black-box adversarial attacks with ban-
dits and priors. arXiv preprint arXiv:1807.07978 , 2018. 2,
3
[22] Yujia Liu, Seyed-Mohsen Moosavi-Dezfooli, and Pascal
Frossard. A geometry-inspired decision-based attack. arXiv
preprint arXiv:1903.10826 , 2019. 1, 2, 4, 6
[23] G ¬¥abor Lugosi, Shahar Mendelson, et al. Sub-gaussian esti-
mators of the mean of a random vector. The Annals of Statis-
tics, 47(2):783‚Äì794, 2019. 4, 11
[24] Rachid Marsli. Bounds for the smallest and largest eigenval-
ues of hermitian matrices. International Journal of Algebra ,
9(8):379‚Äì394, 2015. 11
[25] Seyed Mohsen Moosavi Dezfooli. Geometry of adversar-
ial robustness of deep networks: methods and applications.
Technical report, EPFL, 2019. 8
[26] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar
Fawzi, and Pascal Frossard. Universal adversarial perturba-
tions. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 1765‚Äì1773, 2017. 4
[27] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and
Pascal Frossard. Deepfool: a simple and accurate method to
fool deep neural networks. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
2574‚Äì2582, 2016. 1, 2, 7, 8
[28] Nina Narodytska and Shiva Prasad Kasiviswanathan. Simple
black-box adversarial perturbations for deep networks. arXiv
preprint arXiv:1612.06299 , 2016. 1, 2
[29] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus.
Intriguing properties of neural networks. arXiv preprint
arXiv:1312.6199 , 2013. 1[30] GM Tallis. Plane truncation in normal populations. Journal
of the Royal Statistical Society: Series B (Methodological) ,
27(2):301‚Äì307, 1965. 3, 11
[31] Florian Tram `er, Nicolas Papernot, Ian Goodfellow, Dan
Boneh, and Patrick McDaniel. The space of transferable ad-
versarial examples. arXiv preprint arXiv:1704.03453 , 2017.
4
[32] Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan
Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming Cheng.
Autozoom: Autoencoder-based zeroth order optimization
method for attacking black-box neural networks. arXiv
preprint arXiv:1805.11770 , 2018. 2
[33] Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan
Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming Cheng.
Autozoom: Autoencoder-based zeroth order optimization
method for attacking black-box neural networks. In Pro-
ceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence ,
volume 33, pages 742‚Äì749, 2019. 3
[34] Pu Zhao, Sijia Liu, Pin-Yu Chen, Nghia Hoang, Kaidi Xu,
Bhavya Kailkhura, and Xue Lin. On the design of black-box
adversarial examples by leveraging gradient-free optimiza-
tion and operator splitting method. In Proceedings of the
IEEE International Conference on Computer Vision , pages
121‚Äì130, 2019. 3
108. Appendix
A. Proof of Lemma 2
Proof. LetXibe a random vector taking values in Rdwith
mean=E[X]and covariance matrix R=E(X 
)(X )T. Given the X1;:::;Xn, the goal is to esti-
mate. IfXhas a multivariate Gaussian or sub-Gaussian
distribution, the sample mean N=1
NPN
i=1Xiis the re-
sult of MLE estimation, which satisÔ¨Åes, with probability at
least1 
jjN jjr
Tr(R)
N+r
2maxlog(1=)
N(21)
where Tr (R)andmaxdenote the trace and largest eigen-
value of the covariance matrix R, respectively [17]. We
already know the truncated normal distribution mean and
variance. Although, the truncated distribution is similar to
Gaussian, we need to prove that it satisÔ¨Åes the sub-Gaussian
distribution property so that we can use the bound in (21).
The truncated distribution with mean and covariance
matrixRis a sub-Gaussian distribution. A given distribu-
tion is sub-Gaussian if for all unit vectors fv2Rd:jjvjj=
1g[23], the following condition holds
E[exp(hv;X i)]exp(c2hv;vi): (22)
Assuming the hyperplane wTX0truncated Normal dis-
tribution with mean zero and covariance matrix , the left
hand side of the (13) can be computed as:
E[exp(hv;X i)] =Z
H+exp(vTX)d(Xj)dX
(23)
whereH+=fX2Rd:wTX0g. SinceRis a sym-
metric, positive deÔ¨Ånite matrix, using Cholesky decomposi-
tion we can have R 1= T where is a non-singular,
upper triangular matrix [19]. By transforming the variables,
we haveY= X. UsingY, with some manipulation as in
[30], one can get
E[exp(hv;X i)] = exp1
22vTv
wTv

(24)
and2=wTw, and [:]is the cumulative distribu-
tion function of the univariate normal distribution. Plugging
=I, one can get
E[exp(hv;Xi)] = exp1
22

wTv
exp1
22
; (25)
where the inequality is valid due to the fact that the CDF
function is equal to 1 in the maximum. Comparing with theright hand side of the (13):
exp1
22
exp1
2c2
; (26)
one can see that it is valid for any c1. Thus, the truncated
Normal distribution is a sub-Gaussian distribution.
The above proof is consistent with our intuition as the
truncated Gaussian has the tails approaching zero at least as
fast as exponential distribution. The truncated part of the
Gaussian is already equal to zero so there is no chance for
being a heavy tailed distribution. Thus, the bound provided
in (21) can be valid for our problem [23].
Since the covariance matrix Ris unknown, we need to
Ô¨Ånd bounds for Tr (R)andmaxas well. It can easily be
obtained that
Tr(R) =d+c2wTw=d+c2 (27)
In order to obtain the maximum eigenvalue of the R, we
use Weyl‚Äôs inequality to have an upper bound for largest
eigenvalue of the covariance matrix as [24]:
max(A+B)max(A) +max(B) (28)
The largest eigenvalue for the identity matrix Iis1. For
the rank-1 matrix c2wwTwhich is the outer product of the
normal vector is given by:
max(c2wwT) =c2Tr(wwT) =c2wTw=c2 (29)
which immediately results in max(R)1 +c2. Sub-
stituting the above values to the (12), the sample mean
N=1
NPN
i=1Xiis the result of MLE estimation, which
satisÔ¨Åes, with probability at least 1 
kN kr
d+c2
N+r
2(1 +c2) log(1=)
N(30)
This bound can provide an upper bound with probability at
least1 for the error of the sample mean while getting N
queries from the neural network.
R
sin(t)=rt
sin(t+1)(31)
B. Proof of Theorem 1
In the following subsections, we consider two cases for
the curvature of the boundary.
Convex Curved Bounded Boundary
We assume that the curvature of the boundary is convex as
given in Fig. 6. As given in [12], if tsatisÔ¨Åes the two
11Gradient DirectionFigure 6: Convex decision boundary with bounded curva-
ture.
assumptions tan2(t)0:2R=r andr=R < 1, the value
forkxt x0k2=rtis given as follows:
rt= (R r) cos(t) +p
(R r)2cos2(t) + 2Rr r2
(32)
wherekxt+1 x0k2=rt+1can be obtained in a similar
way. It can be observed that the value of the rtis an increas-
ing function of the tbecause:
@rt
@t= (R r) sin(t)
 (R r)2cos(t) sin(t)p
(R r)2cos2(t) + 2Rr r2; (33)
Setting@rt
@t>0, with some manipulations one can get
2R > r which shows that rtis an increasing function of
thet. Thus, if we can show that t> t+1, it means that
rt>rt+1which means that rtcan converge to r. Here, we
assume that the given image is in the vicinity of the bound-
aryr=R < 1. The line connecting point otox0intersects
the two parallel lines. Based on the law of sines, one can get
Sincert< R , one can conclude that t> t+1using the
sines law. Thus, as rtis an increasing function of t, we can
getrt+1< rt. Thus, after several iterations, the following
update rule
xt=x0+rt^wNt (34)
converges to the minimum perturbation r.
Applying the sine law for kiterations, one can get the
following equation using (31):
sin(t) =Qt
i=0ri
Rtsin(0) (35)
We know that rt< R and in each iteration, it gets smaller
and smaller. Thus, for the convergence, we consider the
worst case. We know that maxi=0;1;:::;tfrig=rt. Thus, To
Gradient DirectionFigure 7: Concave decision boundary with bounded curva-
ture.
bound this, we can have:
sin(t+K) =QK
k=0rt+k
RKsin(t)(rt
R)Ksin(t)(36)
where can be reduced to
sin(t+K)(rt
R)Ksin(t) (37)
This shows that sin(t+K)converges to zero exponentially
sincert< R . Thus,t+kgoes to zero which results that
the in coinciding the rtandrin the same magnitude. Thus,
we have
lim
k!1rt+k=r (38)
We already know that
rt+1= (R r) cos(t+1)
+p
(R r)2cos2(t+1) + 2Rr r2; (39)
Considering the cosine law, based on the Ô¨Ågure, we can see
that
r2
t= (R+r)2+R2 2R(R+r) cos(t+1) (40)
By combining the above equations and eliminating the
cos(t+1), one can get:
rt+1= (R r)(R+r)2+R2 r2
t
2R(R+r)
+s
(R r)2((R+r)2+R2 r2
t)2
4R2(R+r)2+ 2Rr r2;
(41)
Plugging (41) into the following limit,
lim
t!1rt+1 r
rt r(42)
12fort!1 , we get0
0. Thus, using the L‚ÄôHospital‚Äôs Rule, we
take the derivative of the numerator and the denominator as:
@rt+1
@rt= rt(R r)
R(R+r)
+((R+r)2+R2 r2
t)rt
2q
(R r)2((R+r)2+R2 r2
t)2
4R2(R+r)2 + 2Rr r2(R r)2
R2(R+r)2
(43)
Havingt!1 , we can get rt!r, since we have ^rt!rt,
thus:
lim
t!1^rt+1 r
^rt r=r2(R r)
R2(R+r)=<1 (44)
Asr < R , the rate of convergence 2(0;1)which com-
pletes the proof.
Concave Curved Bounded Boundary
As in [12], the value for kxt x0k2=rtis given as follows:
rt= (R+r) cos(t) p
(R+r)2cos2(t) 2Rr r2
(45)
wherekxt+1 x0k2=rt+1can be obtained in a similar
way. It can easily be seen that the t> t+1. Assuming
r=R < 1,rtis a decreasing function with respect to t
which results in rt< rt+1. Similar proof of convergence
can be obtained for this case as well.
C. Proof of Theorem 2
Given the point rt 1, the goal is to Ô¨Ånd the estimate of
the^rtwith limited query. Assuming the normalized version
of the true gradient wt=t=ktk2, we have
k^wNt wtkpNt(46)
where=p
Tr(R) +p
2maxlog(1=),^wNtis the esti-
mated gradient at iteration tandNtis the number of queries
to estimate the gradient at point xt 1. Based on the reverse
triangle inequality kxk kykkx yk, we can have
k^wNtk 1k^wNt wtkpNt: (47)
Multiplying by rt, we have:
rt rtpNt^rtrt+rtpNt: (48)
where ^rt=rtk^wNtk. Here, we conduct the analysis in the
limit sense and we observe in the simulations that it is valid
in limited iterations as well. Given rt 1, for larget, we
have:
rt r(rt 1 r) (49)Considering the best and worst case for the estimated gra-
dient, we can Ô¨Ånd the following bound. In particular, the
best case is the case in which all the gradient errors are con-
structive and make the ^rtin each iteration smaller than rt.
In contrast, the worst case happens when all the gradients
directions are destructive and make the ^rtgreater than rt.
In practice, however, what is happening is something in be-
tween. Substituting rtfrom (48) in (49), one can obtain:
(rt 1 r) rtpNt^rt r(rt 1 r) +rtpNt(50)
By using the iterative equation, one can get the following
bound:
t(r0 r) e(N)^rt rt(r0 r) +e(N)(51)
wheree(N) =Pt
i=1t iripNiis the error due to limited
number of queries.
D. Proof of Theorem 3
It can easily be observed that the optimization problem
is convex. Thus, the duality gap between this problem and
its dual optimization problem is zero. Therefore, we can
solve the given problem by solving its dual problem. The
Lagrangian is given by:
L(N;) =TX
i=1 iripNi+ TX
i=1Ni N!
(52)
whereis the non-negative dual variable associated with
the budget constraint. The KKT conditions are given as fol-
lows [1]:
@L(N;)
@Nt= 0;8i (53)
 TX
i=0Ni N!
= 0 (54)
TX
i=1NiN (55)
Based on (53), taking the derivative and setting equal to
zero, we can have
Nt= trt
22
3
(56)
We see that the constraint holds with equality. Assume thatPt
i=0Ni6=N, then based on (54), = 0. If= 0 then
based on (56), we have Ni=1;8iwhich contradicts with
(55). Substituting (56) inPt
i=0Ni=N, the Lagrangian
multiplier can be obtained as
2
3=1
22
3PT
i=1( iri)2
3
N(57)
13Substituting in (56), one can get the optimal number of
queries as:
N
t=( trt)2
3
PT
i=1( iri)2
3N (58)
Fort!1 , we havert!r. Based on this, the ratio of the
optimal number if queries for each iteration is given by:
N
t 2
3t
PT
i=1 2
3iN (59)
This equation shows that the distribution of the queries
should be increased by a factor of  2
3where 0<  < 1.
By approximation, we have
N
t+1
N
t 2
3 (60)
which completes the proof.
9. Additional experiment results
Here we show more experiments on the performance of
GeoDA on different `pnorms. In Figs. 8, Figs. 9, Figs. 10,
and Figs. 11, we have generated adversarial examples using
GeoDA. For each image, the Ô¨Årst row consists of (from left
to right) original image, `2fullspace adversarial example,
`2subspace adversarial example, `1fullspace adversarial
example,`1subspace adversarial example, and `1adver-
sarial example, respectively. However, as can be seen the
perturbations are not quite visible in the actual adversarial
examples in the Ô¨Årst row. In the second row, we show the
magniÔ¨Åed version of perturbations for `2and`1. To do so,
the norm of all the perturbations is magniÔ¨Åed to 100 given
that the images coordinate normalized to the 0 to 1 scale.
For the sparse case, we do not magnify the perturbations
as they are visible and equal to their maximum (minimum)
values. Finally, in the third row, we added a magniÔ¨Åed ver-
sion of the perturbation with norm of 30 to have a better
visualization.
Queries ResNet-50 ResNet-101
500 11.76 17.91
GeoDA (`2) 2000 3.35 6.38
10000 1.06 1.87
Table 3: The performance comparison of GeoDA on differ-
ent ResNet image classiÔ¨Åers.
In Table 3, we have compared the performance of
GeoDA with different deep network image classiÔ¨Åers. The
proposed algorithm GeoDA follows almost the same trend
on a wide variety of deep networks. The reason is that the
core assumption of GeoDA, i.e. boundary has a low meancurvature in vicinity of the datapoints, is veriÔ¨Åed empiri-
cally for a wide variety of deep networks. We can provide
the experimental results on different networks.
14image +magnifiedperturbationmagnifiedperturbation
image +magnifiedperturbationmagnifiedperturbationFigure 8: Original images and adversarial perturbations generated by GeoDA for `2fullspace,`2subspace,`1fullspace,`1
subspace, and `1sparse withN= 10000 queries.
15image +magnifiedperturbationmagnifiedperturbation
image +magnifiedperturbationmagnifiedperturbation
Figure 9: Original images and adversarial perturbations generated by GeoDA for `2fullspace,`2subspace,`1fullspace,`1
subspace, and `1sparse withN= 10000 queries.
16image +magnifiedperturbationmagnifiedperturbation
image +magnifiedperturbationmagnifiedperturbationFigure 10: Original images and adversarial perturbations generated by GeoDA for `2fullspace,`2subspace,`1fullspace,
`1subspace, and `1sparse withN= 10000 queries.
17image +magnifiedperturbationmagnifiedperturbation
image +magnifiedperturbationmagnifiedperturbationFigure 11: Original images and adversarial perturbations generated by GeoDA for `2fullspace,`2subspace,`1fullspace,
`1subspace, and `1sparse withN= 10000 queries.
18