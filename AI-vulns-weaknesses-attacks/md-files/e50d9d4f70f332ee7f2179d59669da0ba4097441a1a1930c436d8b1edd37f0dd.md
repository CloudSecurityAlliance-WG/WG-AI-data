BREAKING CERTIFIED DEFENSES : SEMANTIC ADVER -
SARIAL EXAMPLES WITH SPOOFED ROBUSTNESS CER -
TIFICATES
Amin Ghiasi, Ali Shafahi& Tom Goldstein
University of Maryland
famin,ashafahi,tomg g@cs.umd.edu
ABSTRACT
To deﬂect adversarial attacks, a range of “certiﬁed” classiﬁers have been proposed.
In addition to labeling an image, certiﬁed classiﬁers produce (when possible) a
certiﬁcate guaranteeing that the input image is not an `p-bounded adversarial ex-
ample. We present a new attack that exploits not only the labelling function of
a classiﬁer, but also the certiﬁcate generator. The proposed method applies large
perturbations that place images far from a class boundary while maintaining the
imperceptibility property of adversarial examples. The proposed “Shadow At-
tack” causes certiﬁably robust networks to mislabel an image and simultaneously
produce a “spoofed” certiﬁcate of robustness.
1 I NTRODUCTION
Conventional training of neural networks has been shown to produce classiﬁers that are highly sensi-
tive to adversarial perturbations (Szegedy et al., 2013; Biggio et al., 2013), “natural looking” images
that have been manipulated to cause misclassiﬁcation by a neural network (Figure 1). While a wide
range of defenses exist that harden neural networks against such attacks (Madry et al., 2017; Shafahi
et al., 2019), defenses based on heuristics and tricks are often easily breakable Athalye et al. (2018).
This has motivated work on certiﬁably secure networks — classiﬁers that produce a label for an
image, and also (when possible) a rigorous guarantee that the input is not adversarially manipulated
(Cohen et al., 2019; Zhang et al., 2019b).
Natural
macaw (0:6%)`1: 8=255
bucket (1:1%)Semantic
bucket (0:6%)Shadow Attack
bucket (0:6%)
Figure 1: All adversarial examples have the goal of fooling classiﬁers while looking “natural”.
Standard attacks limit the `p-norm of the perturbation, while semantic attacks have large `p-norm
while still producing natural looking images. Our attack produces large but visually subtle seman-
tic perturbations that not only cause misclassiﬁcation, but also cause a certiﬁed defense to issue a
“spoofed” high-conﬁdence certiﬁcate. In this case, a certiﬁed Gaussian smoothing classiﬁer misla-
bels the image, and yet issues a certiﬁcate with radius 0.24, which is a larger certiﬁed radius than its
corresponding unmodiﬁed ImageNet image which is 0.13.
equal contribution
1arXiv:2003.08937v1 [cs.LG] 19 Mar 2020To date, all work on certiﬁable defenses has focused on deﬂecting `p-bounded attacks, where p= 2
or1(Cohen et al., 2019; Gowal et al., 2018; Wong et al., 2018). After labelling an image, these
defenses then check whether there exists an image of a different label within units (in the `pmetric)
of the input, where is a security parameter chosen by the user. If the classiﬁer assigns all images
within theball the same class label, then a certiﬁcate is issued, and the input image known not to
be an`padversarial example.
In this work, we demonstrate how a system that relies on certiﬁcates as a measure of label security
can be exploited. We present a new class of adversarial examples that target not only the classiﬁer
output label, but also the certiﬁcate. We do this by adding adversarial perturbations to images that
are large in the `pnorm (larger than the used by the certiﬁcate generator), and produce attack
images that are surrounded by a large ball exclusively containing images of the same label. The
resulting attacks produce a “spoofed” certiﬁcate with a seemingly strong security guarantee despite
being adversarially manipulated. Note that the statement made by the certiﬁcate (i.e., that the input
image is not an adversarial example in the chosen norm) is still technically correct, however in this
case the adversary is hiding behind a certiﬁcate to avoid detection by a certiﬁable defense.
In summary, we consider methods that attack a certiﬁed classiﬁer in the following sense:
Imperceptibility: the adversarial example “looks like” its corresponding natural base ex-
ample,
Misclassiﬁcation: the certiﬁed classiﬁer assigns an incorrect label to the adversarial exam-
ple, and
Strongly certiﬁed: the certiﬁed classiﬁer provides a strong/large-radius certiﬁcate for the
adversarial example.
While the existence of such an attack does not invalidate the certiﬁcates produced by certiﬁable
systems, it should serve as a warning that certiﬁable defenses are not inherently secure, and one
should take caution when relying on certiﬁcates as an indicator of label correctness.
BACKGROUND
In the white-box setting, where the attacker knows the victim’s network and parameters, adversarial
perturbation are often constructed using ﬁrst-order gradient information (Carlini & Wagner, 2017;
Kurakin et al., 2016; Moosavi-Dezfooli et al., 2016) or using approximations of the gradient (Uesato
et al., 2018; Athalye et al., 2018). The prevailing formulation for crafting attacks uses an additive
adversarial perturbation, and perceptibility is minimized using an `p-norm constraint. For example,
`1-bounded attacks limit how much each pixel can move, while `0adversarial attacks limit the
number of pixels that can be modiﬁed, without limiting the size of the change to each pixel (Wiyatno
& Xu, 2018).
It is possible to craft imperceptible attacks without using `pbounds (Brown et al., 2018). For
example, Hosseini & Poovendran (2018) use shifting color channels, Wong et al. (2019) use the
Wasserstein ball/distance, and Engstrom et al. (2017) use rotation and translation to craft “seman-
tic” adversarial examples. In Figure 1, we produce semantic adversarial examples using the method
of Hosseini & Poovendran (2018) which is a greedy approach that transforms the image into HSV
space, and then, while keeping V constant, tries to ﬁnd the smallest S perturbation causing mis-
classiﬁcation1. Other variants use generative models to construct natural looking images causing
misclassiﬁcation (Song et al., 2018; Dunn et al., 2019).
In practice, many of the defenses which top adversarial defense leader-board challenges are non-
certiﬁed defenses (Madry et al., 2017; Zhang et al., 2019a; Shafahi et al., 2019). The majority of
these defenses make use of adversarial training , in which attack images are crafted during training
and injected into the training set. These non-certiﬁed defenses are mostly evaluated against PGD-
based attacks, resulting in an upper-bound on robustness.
Certiﬁed defenses, on the other-hand, provably make networks resist `p-bounded perturbations of
a certain radius. For instance, randomized smoothing (Cohen et al., 2019) is a certiﬁable defense
against`2-norm bounded attacks, and CROWN-IBP (Zhang et al., 2019b) is a certiﬁable defense
1In ﬁg. 1, the adversarial example has saturation=0
2againstl1-norm bounded perturbations. Both of these defenses produce a class label, and also a
guarantee that the image could not have been crafted by making small perturbations to an image of
a different label. Certiﬁed defenses can also beneﬁt from adversarial training. For example, Salman
et al. (2019) recently improved the certiﬁed radii of randomized smoothing (Cohen et al., 2019) by
training on adversarial examples generated for the smoothed classiﬁer.
To the best of our knowledge, prior works have focused on making adversarial examples that sat-
isfy the imperceptibility and misclassiﬁcation conditions, but none have investigated manipulating
certiﬁcates, which is our focus here.
The reminder of this paper is organized as follows. In section 2 we introduce our new approach
Shadow Attack for generating adversarial perturbations. This is a hybrid model that allows various
kinds of attacks to be compounded together, resulting in perturbations of large radii. In section
3 we present an attack on “randomized smoothing” certiﬁcates (Cohen et al., 2019). Section 4
shows an ablation study which illustrates why the elements of the Shadow Attack are important
for successfully manipulating certiﬁed models. In section 5 we generate adversarial examples for
“CROWN-IBP” (Zhang et al., 2019b). Finally, we discuss results and wrap up in section 6.
2 T HESHADOW ATTACK
Because certiﬁcate spooﬁng requires large perturbations (larger than the `pball of the certiﬁcate),
we propose a simple attack that ensembles numerous modes to create large perturbations. Our attack
can be seen as the generalization of the well-known PGD attack, which creates adversarial images
by modifying a clean base image. Given a loss function Land an`p-norm bound for somep0,
PGD attacks solve the following optimization problem:
max
L(;x+) (1)
s.t.kkp; (2)
whereare the network parameters and is the adversarial perturbation to be added to the clean
input image x. Constraint 2 promotes imperceptibility of the resulting perturbation to the human
eye by limiting the perturbation size. In the shadow attack, instead of solving the above constrained
optimization problem, we solve the following problem with a range of penalties:
max
L(;x+) cC() tvTV() sDissim (); (3)
wherec;tv;sare scalar penalty weights. Penalty TV()forces the perturbation to have small
total variation ( TV), and so appear more smooth and natural. Penalty C()limits the perturbation 
globally by constraining the change in the mean of each color channel c. This constraint is needed
since total variation is invariant to constant/scalar additions to each color channel, and it is desirable
to suppress extreme changes in the color balances of images.
PenaltyDissim ()promotes perturbations that assume similar values in each color channel. In
the case of an RGB image of shape 3WH, ifDissim ()is small, the perturbations to red,
green, and blue channels are similar, i.e., R;w;hG;w;hB;w;h;8(w;h)2WH. This
amounts to making the pixels darker/lighter, without changing the color balance of the image. Later,
in section 3, we suggest two ways of enforcing such similarity between RGB channels and we ﬁnd
both of them effective:
1-channel attack strictly enforces R;iG;iB;i;8iby using just one array to si-
multaneously represent each color channel WH:On the forward pass, we duplicate to
make a 3-channel image. In this case, Dissim () = 0 , and the perturbation is greyscale.
3-channel attack uses a 3-channel perturbation 3WH, along with the dissimilarity met-
ricDissim () =kR Bkp+kR Gkp+kB Gkp.
All together, the three penalties minimize perceptibility of perturbations by forcing them to be (a)
small, (b)smooth, and (c)without dramatic color changes (e.g. swapping blue to red). At the same
time, these penalties allow perturbations that are very large in `p-norm.
32.1 C REATING UNTARGETED ATTACKS
We focus on spooﬁng certiﬁcates for untargeted attacks, in which the attacker does not specify the
class into which the attack image moves. To achieve this, we generate an adversarial perturbation
for all possible wrong classes yand choose the best one as our strong attack:
max
y6=y; L(;x+ky) cC() tvTV() sDissim () (4)
whereyis the true label/class for the clean image x, andLis a spooﬁng loss that promotes a strong
certiﬁcate. We examine different choices for Lfor different certiﬁcates below.
3 A TTACKS ON RANDOMIZED SMOOTHING
The Randomized Smoothing method, ﬁrst proposed by Lecuyer et al. (2018) and later improved by
Li et al. (2018), is an adversarial defense against `2-norm bounded attacks. Cohen et al. (2019) prove
a tight robustness guarantee under the `2norm for smoothing with Gaussian noise. Their study was
the ﬁrst certiﬁable defense for the ImageNet dataset (Deng et al., 2009). The method constructs
certiﬁcates by ﬁrst creating many copies of an input image contaminated with random Gaussian
noise of standard deviation . Then, it uses a base classiﬁer (a neural net) to make a prediction for
all of the images in the randomly augmented batch. Depending on the level of the consensus of the
class labels at these random images, a certiﬁed radius is calculated that can be at most 4(in the
case of perfect consensus).
Intuitively, if the image is far away from the decision boundary, the base classiﬁer should predict
the same label for each noisy copy of the test image, in which case the certiﬁcate is strong. On the
other hand, if the image is adjacent to the decision boundary, the base classiﬁer’s predictions for the
Gaussian augmented copies may vary. If the variation is large, the smoothed classiﬁer abstains from
making a prediction.
To spoof strong certiﬁcates (large certiﬁed radius) for an incorrect class, we must make sure that
the majority of a batch of noisy images around the adversarial image are assigned the same (wrong)
label. We do this by minimizing the cross entropy loss relative to a chosen (incorrect) label, averaged
over a large set of randomly perturbed images. To this end, we minimize equation 4, where Lis
chosen to be the average cross-entropy over a batch of Gaussian perturbed copies. This method is
analogous to the technique presented by Shafahi et al. (2018) for generating universal perturbations
that are effective when added to a large number of different images.
RESULTS
Cohen et al. (2019) show the performance of the Gaussian smoothed classiﬁer on CIFAR-10
(Krizhevsky et al.) and ImageNet (Deng et al., 2009). To attack the CIFAR-10 and ImageNet
smoothed classiﬁers, we use 400randomly sampled Gaussian images, tv= 0:3,c= 1:0, and
perform 300 steps of SGD with learning rate 0:1. We choose the functional regularizers C()and
TV()to be
C() =kAvg(jRj);Avg(jGj);Avg(jBj)k2
2;andTV(i;j) =anisotropic-TV (i;j)2;
wherejjis the element-wise absolute value operator, and Avg computes the average. For the
Dissim regularizer, we experiment with both the 1-Channel attack that ensures Dissim () = 0;
and the 3-Channel attack by setting Dissim () =k(R G)2;(R B)2;(G B)2k2and
s= 0:5. For the validation examples on which the smoothed classiﬁer does not abstain (see
Cohen et al. (2019) for more details), the less-constrained 3-channel attack is always able to ﬁnd
an adversarial example while the 1-channel attack also performs well, achieving 98.5% success.
2In section 4 we will discuss in more detail other differences between 1-channel and 3-channel
attacks. The results are summarized in Table 1. For the various base-models and choices of , our
adversarial examples are able to produce certiﬁed radii that are on average larger than the certiﬁed
radii produced for natural images. For ImageNet, since attacking all 999 remaining target classes
is computationally expensive, we only attacked target class IDs 100, 200, 300, 400, 500, 600, 700,
800, 900, and 1000.
2Source code for all experiments can be found at: https://github.com/AminJun/BreakingCertiﬁableDefenses
4Table 1: Certiﬁed radii produced by the Randomized Smoothing method for Shadow Attack images
and also natural images (larger radii means a stronger/more conﬁdent certiﬁcate).
Dataset (l2)Unmodiﬁed/Natural Images Shadow Attack
Mean STD Mean STD
CIFAR-100.12 0.14 0.056 0.22 0.005
0.25 0.30 0.111 0.35 0.062
0.50 0.47 0.234 0.65 0.14
1.00 0.78 0.556 0.85 0.442
ImageNet0.25 0.30 0.109 0.31 0.109
0.50 0.61 0.217 0.38 0.191
1.00 1.04 0.519 0.64 0.322
(a) Natural image ( x)-
(b) Adversarial perturbation ( )-
(c) Adversarial example ( x+)
Figure 2: An adversarial example built using our Shadow Attack for the smoothed ImageNet classi-
ﬁer for which the certiﬁable classiﬁer produces a large certiﬁed radii. The adversarial perturbation
is smooth and natural looking even-though it is large when measured using `p-metrics. Also see
Figure 16 in the appendix.
Figure 2 depicts a sample adversarial example built for the smoothed ImageNet classiﬁer that pro-
duces a strong certiﬁcate. The adversarial perturbation causes the batch of Gaussian augmented
black swan images to get misclassiﬁed as hooks. For more, see appendix 16.
4 A BLATION STUDY OF THE ATTACK PARAMETERS
In this section we perform an ablation study on the parameters of the Shadow Attack to evaluate
(i)the number of SGD steps needed, (ii)the importance of s(or alternatively using 1-channel
attacks), and (iii)the effect of tv.
The default parameters for all of the experiments are as follows unless explicitly mentioned: We
use30SGD steps with learning rate 0:1for the optimization. All experiments except part (ii)use
1-channel attacks for the sake of simplicity and efﬁciency (since it has less parameters). We assume
tv= 0:3,c= 20;and use batch-size 50. We present results using the ﬁrst example from each
class of the CIFAR-10 validation set.
Figure 3 shows how the adversarial example evolves during the ﬁrst few steps of optimization (See
appendix 13 for more examples). Also, ﬁgures 4, 5, and 6 show the evolution of L(),TV(), and
C(), respectively (Note that we use 1-channel attacks, so Dissim ()is always 0). We ﬁnd that
taking just 10 SGD steps is enough for convergence on CIFAR-10, but for our main results (i.e.
attacking Randomized Smoothing in section 3 and attacking CROWN-IBP in section 5) we take 300
steps to be safe.
50 1 2 3 4 5 6 7 8 9 10 original
Figure 3: The ﬁrst 10 steps of optimization (beginning with a randomly perturbed image copy).
0.0 0.2 0.4 0.6 0.80.02.55.07.510.012.515.017.520.0 Natural
1-Channel
3-Channel
Figure 9: Histogram of random-
ized smoothed certiﬁcate radii for
100 randomly sampled CIFAR-10
validation images vs those calcu-
lated for their adversarial examples
crafted using our 1-channel and 3-
channel adversarial Shadow Attack
attacks. The “robust” victim clas-
siﬁer is based off Resnet-110, and
smoothed with = 0:50. 1-channel
attacks are almost as good as the
less-restricted 3-channel attacks.
0 2 4 6 8 10050100150200250300mean (Lb(δ))
Figure 4: Average Lb()in
the ﬁrst 10steps.
0 2 4 6 8 1002004006008001000mean (TV(δ))
Figure 5: Average TV()in
the ﬁrst 10steps.
0 2 4 6 8 100.0250.0500.0750.1000.1250.1500.175mean (C(δ))
Figure 6: Average C()in the
ﬁrst10steps.
1 2 3 4 5
λs0.10.20.30.40.50.6Dissim (δ)
Figure 7: The effect of son
the resulting Dissim ()
0.00 0.05 0.10 0.15 0.20 0.25 0.30
λtv0100200300400500TV(δ)
Figure 8: The effect of tvon
the resulting TV()
To explore the importance of s, we use 3-channel attacks and vary sto produce different images
in ﬁgure 113.
3See ﬁgure 14 in the appendix for more examples.
6natural
1-channel
3-channel
Figure 10: The visual effect of Shadow Attack on 9 randomly selected CIFAR-10 examples using
1-Channel and 3-Channel attacks.
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 original
Figure 11: The visual effect of son perceptibility of the perturbations. The ﬁrst row shows the
value ofs.
Also, ﬁgure 7 shows the mean Dissim ()for different values of s(0s5:0). We also
plot the histogram of the certiﬁcate radii in ﬁgure 9. Figure 10 compares 1-Channel vs 3-Channel
attacks for some of randomly selected CIFAR-10 images. Finally, we explore the effect of tvon
imperceptibility of the perturbations in Figure 12. See table 15 for more images, and ﬁgure 8 for the
impact of parameters on TV().
5 A TTACKS ON CROWN-IBP
Interval Bound Propagation (IBP) methods have been recently studied as a defense against `1-
bounded attacks. Many recent studies such as Gowal et al. (2018); Xiao et al. (2018); Wong et al.
(2018); Mirman et al. (2018) have investigated IBP methods to train provably robust networks. To
the best of our knowledge, the CROWN-IBP method by Zhang et al. (2019b) achieves state-of-
the-art performance for MNIST (LeCun & Cortes, 2010), Fashion-MNIST (Xiao et al., 2017), and
CIFAR-10 datasets among certiﬁable `1defenses. In this section we focus on attacking Zhang et al.
(2019b) using CIFAR-10.
IBP methods (over)estimate how much a small `1-bounded noise in the input can impact the clas-
siﬁcation layer. This is done by propagating errors from layer to layer, computing bounds on the
maximum possible perturbation to each activation. During testing, the user chooses an `1per-
turbation bound ;and error propagation is used to bound the magnitude of the largest achievable
perturbation in network output. If the output perturbation is not large enough to ﬂip the image la-
bel, then a certiﬁcate is produced. If the output perturbation is large enough to ﬂip the image label,
0.0 0.03 0.06 0.09 0.12 0.15 0.18 0.21 0.24 0.27 0.30 original
Figure 12: The visual effect of tvon the on imperceptibility of the perturbations. The ﬁrst row
shows the value of tv
7Table 2: “Robust error” for natural images, and “attack error” for Shadow Attack images using the
CIFAR-10 dataset, and CROWN-IBP models. Smaller is better.
(l1) Model Family MethodRobustness Errors
Min Mean Max
2=2559 small modelsCROWN-IPB 52.46 57.55 60.67
Shadow Attack 45.90 53.89 65.74
8 large modelsCROWN-IBP 52.52 53.9 56.05
Shadow Attack 46.21 49.77 51.79
8=2559 small modelsCROWN-IBP 71.28 72.15 73.66
Shadow Attack 63.43 66.94 71.02
8 large modelsCROWN-IBP 70.79 71.17 72.29
Shadow Attack 64.04 67.32 71.16
then a certiﬁcate is not produced. During network training, IBP methods include a term in the loss
function that promotes tight error bounds that result in certiﬁcates. Our attack directly uses the
loss function term used during IBP network training in addition to the Shadow Attack penalties;
we search for an image that is visually similar to the base image, while producing a bound on the
output perturbation that is too small to ﬂip the image label. Note that there is a subtle difference
between crafting conventional adversarial examples which ultimately targets misclassiﬁcation and
our adversarial examples which aim to produce adversarial examples which cause misclassiﬁcation
andproduce strong certiﬁcates. In the former case, we only need to attack the cross-entropy loss. If
we use our Shadow Attack to craft adversarial examples based on the cross-entropy loss, the robust-
ness errors are on average roughly 50% larger than those reported in table 2 (i.e., simple adversarial
examples produce weaker certiﬁcates.)
We attack 4 classes of networks released by Zhang et al. (2019b) for CIFAR-10. There are two
classes of IBP architectures, one of them consists of 9 small models and the other consists of 8
larger models. For each class of architecture, there are two sets of pre-trained models: one for
= 2=255and one for = 8=255. We usetv= 0:000009 ,c= 0:02,C() =kk2and set the
learning rate to 200and for the rest of the regularizers and hyper-paramters we use the same hyper-
parameters and regularizers as in 3. For the sake of efﬁciency, we only do 1-channel attacks. We
attack the 4 classes of models and for each class, we report the min, mean, and max of the robustness
errors and compare them with those of the CROWN-IBP paper.
To quantify the success of our attack, we deﬁne two metrics of error. For natural images, we report
the rate of “robust errors,” which are images that are either (i) incorrectly labeled, or (ii) correctly
labelled but without a certiﬁcate. In contrast, for attack images, we report the rate of “attack errors,”
which are either (i) correctly classiﬁed or (ii) incorrectly classiﬁed but without a certiﬁcate. Table 2
shows the robust error on natural images, and the attack error on Shadow Attack images for the
CROWN-IBP models. With = 2=255, our attack ﬁnds adversarial examples that certify roughly
15% of the time (i.e., attack error <85%). With = 8=255, our attack ﬁnds adversarial examples
that are incorrectly classiﬁed, and yet certify even more often than natural images.
6 CONCLUSION
We demonstrate that it is possible to produce adversarial examples with “spoofed” certiﬁed ro-
bustness by using large-norm perturbations. Our adversarial examples are built using our Shadow
Attack that produces smooth and natural looking perturbations that are often less perceptible than
those of the commonly used `p-bounded perturbations, while being large enough in norm to escape
the certiﬁcation regions of state of the art principled defenses. This work suggests that the certiﬁ-
cates produced by certiﬁably robust classiﬁers, while mathematically rigorous, are not always good
indicators of robustness or accuracy.
Acknowledgements: Goldstein and his students were supported by the DARPA QED for RML
program, the DARPA GARD program, and the National Science Foundation.
8REFERENCES
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420 ,
2018.
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim ˇSrndi ´c, Pavel Laskov, Gior-
gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint
European conference on machine learning and knowledge discovery in databases , pp. 387–402.
Springer, 2013.
Tom B Brown, Nicholas Carlini, Chiyuan Zhang, Catherine Olsson, Paul Christiano, and Ian Good-
fellow. Unrestricted adversarial examples. arXiv preprint arXiv:1809.08352 , 2018.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. In Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and
Security , pp. 3–14. ACM, 2017.
Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certiﬁed adversarial robustness via randomized
smoothing. arXiv preprint arXiv:1902.02918 , 2019.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09 , 2009.
Isaac Dunn, Tom Melham, and Daniel Kroening. Generating realistic unrestricted adversarial inputs
using dual-objective gan training. arXiv preprint arXiv:1905.02463 , 2019.
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A
rotation and a translation sufﬁce: Fooling cnns with simple transformations. arXiv preprint
arXiv:1712.02779 , 2017.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for
training veriﬁably robust models. arXiv preprint arXiv:1810.12715 , 2018.
Hossein Hosseini and Radha Poovendran. Semantic adversarial examples. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition Workshops , pp. 1614–1619, 2018.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-
search). URL http://www.cs.toronto.edu/ ˜kriz/cifar.html .
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533 , 2016.
Cassidy Laidlaw and Soheil Feizi. Functional adversarial attacks. arXiv preprint arXiv:1906.00001 ,
2019.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/ .
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certiﬁed
robustness to adversarial examples with differential privacy. arXiv preprint arXiv:1802.03471 ,
2018.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Second-order adversarial attack
and certiﬁable robustness. CoRR , abs/1809.03113, 2018. URL http://arxiv.org/abs/
1809.03113 .
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 ,
2017.
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning , pp. 3575–3583,
2018.
9Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition , pp. 2574–2582, 2016.
Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien
Bubeck. Provably robust deep learning via adversarially trained smoothed classiﬁers. arXiv
preprint arXiv:1906.04584 , 2019.
Ali Shafahi, Mahyar Najibi, Zheng Xu, John Dickerson, Larry S Davis, and Tom Goldstein. Uni-
versal adversarial training. arXiv preprint arXiv:1811.11304 , 2018.
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S
Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! arXiv preprint
arXiv:1904.12843 , 2019.
Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Constructing unrestricted adversarial
examples with generative models. In Advances in Neural Information Processing Systems , pp.
8312–8323, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 , 2013.
Jonathan Uesato, Brendan O’Donoghue, Aaron van den Oord, and Pushmeet Kohli. Adversarial risk
and the dangers of evaluating against weak attacks. arXiv preprint arXiv:1802.05666 , 2018.
Rey Wiyatno and Anqi Xu. Maximal jacobian-based saliency map attack. arXiv preprint
arXiv:1808.07945 , 2018.
Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial
defenses. In Advances in Neural Information Processing Systems , pp. 8400–8409, 2018.
Eric Wong, Frank R Schmidt, and J Zico Kolter. Wasserstein adversarial examples via projected
sinkhorn iterations. arXiv preprint arXiv:1902.07906 , 2019.
Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
Kai Y Xiao, Vincent Tjeng, Nur Muhammad Shaﬁullah, and Aleksander Madry. Training for faster
adversarial robustness veriﬁcation via inducing relu stability. arXiv preprint arXiv:1809.03008 ,
2018.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I
Jordan. Theoretically principled trade-off between robustness and accuracy. arXiv preprint
arXiv:1901.08573 , 2019a.
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Towards sta-
ble and efﬁcient training of veriﬁably robust neural networks. arXiv preprint arXiv:1906.06316 ,
2019b.
10A A PPENDIX
In this section, we include the complete results of our ablation study. As we mentioned in section
4, we use is a subset of CIFAR-10 dataset, including one example per each class. For the sake of
simplicity, we call the dataset Tiny-CIFAR-10. Here, we show the complete results for the ablation
experiments on all of Tiny-CIFAR-10 examples. Figure 13 shows that taking a few optimization
steps is enough for the resulting images to look natural-looking. Figure 14 and 15, respectively
show the effect of sandTVon the imperceptability of the perturbations.
110 1 2 3 4 5 6 7 8 9 10 original
Figure 13: The ﬁrst 10 steps of the optimization vs the original image for Tiny-CIFAR-10. See
section 4 for the details of the experiments.
120.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 original
Figure 14: The visual effect of sonDissim ()on Tiny-CIFAR-10. See section 4 for the details
of the experiments.
130.0 0.03 0.06 0.09 0.12 0.15 0.18 0.21 0.24 0.27 0.30 original
Figure 15: The visual effect of tvon the perturbation Tiny-CIFAR-10. See section 4 for the details
of the experiments.
14Table 3: Certiﬁed radii statistics produced by the Adversarially Trained Randomized Smoothing
method for our adversarial examples crafted using Shadow Attack and the natural examples (larger
radii are better).
Dataset (l2)Adversarially Trained Randomized Smoothed Shadow Attack
Mean STD Mean STD
CIFAR-10 0.5 0.60 0.34 0.65 0.16
IMAGE NETRESULTS
Many of the recent studies have explored the semantic attacks. Semantic attacks are powerful for
attacking defenses (Engstrom et al., 2017; Hosseini & Poovendran, 2018; Laidlaw & Feizi, 2019).
Many of semantic attacks are applicable to Imagenet, however, none of them consider increasing the
radii of the certiﬁcates generated by the certiﬁable defenses.
Some other works focus on using generative models to generate adversarial examples (Song et al.,
2018), but unfortunately none of the GAN’s are expressive enough to capture the manifold of the
ImageNet.
Figure 16 illustrates some of our successful examples generated by Shadow Attack to attack Ran-
domized Smoothed classiﬁers for ImageNet.
B C ERTIFICATE SPOOFING ATTACKS ON ADVERSARIALLY TRAINED SMOOTH
CLASSIFIERS
Recently, Salman et al. (2019) signiﬁcantly improved the robustness of Gaussian smoothed classi-
ﬁers by generating adversarial examples for the smoothed classiﬁer and training on them. In this
section, we use our Shadow Attack to generate adversarial examples using the loss in eq. 4 for the
SmoothAdv classiﬁer4. Due to computation limitations, we attack a sample of 40% of the same
validation images used for evaluating the randomized smooth classiﬁer in section 3. The results are
summarized in table 3. Comparing the results of table 1 to table 3, we can see that the SmoothAdv
classiﬁer, does produce stronger certiﬁed radii for natural examples (many of the examples in fact
have the maximum radii) compared to the original randomized smoothing classiﬁer. This can be
associate to the excessive invariance introduced as a result of adversarial training. However, table 3
empirically veriﬁes that its certiﬁcates are still subject to attacks and the certiﬁcate should not be
used as a measure for robustness.
4We use the ofﬁcial models released at https://github.com/Hadisalman/smoothing-adversarial for attacking
the SmoothAdv classiﬁer.
15Figure 16: Natural looking Imperceptible ImageNet adversarial images which produce large certi-
ﬁed radii for the ImageNet Gaussian smoothed classiﬁer.
16