BREAKING CERTIFIED DEFENSES : SEMANTIC ADVER -
SARIAL EXAMPLES WITH SPOOFED ROBUSTNESS CER -
TIFICATES
Amin Ghiasi, Ali Shafahi& Tom Goldstein
University of Maryland
famin,ashafahi,tomg g@cs.umd.edu
ABSTRACT
To deÔ¨Çect adversarial attacks, a range of ‚ÄúcertiÔ¨Åed‚Äù classiÔ¨Åers have been proposed.
In addition to labeling an image, certiÔ¨Åed classiÔ¨Åers produce (when possible) a
certiÔ¨Åcate guaranteeing that the input image is not an `p-bounded adversarial ex-
ample. We present a new attack that exploits not only the labelling function of
a classiÔ¨Åer, but also the certiÔ¨Åcate generator. The proposed method applies large
perturbations that place images far from a class boundary while maintaining the
imperceptibility property of adversarial examples. The proposed ‚ÄúShadow At-
tack‚Äù causes certiÔ¨Åably robust networks to mislabel an image and simultaneously
produce a ‚Äúspoofed‚Äù certiÔ¨Åcate of robustness.
1 I NTRODUCTION
Conventional training of neural networks has been shown to produce classiÔ¨Åers that are highly sensi-
tive to adversarial perturbations (Szegedy et al., 2013; Biggio et al., 2013), ‚Äúnatural looking‚Äù images
that have been manipulated to cause misclassiÔ¨Åcation by a neural network (Figure 1). While a wide
range of defenses exist that harden neural networks against such attacks (Madry et al., 2017; Shafahi
et al., 2019), defenses based on heuristics and tricks are often easily breakable Athalye et al. (2018).
This has motivated work on certiÔ¨Åably secure networks ‚Äî classiÔ¨Åers that produce a label for an
image, and also (when possible) a rigorous guarantee that the input is not adversarially manipulated
(Cohen et al., 2019; Zhang et al., 2019b).
Natural
macaw (0:6%)`1: 8=255
bucket (1:1%)Semantic
bucket (0:6%)Shadow Attack
bucket (0:6%)
Figure 1: All adversarial examples have the goal of fooling classiÔ¨Åers while looking ‚Äúnatural‚Äù.
Standard attacks limit the `p-norm of the perturbation, while semantic attacks have large `p-norm
while still producing natural looking images. Our attack produces large but visually subtle seman-
tic perturbations that not only cause misclassiÔ¨Åcation, but also cause a certiÔ¨Åed defense to issue a
‚Äúspoofed‚Äù high-conÔ¨Ådence certiÔ¨Åcate. In this case, a certiÔ¨Åed Gaussian smoothing classiÔ¨Åer misla-
bels the image, and yet issues a certiÔ¨Åcate with radius 0.24, which is a larger certiÔ¨Åed radius than its
corresponding unmodiÔ¨Åed ImageNet image which is 0.13.
equal contribution
1arXiv:2003.08937v1 [cs.LG] 19 Mar 2020To date, all work on certiÔ¨Åable defenses has focused on deÔ¨Çecting `p-bounded attacks, where p= 2
or1(Cohen et al., 2019; Gowal et al., 2018; Wong et al., 2018). After labelling an image, these
defenses then check whether there exists an image of a different label within units (in the `pmetric)
of the input, where is a security parameter chosen by the user. If the classiÔ¨Åer assigns all images
within theball the same class label, then a certiÔ¨Åcate is issued, and the input image known not to
be an`padversarial example.
In this work, we demonstrate how a system that relies on certiÔ¨Åcates as a measure of label security
can be exploited. We present a new class of adversarial examples that target not only the classiÔ¨Åer
output label, but also the certiÔ¨Åcate. We do this by adding adversarial perturbations to images that
are large in the `pnorm (larger than the used by the certiÔ¨Åcate generator), and produce attack
images that are surrounded by a large ball exclusively containing images of the same label. The
resulting attacks produce a ‚Äúspoofed‚Äù certiÔ¨Åcate with a seemingly strong security guarantee despite
being adversarially manipulated. Note that the statement made by the certiÔ¨Åcate (i.e., that the input
image is not an adversarial example in the chosen norm) is still technically correct, however in this
case the adversary is hiding behind a certiÔ¨Åcate to avoid detection by a certiÔ¨Åable defense.
In summary, we consider methods that attack a certiÔ¨Åed classiÔ¨Åer in the following sense:
Imperceptibility: the adversarial example ‚Äúlooks like‚Äù its corresponding natural base ex-
ample,
MisclassiÔ¨Åcation: the certiÔ¨Åed classiÔ¨Åer assigns an incorrect label to the adversarial exam-
ple, and
Strongly certiÔ¨Åed: the certiÔ¨Åed classiÔ¨Åer provides a strong/large-radius certiÔ¨Åcate for the
adversarial example.
While the existence of such an attack does not invalidate the certiÔ¨Åcates produced by certiÔ¨Åable
systems, it should serve as a warning that certiÔ¨Åable defenses are not inherently secure, and one
should take caution when relying on certiÔ¨Åcates as an indicator of label correctness.
BACKGROUND
In the white-box setting, where the attacker knows the victim‚Äôs network and parameters, adversarial
perturbation are often constructed using Ô¨Årst-order gradient information (Carlini & Wagner, 2017;
Kurakin et al., 2016; Moosavi-Dezfooli et al., 2016) or using approximations of the gradient (Uesato
et al., 2018; Athalye et al., 2018). The prevailing formulation for crafting attacks uses an additive
adversarial perturbation, and perceptibility is minimized using an `p-norm constraint. For example,
`1-bounded attacks limit how much each pixel can move, while `0adversarial attacks limit the
number of pixels that can be modiÔ¨Åed, without limiting the size of the change to each pixel (Wiyatno
& Xu, 2018).
It is possible to craft imperceptible attacks without using `pbounds (Brown et al., 2018). For
example, Hosseini & Poovendran (2018) use shifting color channels, Wong et al. (2019) use the
Wasserstein ball/distance, and Engstrom et al. (2017) use rotation and translation to craft ‚Äúseman-
tic‚Äù adversarial examples. In Figure 1, we produce semantic adversarial examples using the method
of Hosseini & Poovendran (2018) which is a greedy approach that transforms the image into HSV
space, and then, while keeping V constant, tries to Ô¨Ånd the smallest S perturbation causing mis-
classiÔ¨Åcation1. Other variants use generative models to construct natural looking images causing
misclassiÔ¨Åcation (Song et al., 2018; Dunn et al., 2019).
In practice, many of the defenses which top adversarial defense leader-board challenges are non-
certiÔ¨Åed defenses (Madry et al., 2017; Zhang et al., 2019a; Shafahi et al., 2019). The majority of
these defenses make use of adversarial training , in which attack images are crafted during training
and injected into the training set. These non-certiÔ¨Åed defenses are mostly evaluated against PGD-
based attacks, resulting in an upper-bound on robustness.
CertiÔ¨Åed defenses, on the other-hand, provably make networks resist `p-bounded perturbations of
a certain radius. For instance, randomized smoothing (Cohen et al., 2019) is a certiÔ¨Åable defense
against`2-norm bounded attacks, and CROWN-IBP (Zhang et al., 2019b) is a certiÔ¨Åable defense
1In Ô¨Åg. 1, the adversarial example has saturation=0
2againstl1-norm bounded perturbations. Both of these defenses produce a class label, and also a
guarantee that the image could not have been crafted by making small perturbations to an image of
a different label. CertiÔ¨Åed defenses can also beneÔ¨Åt from adversarial training. For example, Salman
et al. (2019) recently improved the certiÔ¨Åed radii of randomized smoothing (Cohen et al., 2019) by
training on adversarial examples generated for the smoothed classiÔ¨Åer.
To the best of our knowledge, prior works have focused on making adversarial examples that sat-
isfy the imperceptibility and misclassiÔ¨Åcation conditions, but none have investigated manipulating
certiÔ¨Åcates, which is our focus here.
The reminder of this paper is organized as follows. In section 2 we introduce our new approach
Shadow Attack for generating adversarial perturbations. This is a hybrid model that allows various
kinds of attacks to be compounded together, resulting in perturbations of large radii. In section
3 we present an attack on ‚Äúrandomized smoothing‚Äù certiÔ¨Åcates (Cohen et al., 2019). Section 4
shows an ablation study which illustrates why the elements of the Shadow Attack are important
for successfully manipulating certiÔ¨Åed models. In section 5 we generate adversarial examples for
‚ÄúCROWN-IBP‚Äù (Zhang et al., 2019b). Finally, we discuss results and wrap up in section 6.
2 T HESHADOW ATTACK
Because certiÔ¨Åcate spooÔ¨Ång requires large perturbations (larger than the `pball of the certiÔ¨Åcate),
we propose a simple attack that ensembles numerous modes to create large perturbations. Our attack
can be seen as the generalization of the well-known PGD attack, which creates adversarial images
by modifying a clean base image. Given a loss function Land an`p-norm bound for somep0,
PGD attacks solve the following optimization problem:
max
L(;x+) (1)
s.t.kkp; (2)
whereare the network parameters and is the adversarial perturbation to be added to the clean
input image x. Constraint 2 promotes imperceptibility of the resulting perturbation to the human
eye by limiting the perturbation size. In the shadow attack, instead of solving the above constrained
optimization problem, we solve the following problem with a range of penalties:
max
L(;x+) cC() tvTV() sDissim (); (3)
wherec;tv;sare scalar penalty weights. Penalty TV()forces the perturbation to have small
total variation ( TV), and so appear more smooth and natural. Penalty C()limits the perturbation 
globally by constraining the change in the mean of each color channel c. This constraint is needed
since total variation is invariant to constant/scalar additions to each color channel, and it is desirable
to suppress extreme changes in the color balances of images.
PenaltyDissim ()promotes perturbations that assume similar values in each color channel. In
the case of an RGB image of shape 3WH, ifDissim ()is small, the perturbations to red,
green, and blue channels are similar, i.e., R;w;hG;w;hB;w;h;8(w;h)2WH. This
amounts to making the pixels darker/lighter, without changing the color balance of the image. Later,
in section 3, we suggest two ways of enforcing such similarity between RGB channels and we Ô¨Ånd
both of them effective:
1-channel attack strictly enforces R;iG;iB;i;8iby using just one array to si-
multaneously represent each color channel WH:On the forward pass, we duplicate to
make a 3-channel image. In this case, Dissim () = 0 , and the perturbation is greyscale.
3-channel attack uses a 3-channel perturbation 3WH, along with the dissimilarity met-
ricDissim () =kR Bkp+kR Gkp+kB Gkp.
All together, the three penalties minimize perceptibility of perturbations by forcing them to be (a)
small, (b)smooth, and (c)without dramatic color changes (e.g. swapping blue to red). At the same
time, these penalties allow perturbations that are very large in `p-norm.
32.1 C REATING UNTARGETED ATTACKS
We focus on spooÔ¨Ång certiÔ¨Åcates for untargeted attacks, in which the attacker does not specify the
class into which the attack image moves. To achieve this, we generate an adversarial perturbation
for all possible wrong classes yand choose the best one as our strong attack:
max
y6=y; L(;x+ky) cC() tvTV() sDissim () (4)
whereyis the true label/class for the clean image x, andLis a spooÔ¨Ång loss that promotes a strong
certiÔ¨Åcate. We examine different choices for Lfor different certiÔ¨Åcates below.
3 A TTACKS ON RANDOMIZED SMOOTHING
The Randomized Smoothing method, Ô¨Årst proposed by Lecuyer et al. (2018) and later improved by
Li et al. (2018), is an adversarial defense against `2-norm bounded attacks. Cohen et al. (2019) prove
a tight robustness guarantee under the `2norm for smoothing with Gaussian noise. Their study was
the Ô¨Årst certiÔ¨Åable defense for the ImageNet dataset (Deng et al., 2009). The method constructs
certiÔ¨Åcates by Ô¨Årst creating many copies of an input image contaminated with random Gaussian
noise of standard deviation . Then, it uses a base classiÔ¨Åer (a neural net) to make a prediction for
all of the images in the randomly augmented batch. Depending on the level of the consensus of the
class labels at these random images, a certiÔ¨Åed radius is calculated that can be at most 4(in the
case of perfect consensus).
Intuitively, if the image is far away from the decision boundary, the base classiÔ¨Åer should predict
the same label for each noisy copy of the test image, in which case the certiÔ¨Åcate is strong. On the
other hand, if the image is adjacent to the decision boundary, the base classiÔ¨Åer‚Äôs predictions for the
Gaussian augmented copies may vary. If the variation is large, the smoothed classiÔ¨Åer abstains from
making a prediction.
To spoof strong certiÔ¨Åcates (large certiÔ¨Åed radius) for an incorrect class, we must make sure that
the majority of a batch of noisy images around the adversarial image are assigned the same (wrong)
label. We do this by minimizing the cross entropy loss relative to a chosen (incorrect) label, averaged
over a large set of randomly perturbed images. To this end, we minimize equation 4, where Lis
chosen to be the average cross-entropy over a batch of Gaussian perturbed copies. This method is
analogous to the technique presented by Shafahi et al. (2018) for generating universal perturbations
that are effective when added to a large number of different images.
RESULTS
Cohen et al. (2019) show the performance of the Gaussian smoothed classiÔ¨Åer on CIFAR-10
(Krizhevsky et al.) and ImageNet (Deng et al., 2009). To attack the CIFAR-10 and ImageNet
smoothed classiÔ¨Åers, we use 400randomly sampled Gaussian images, tv= 0:3,c= 1:0, and
perform 300 steps of SGD with learning rate 0:1. We choose the functional regularizers C()and
TV()to be
C() =kAvg(jRj);Avg(jGj);Avg(jBj)k2
2;andTV(i;j) =anisotropic-TV (i;j)2;
wherejjis the element-wise absolute value operator, and Avg computes the average. For the
Dissim regularizer, we experiment with both the 1-Channel attack that ensures Dissim () = 0;
and the 3-Channel attack by setting Dissim () =k(R G)2;(R B)2;(G B)2k2and
s= 0:5. For the validation examples on which the smoothed classiÔ¨Åer does not abstain (see
Cohen et al. (2019) for more details), the less-constrained 3-channel attack is always able to Ô¨Ånd
an adversarial example while the 1-channel attack also performs well, achieving 98.5% success.
2In section 4 we will discuss in more detail other differences between 1-channel and 3-channel
attacks. The results are summarized in Table 1. For the various base-models and choices of , our
adversarial examples are able to produce certiÔ¨Åed radii that are on average larger than the certiÔ¨Åed
radii produced for natural images. For ImageNet, since attacking all 999 remaining target classes
is computationally expensive, we only attacked target class IDs 100, 200, 300, 400, 500, 600, 700,
800, 900, and 1000.
2Source code for all experiments can be found at: https://github.com/AminJun/BreakingCertiÔ¨ÅableDefenses
4Table 1: CertiÔ¨Åed radii produced by the Randomized Smoothing method for Shadow Attack images
and also natural images (larger radii means a stronger/more conÔ¨Ådent certiÔ¨Åcate).
Dataset (l2)UnmodiÔ¨Åed/Natural Images Shadow Attack
Mean STD Mean STD
CIFAR-100.12 0.14 0.056 0.22 0.005
0.25 0.30 0.111 0.35 0.062
0.50 0.47 0.234 0.65 0.14
1.00 0.78 0.556 0.85 0.442
ImageNet0.25 0.30 0.109 0.31 0.109
0.50 0.61 0.217 0.38 0.191
1.00 1.04 0.519 0.64 0.322
(a) Natural image ( x)-
(b) Adversarial perturbation ( )-
(c) Adversarial example ( x+)
Figure 2: An adversarial example built using our Shadow Attack for the smoothed ImageNet classi-
Ô¨Åer for which the certiÔ¨Åable classiÔ¨Åer produces a large certiÔ¨Åed radii. The adversarial perturbation
is smooth and natural looking even-though it is large when measured using `p-metrics. Also see
Figure 16 in the appendix.
Figure 2 depicts a sample adversarial example built for the smoothed ImageNet classiÔ¨Åer that pro-
duces a strong certiÔ¨Åcate. The adversarial perturbation causes the batch of Gaussian augmented
black swan images to get misclassiÔ¨Åed as hooks. For more, see appendix 16.
4 A BLATION STUDY OF THE ATTACK PARAMETERS
In this section we perform an ablation study on the parameters of the Shadow Attack to evaluate
(i)the number of SGD steps needed, (ii)the importance of s(or alternatively using 1-channel
attacks), and (iii)the effect of tv.
The default parameters for all of the experiments are as follows unless explicitly mentioned: We
use30SGD steps with learning rate 0:1for the optimization. All experiments except part (ii)use
1-channel attacks for the sake of simplicity and efÔ¨Åciency (since it has less parameters). We assume
tv= 0:3,c= 20;and use batch-size 50. We present results using the Ô¨Årst example from each
class of the CIFAR-10 validation set.
Figure 3 shows how the adversarial example evolves during the Ô¨Årst few steps of optimization (See
appendix 13 for more examples). Also, Ô¨Ågures 4, 5, and 6 show the evolution of L(),TV(), and
C(), respectively (Note that we use 1-channel attacks, so Dissim ()is always 0). We Ô¨Ånd that
taking just 10 SGD steps is enough for convergence on CIFAR-10, but for our main results (i.e.
attacking Randomized Smoothing in section 3 and attacking CROWN-IBP in section 5) we take 300
steps to be safe.
50 1 2 3 4 5 6 7 8 9 10 original
Figure 3: The Ô¨Årst 10 steps of optimization (beginning with a randomly perturbed image copy).
0.0 0.2 0.4 0.6 0.80.02.55.07.510.012.515.017.520.0 Natural
1-Channel
3-Channel
Figure 9: Histogram of random-
ized smoothed certiÔ¨Åcate radii for
100 randomly sampled CIFAR-10
validation images vs those calcu-
lated for their adversarial examples
crafted using our 1-channel and 3-
channel adversarial Shadow Attack
attacks. The ‚Äúrobust‚Äù victim clas-
siÔ¨Åer is based off Resnet-110, and
smoothed with = 0:50. 1-channel
attacks are almost as good as the
less-restricted 3-channel attacks.
0 2 4 6 8 10050100150200250300mean (Lb(Œ¥))
Figure 4: Average Lb()in
the Ô¨Årst 10steps.
0 2 4 6 8 1002004006008001000mean (TV(Œ¥))
Figure 5: Average TV()in
the Ô¨Årst 10steps.
0 2 4 6 8 100.0250.0500.0750.1000.1250.1500.175mean (C(Œ¥))
Figure 6: Average C()in the
Ô¨Årst10steps.
1 2 3 4 5
Œªs0.10.20.30.40.50.6Dissim (Œ¥)
Figure 7: The effect of son
the resulting Dissim ()
0.00 0.05 0.10 0.15 0.20 0.25 0.30
Œªtv0100200300400500TV(Œ¥)
Figure 8: The effect of tvon
the resulting TV()
To explore the importance of s, we use 3-channel attacks and vary sto produce different images
in Ô¨Ågure 113.
3See Ô¨Ågure 14 in the appendix for more examples.
6natural
1-channel
3-channel
Figure 10: The visual effect of Shadow Attack on 9 randomly selected CIFAR-10 examples using
1-Channel and 3-Channel attacks.
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 original
Figure 11: The visual effect of son perceptibility of the perturbations. The Ô¨Årst row shows the
value ofs.
Also, Ô¨Ågure 7 shows the mean Dissim ()for different values of s(0s5:0). We also
plot the histogram of the certiÔ¨Åcate radii in Ô¨Ågure 9. Figure 10 compares 1-Channel vs 3-Channel
attacks for some of randomly selected CIFAR-10 images. Finally, we explore the effect of tvon
imperceptibility of the perturbations in Figure 12. See table 15 for more images, and Ô¨Ågure 8 for the
impact of parameters on TV().
5 A TTACKS ON CROWN-IBP
Interval Bound Propagation (IBP) methods have been recently studied as a defense against `1-
bounded attacks. Many recent studies such as Gowal et al. (2018); Xiao et al. (2018); Wong et al.
(2018); Mirman et al. (2018) have investigated IBP methods to train provably robust networks. To
the best of our knowledge, the CROWN-IBP method by Zhang et al. (2019b) achieves state-of-
the-art performance for MNIST (LeCun & Cortes, 2010), Fashion-MNIST (Xiao et al., 2017), and
CIFAR-10 datasets among certiÔ¨Åable `1defenses. In this section we focus on attacking Zhang et al.
(2019b) using CIFAR-10.
IBP methods (over)estimate how much a small `1-bounded noise in the input can impact the clas-
siÔ¨Åcation layer. This is done by propagating errors from layer to layer, computing bounds on the
maximum possible perturbation to each activation. During testing, the user chooses an `1per-
turbation bound ;and error propagation is used to bound the magnitude of the largest achievable
perturbation in network output. If the output perturbation is not large enough to Ô¨Çip the image la-
bel, then a certiÔ¨Åcate is produced. If the output perturbation is large enough to Ô¨Çip the image label,
0.0 0.03 0.06 0.09 0.12 0.15 0.18 0.21 0.24 0.27 0.30 original
Figure 12: The visual effect of tvon the on imperceptibility of the perturbations. The Ô¨Årst row
shows the value of tv
7Table 2: ‚ÄúRobust error‚Äù for natural images, and ‚Äúattack error‚Äù for Shadow Attack images using the
CIFAR-10 dataset, and CROWN-IBP models. Smaller is better.
(l1) Model Family MethodRobustness Errors
Min Mean Max
2=2559 small modelsCROWN-IPB 52.46 57.55 60.67
Shadow Attack 45.90 53.89 65.74
8 large modelsCROWN-IBP 52.52 53.9 56.05
Shadow Attack 46.21 49.77 51.79
8=2559 small modelsCROWN-IBP 71.28 72.15 73.66
Shadow Attack 63.43 66.94 71.02
8 large modelsCROWN-IBP 70.79 71.17 72.29
Shadow Attack 64.04 67.32 71.16
then a certiÔ¨Åcate is not produced. During network training, IBP methods include a term in the loss
function that promotes tight error bounds that result in certiÔ¨Åcates. Our attack directly uses the
loss function term used during IBP network training in addition to the Shadow Attack penalties;
we search for an image that is visually similar to the base image, while producing a bound on the
output perturbation that is too small to Ô¨Çip the image label. Note that there is a subtle difference
between crafting conventional adversarial examples which ultimately targets misclassiÔ¨Åcation and
our adversarial examples which aim to produce adversarial examples which cause misclassiÔ¨Åcation
andproduce strong certiÔ¨Åcates. In the former case, we only need to attack the cross-entropy loss. If
we use our Shadow Attack to craft adversarial examples based on the cross-entropy loss, the robust-
ness errors are on average roughly 50% larger than those reported in table 2 (i.e., simple adversarial
examples produce weaker certiÔ¨Åcates.)
We attack 4 classes of networks released by Zhang et al. (2019b) for CIFAR-10. There are two
classes of IBP architectures, one of them consists of 9 small models and the other consists of 8
larger models. For each class of architecture, there are two sets of pre-trained models: one for
= 2=255and one for = 8=255. We usetv= 0:000009 ,c= 0:02,C() =kk2and set the
learning rate to 200and for the rest of the regularizers and hyper-paramters we use the same hyper-
parameters and regularizers as in 3. For the sake of efÔ¨Åciency, we only do 1-channel attacks. We
attack the 4 classes of models and for each class, we report the min, mean, and max of the robustness
errors and compare them with those of the CROWN-IBP paper.
To quantify the success of our attack, we deÔ¨Åne two metrics of error. For natural images, we report
the rate of ‚Äúrobust errors,‚Äù which are images that are either (i) incorrectly labeled, or (ii) correctly
labelled but without a certiÔ¨Åcate. In contrast, for attack images, we report the rate of ‚Äúattack errors,‚Äù
which are either (i) correctly classiÔ¨Åed or (ii) incorrectly classiÔ¨Åed but without a certiÔ¨Åcate. Table 2
shows the robust error on natural images, and the attack error on Shadow Attack images for the
CROWN-IBP models. With = 2=255, our attack Ô¨Ånds adversarial examples that certify roughly
15% of the time (i.e., attack error <85%). With = 8=255, our attack Ô¨Ånds adversarial examples
that are incorrectly classiÔ¨Åed, and yet certify even more often than natural images.
6 CONCLUSION
We demonstrate that it is possible to produce adversarial examples with ‚Äúspoofed‚Äù certiÔ¨Åed ro-
bustness by using large-norm perturbations. Our adversarial examples are built using our Shadow
Attack that produces smooth and natural looking perturbations that are often less perceptible than
those of the commonly used `p-bounded perturbations, while being large enough in norm to escape
the certiÔ¨Åcation regions of state of the art principled defenses. This work suggests that the certiÔ¨Å-
cates produced by certiÔ¨Åably robust classiÔ¨Åers, while mathematically rigorous, are not always good
indicators of robustness or accuracy.
Acknowledgements: Goldstein and his students were supported by the DARPA QED for RML
program, the DARPA GARD program, and the National Science Foundation.
8REFERENCES
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420 ,
2018.
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim ÀáSrndi ¬¥c, Pavel Laskov, Gior-
gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint
European conference on machine learning and knowledge discovery in databases , pp. 387‚Äì402.
Springer, 2013.
Tom B Brown, Nicholas Carlini, Chiyuan Zhang, Catherine Olsson, Paul Christiano, and Ian Good-
fellow. Unrestricted adversarial examples. arXiv preprint arXiv:1809.08352 , 2018.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. In Proceedings of the 10th ACM Workshop on ArtiÔ¨Åcial Intelligence and
Security , pp. 3‚Äì14. ACM, 2017.
Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. CertiÔ¨Åed adversarial robustness via randomized
smoothing. arXiv preprint arXiv:1902.02918 , 2019.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09 , 2009.
Isaac Dunn, Tom Melham, and Daniel Kroening. Generating realistic unrestricted adversarial inputs
using dual-objective gan training. arXiv preprint arXiv:1905.02463 , 2019.
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A
rotation and a translation sufÔ¨Åce: Fooling cnns with simple transformations. arXiv preprint
arXiv:1712.02779 , 2017.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for
training veriÔ¨Åably robust models. arXiv preprint arXiv:1810.12715 , 2018.
Hossein Hosseini and Radha Poovendran. Semantic adversarial examples. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition Workshops , pp. 1614‚Äì1619, 2018.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-
search). URL http://www.cs.toronto.edu/ Àúkriz/cifar.html .
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533 , 2016.
Cassidy Laidlaw and Soheil Feizi. Functional adversarial attacks. arXiv preprint arXiv:1906.00001 ,
2019.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/ .
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. CertiÔ¨Åed
robustness to adversarial examples with differential privacy. arXiv preprint arXiv:1802.03471 ,
2018.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Second-order adversarial attack
and certiÔ¨Åable robustness. CoRR , abs/1809.03113, 2018. URL http://arxiv.org/abs/
1809.03113 .
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 ,
2017.
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning , pp. 3575‚Äì3583,
2018.
9Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition , pp. 2574‚Äì2582, 2016.
Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien
Bubeck. Provably robust deep learning via adversarially trained smoothed classiÔ¨Åers. arXiv
preprint arXiv:1906.04584 , 2019.
Ali Shafahi, Mahyar Najibi, Zheng Xu, John Dickerson, Larry S Davis, and Tom Goldstein. Uni-
versal adversarial training. arXiv preprint arXiv:1811.11304 , 2018.
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S
Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! arXiv preprint
arXiv:1904.12843 , 2019.
Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Constructing unrestricted adversarial
examples with generative models. In Advances in Neural Information Processing Systems , pp.
8312‚Äì8323, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 , 2013.
Jonathan Uesato, Brendan O‚ÄôDonoghue, Aaron van den Oord, and Pushmeet Kohli. Adversarial risk
and the dangers of evaluating against weak attacks. arXiv preprint arXiv:1802.05666 , 2018.
Rey Wiyatno and Anqi Xu. Maximal jacobian-based saliency map attack. arXiv preprint
arXiv:1808.07945 , 2018.
Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial
defenses. In Advances in Neural Information Processing Systems , pp. 8400‚Äì8409, 2018.
Eric Wong, Frank R Schmidt, and J Zico Kolter. Wasserstein adversarial examples via projected
sinkhorn iterations. arXiv preprint arXiv:1902.07906 , 2019.
Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
Kai Y Xiao, Vincent Tjeng, Nur Muhammad ShaÔ¨Åullah, and Aleksander Madry. Training for faster
adversarial robustness veriÔ¨Åcation via inducing relu stability. arXiv preprint arXiv:1809.03008 ,
2018.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I
Jordan. Theoretically principled trade-off between robustness and accuracy. arXiv preprint
arXiv:1901.08573 , 2019a.
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Towards sta-
ble and efÔ¨Åcient training of veriÔ¨Åably robust neural networks. arXiv preprint arXiv:1906.06316 ,
2019b.
10A A PPENDIX
In this section, we include the complete results of our ablation study. As we mentioned in section
4, we use is a subset of CIFAR-10 dataset, including one example per each class. For the sake of
simplicity, we call the dataset Tiny-CIFAR-10. Here, we show the complete results for the ablation
experiments on all of Tiny-CIFAR-10 examples. Figure 13 shows that taking a few optimization
steps is enough for the resulting images to look natural-looking. Figure 14 and 15, respectively
show the effect of sandTVon the imperceptability of the perturbations.
110 1 2 3 4 5 6 7 8 9 10 original
Figure 13: The Ô¨Årst 10 steps of the optimization vs the original image for Tiny-CIFAR-10. See
section 4 for the details of the experiments.
120.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 original
Figure 14: The visual effect of sonDissim ()on Tiny-CIFAR-10. See section 4 for the details
of the experiments.
130.0 0.03 0.06 0.09 0.12 0.15 0.18 0.21 0.24 0.27 0.30 original
Figure 15: The visual effect of tvon the perturbation Tiny-CIFAR-10. See section 4 for the details
of the experiments.
14Table 3: CertiÔ¨Åed radii statistics produced by the Adversarially Trained Randomized Smoothing
method for our adversarial examples crafted using Shadow Attack and the natural examples (larger
radii are better).
Dataset (l2)Adversarially Trained Randomized Smoothed Shadow Attack
Mean STD Mean STD
CIFAR-10 0.5 0.60 0.34 0.65 0.16
IMAGE NETRESULTS
Many of the recent studies have explored the semantic attacks. Semantic attacks are powerful for
attacking defenses (Engstrom et al., 2017; Hosseini & Poovendran, 2018; Laidlaw & Feizi, 2019).
Many of semantic attacks are applicable to Imagenet, however, none of them consider increasing the
radii of the certiÔ¨Åcates generated by the certiÔ¨Åable defenses.
Some other works focus on using generative models to generate adversarial examples (Song et al.,
2018), but unfortunately none of the GAN‚Äôs are expressive enough to capture the manifold of the
ImageNet.
Figure 16 illustrates some of our successful examples generated by Shadow Attack to attack Ran-
domized Smoothed classiÔ¨Åers for ImageNet.
B C ERTIFICATE SPOOFING ATTACKS ON ADVERSARIALLY TRAINED SMOOTH
CLASSIFIERS
Recently, Salman et al. (2019) signiÔ¨Åcantly improved the robustness of Gaussian smoothed classi-
Ô¨Åers by generating adversarial examples for the smoothed classiÔ¨Åer and training on them. In this
section, we use our Shadow Attack to generate adversarial examples using the loss in eq. 4 for the
SmoothAdv classiÔ¨Åer4. Due to computation limitations, we attack a sample of 40% of the same
validation images used for evaluating the randomized smooth classiÔ¨Åer in section 3. The results are
summarized in table 3. Comparing the results of table 1 to table 3, we can see that the SmoothAdv
classiÔ¨Åer, does produce stronger certiÔ¨Åed radii for natural examples (many of the examples in fact
have the maximum radii) compared to the original randomized smoothing classiÔ¨Åer. This can be
associate to the excessive invariance introduced as a result of adversarial training. However, table 3
empirically veriÔ¨Åes that its certiÔ¨Åcates are still subject to attacks and the certiÔ¨Åcate should not be
used as a measure for robustness.
4We use the ofÔ¨Åcial models released at https://github.com/Hadisalman/smoothing-adversarial for attacking
the SmoothAdv classiÔ¨Åer.
15Figure 16: Natural looking Imperceptible ImageNet adversarial images which produce large certi-
Ô¨Åed radii for the ImageNet Gaussian smoothed classiÔ¨Åer.
16