Query-efÔ¨Åcient Black-box Adversarial Examples
Andrew Ilyas12, Logan Engstrom12, Anish Athalye12, Jessy Lin12
1Massachusetts Institute of Technology,2LabSix
failyas,engstrom,aathalye,lnj g@mit.edu
Abstract
Current neural network-based image classiÔ¨Åers are sus-
ceptible to adversarial examples, even in the black-box set-
ting, where the attacker is limited to query access with-
out access to gradients. Previous methods ‚Äî substitute
networks and coordinate-based Ô¨Ånite-difference methods
‚Äî are either unreliable or query-inefÔ¨Åcient, making these
methods impractical for certain problems.
We introduce a new method for reliably generating ad-
versarial examples under more restricted, practical black-
box threat models. First, we apply natural evolution strate-
gies to perform black-box attacks using two to three orders
of magnitude fewer queries than previous methods. Sec-
ond, we introduce a new algorithm to perform targeted ad-
versarial attacks in the partial-information setting, where
the attacker only has access to a limited number of target
classes. Using these techniques, we successfully perform
the Ô¨Årst targeted adversarial attack against a commercially
deployed machine learning system, the Google Cloud Vision
API, in the partial information setting.
1. Introduction
Neural network-based image classiÔ¨Åers, despite surpass-
ing human ability on several benchmark vision tasks, are
susceptible to adversarial examples . Adversarial examples
are correctly classiÔ¨Åed images that are minutely perturbed
to cause misclassiÔ¨Åcation. Targeted adversarial examples
cause misclassiÔ¨Åcation as a chosen class, while untargeted
adversarial examples cause just misclassiÔ¨Åcation.
The existence of these adversarial examples and the fea-
sibility of constructing them in the real world [9, 1] points to
potential exploitation, particularly in the face of the rising
popularity of neural networks in real-world systems. For
commercial or proprietary systems, however, adversarial
examples must be considered under a much more restrictive
threat model. First, these settings are black-box , meaning
that an attacker only has access to input-output pairs of the
Equal contributionclassiÔ¨Åer, often through a binary or API. Furthermore, often
the attacker will only have access to a subset of the classi-
Ô¨Åcation outputs (for example, the top klabels and scores);
to our knowledge this setting, which we denote the partial-
information setting, has not been considered in prior work.
Prior work considering constrained threat models have
only considered the black-box restriction we describe
above; previous work primarily uses substitute networks to
emulate the attacked network, and then attack the substi-
tute with traditional Ô¨Årst-order white-box methods [13, 14].
However, as discussed thoroughly in [4], this approach is
unfavorable for many reasons including imperfect transfer-
ability of attacks from the substitute to the original model,
and the computational and query-wise cost of training a sub-
stitute network. Recent attacks such as [4] have used Ô¨Å-
nite difference methods in order to estimate gradients in the
black-box case, but are still expensive, requiring millions of
queries to generate an adversarial image for an ImageNet
classiÔ¨Åer. Effects such as low throughput, high latency, and
rate limiting on commercially deployed black-box classi-
Ô¨Åers heavily impact the feasibility of current approaches to
black-box attacks on real-world systems.
We present an approach for generating black-box ad-
versarial examples based on Natural Evolutionary Strate-
gies [18]. We provide motivation for the algorithm in terms
of Ô¨Ånite difference estimation in random Gaussian bases.
We demonstrate the effectiveness of the method in prac-
tice, generating adversarial examples with several orders
of magnitude fewer queries compared to existing methods.
We consider the further constrained partial-information set-
ting, and we present a new algorithm for attacking neural
networks under these conditions. We demonstrate the ef-
fectiveness of our method by showing that it can reliably
produce targeted adversarial examples with access to par-
tial input-output pairs.
We use the newfound tractability given by these methods
to both (a) generate the Ô¨Årst transformation-tolerant black-
box adversarial examples and (b) perform the Ô¨Årst targeted
attack on the Google Cloud Vision API, demonstrating the
effectiveness of our proposed method on large, commercial
systems: the GCV API is an opaque (no published enumera-
1arXiv:1712.07113v2 [cs.CV] 6 Apr 2018tion of labels), partial-information (queries return only up to
10 classes with uninterpretable ‚Äúscores‚Äù), several-thousand-
way commercially deployed classiÔ¨Åer.
Our contributions are as follows:
We propose a variant of NES inspired by the treate-
ment in [18] as a method for generating black-box ad-
versarial examples. We relate NES in this special case
with the Ô¨Ånite difference method over Gaussian bases,
providing a theoretical comparison with previous at-
tempts at black-box adversarial examples.
We demonstrate that our method is effective in efÔ¨Å-
ciently synthesizing adversarial examples; the method
does not require a substitute network and is 2-3 orders
of magnitude faster than optimized Ô¨Ånite difference-
based methods such as [4]. We reliably produce black-
box adversarial examples for both CIFAR-10 and Im-
ageNet classiÔ¨Åers.
We propose an approach for synthesizing targeted ad-
versarial examples in the ‚Äúpartial information‚Äù setting,
where the attacker has access only to top- koutputs of
a classiÔ¨Åer, and we demonstrate its effectiveness.
We exploit the increased efÔ¨Åciency of this method to
achieve the following results:
‚Äì Robust black-box examples. In [1], the inabil-
ity of standard-generated adversarial examples to
remain adversarial under transformation is noted,
and the Expectation over Transformation (EOT)
algorithm is introduced. By integrating EOT with
the method presented in this work, we generate
the Ô¨Årst transformation-tolerant black-box adver-
sarial examples.
‚Äì Targeted adversarial examples against a
several-thousand-way commercial classiÔ¨Åer.
We use our method to generate adversarial
examples for the Google Cloud Vision API,
a commercially-deployed system. An attack
against a commercial classiÔ¨Åer of this order of
magnitude demonstrates the applicability and re-
liability of our method.
2. Approach
We outline the key technical components of our ap-
proach allowing us to attack the constructed threat model.
First we describe our application of Natural Evolutionary
Strategies [18]. Then, we outline the strategy used to con-
struct adversarial examples in the partial-information set-
ting.2.1. Natural Evolutionary Strategies
Rather than taking component-wise Ô¨Ånite differences as
in previous state-of-the art methods [4], we use natural evo-
lutionary strategies [18]. Natural evolutionary strategies
(NES) is a method for derivative-free optimisation based on
the idea of a search distribution (jx). In particular, rather
than maximizing the objective function F(x)directly, NES
maximizes the expected value of the loss function under the
search distribution. As demonstrated in Section 4, this al-
lows for gradient estimation in far fewer queries than typical
Ô¨Ånite-difference methods. Concretely, for a loss function
F()and a current set of parameters x, we have from [18]:
E(jx)[F()] =Z
F()(jx) d
rxE(jx)[F()] =rxZ
F()(jx) d
=Z
F()rx(jx) d
=Z
F()(jx)
(jx)rx(jx) d
=Z
(jx)F()rxlog ((jx)) d
=E(jx)[F()rxlog ((jx))]
In a manner similar to that in [18], we choose a search
distribution of random Gaussian noise around the current
imagex; that is, we have =x+N(0;I). Evaluating
the gradient above with this search distribution yields the
following variance-reduced gradient estimate:
rE[F()]1
nnX
i=1iF(+i)
Similarly to [15], we employ antithetic sampling to gen-
erate batches of ivalues; rather than generating nval-
uesi N (0;1), we instead draw these values for i2
f1:::n
2g, and setj= n j+1forj2f(n
2+ 1):::ng.
This optimization has been empirically shown to improve
performance of NES.
Finally, we perform a projected gradient descent update
[11] with momentum based on the NES gradient estimate.
2.1.1 NES as Finite Differences
A closer inspection of the special case of NES that we have
described here suggests an alternative view of the algorithm.
In particular, note that when antithetic sampling is used, the
gradient estimate can be written as the following, where Dv
represents the directional derivative in the direction of v:rE[F(x)]1
nnX
i=1F(x+i)i
=1
n=2n=2X
i=1F(x+i) F(x i)
2i
1
n=2n=2X
i=1Di(x)i
=1
n=2n=2X
i=1(rFi)i
Now, theiare effectively randomly drawn Gaussian
vectors of size widthheightchannels . By a well-known
result, these vectors are nearly orthogonal; a formalization
of this is in [7], which says that for an n-dimensional space
andNrandomly sampled Gaussian vectors v1:::vN,
Ne2n
4[ ln()]1
2=)Pvivj
jjvijjjjvjjj8(i;j)
=
Thus, one can ‚Äúextend‚Äù the randomly sampled vectors
into a complete basis of the space [0;1]n; then we can per-
form a basis decomposition on rF(x)to write:
rF(x) =nX
i=1hrF;iii
Thus, the NES gradient can be seen as essentially ‚Äúclip-
ping‚Äù this space to the Ô¨Årst NGaussian basis vectors and
performing a Ô¨Ånite-differences estimate.
More concretely, considering a matrix withibeing
the columns and the projection (rF), we can use results
from concentration theory to analyze our estimate, either
through the following simple canonical bound or a more
complex treatment such as is given in [5]:
P
(1 )jjrjj2jjrjj2(1 +)jjrjj2 
1 2e c2m
Note that even more rigorous analyses of such
‚ÄúGaussian-projected Ô¨Ånite difference‚Äù gradient estimates
and bounds have been demonstrated by works such as [12],
which detail the algorithm‚Äôs interaction with dimensional-
ity, scaling, and various other factors.
2.2. Partial-Information Setting
Next, we consider the partial-information setting de-
scribed in the previous section. In particular, we now as-
sume access to both probabilities and gradient approxima-
tions through the methods described in Section 2.1, but onlyfor the topkclassesfy1;:::;ykg. In normal settings, given
an image and label (xi;y), generating an adversarial exam-
ple(xadv;yadv)for a targeted yadvcan be acheived using
standard Ô¨Årst-order attacks. These are attacks which involve
essentially ascending the estimated gradient rP(yadvjx).
However, in this case P(yadvjxi)(and by extension, its gra-
dient) is unavailable to the classiÔ¨Åer.
To resolve this, we propose the following algorithm.
Rather than beginning with the image xi, we instead be-
gin with an image x0of the original target class . Thenyadv
will be in the top- kclasses forx0. We perform the follow-
ing iterated optimization:
t= mins.t. rank (P(yadvj(xt 1)))<k
xt= arg max
xP(yadvjt 1(x))
where (x)represents the `1projection of xonto the
-box ofxi. In particular, we concurrently perturb the im-
age to maximize its adversarial probability, while project-
ing onto`1boxes of decreasing sizes centered at the origi-
nal imagexi, maintaining that the adversarial class remains
within the top- kat all times. In practice, we implement this
iterated optimization using backtracking line search to Ô¨Ånd
t, and several iterations projected gradient descent (PGD)
to Ô¨Åndxt. Alternatingly updating xanduntilreaches the
desired value yields an adversarial example that is -away
fromxiwhile maintaining the adversarial classiÔ¨Åcation of
the original image.
3. Threat model
Our threat model is chosen to model the constraints of
attacking deep neural networks deployed in the real world.
No access to gradients, logits, or other internals.
Similar to previous work, we deÔ¨Åne black-box to mean
that access to gradients, logits, and other network in-
ternals is unavailable. Furthermore, the attacker does
not have knowledge of the network architecture. The
attacker only has access to the output of the classiÔ¨Åer:
prediction probabilities for each class.
No access to training-time information. The attacker
has no information about how the model was trained,
and the attacker does not have access to the training
set.
Limited number of queries. In real-time models like
self-driving cars, the format of the input allows us to
make a large number of queries to the network (e.g.
by disassembling the car, overriding the input signals,
and measuring the output signals). In most other cases,
proprietary ML models like the Google Cloud Vision
API are rate-limited or simply unable to support a largenumber of queries to generate a single adversarial ex-
ample.
Partial-information setting: As discussed in Sec-
tion 1, we also consider in this work the case where
the full output of the classiÔ¨Åer is unavailable to the at-
tacker. This more accurately reÔ¨Çects the state of com-
mercial systems where even the list of possible classes
is unknown to the attacker, such as in the Google Cloud
Vision API, Amazon‚Äôs Rekognition API, or the Clari-
fai API.
Attackers can have one of two goals: untargeted or tar-
geted misclassiÔ¨Åcation, where targeted attacks are strictly
harder. A successful targeted adversarial example is one
that is classiÔ¨Åed as a speciÔ¨Åc target class. An untargeted
adversarial example is one that is misclassiÔ¨Åed.
Notably, we omit wall-clock time to attack as a security
parameter in our threat model. This metric is more indica-
tive of hardware resources used for the attack than the efÔ¨Å-
cacy of an attack itself, for which query count is a realistic
and practical measure.
4. Evaluation
4.1. Targeted black-box adversarial examples
We evaluate the effectiveness of our black-box attack
in generating targeted adversarial examples for neural net-
works trained on CIFAR-10 and ImageNet. We demonstrate
our attack against the CIFAR-10 network of Carlini and
Wagner [3] and the InceptionV3 network [16] in the black-
box setting, assuming access to the output probabilities of
the classiÔ¨Åers. For each of the classiÔ¨Åers, we randomly
choose 1000 examples from the test set, and for each exam-
ple, we choose a random target class. We then use projected
gradient descent (PGD) [11] with NES gradient estimates,
maximizing the log probability of the target class while con-
straining to a maximum `1perturbation of = 0:05. We
use a Ô¨Åxed set of hyperparameters across all attacks on a
single classiÔ¨Åer, and we run the attack until we produce an
adversarial image or until we time out (at a maximum of 1
million queries).
Table 1 summarizes the results of our experiment. Our
attack is highly effective and query-efÔ¨Åcient, with a 99.6%
success rate on CIFAR-10 with a mean of 4910 queries to
the black-box classiÔ¨Åer per example, and a 99.2% success
rate on ImageNet with a mean of 24780 queries to the black-
box classiÔ¨Åer per example. Figures 1 and 2 show a sample
of the adversarial examples we produced. Figures 3 and 4
show the distribution of number of queries required to pro-
duce an adversarial example: in most cases, the attack re-
quires only a small number of queries.
Figure 1. Randomly chosen samples from the 1000 adversarial
examples for the CIFAR-10 network.
Figure 2. Randomly chosen samples from the 1000 adversarial
examples for the InceptionV3 network.
0 2000 4000 6000 8000 10000
Queries Required050100150200250300350
Figure 3. Distribution of number of queries required to generate
an adversarial image with a randomly chosen target class for the
CIFAR-10 network over the 1000 test images.
4.2. Robust black-box adversarial examples
We evaluate the effectiveness of our black-box attack in
generating adversarial examples that fool classiÔ¨Åers over
a distribution of transformations using Expectation-Over-
Transformation (EOT) [1]. In this task, given a distribution
of transformations Tand`1constraint, we attempt to Ô¨Ånd
the adversarial example x0(for some original input x) thatDataset Original Top-1 Accuracy Attack Success Rate Mean Queries
CIFAR-10 80.5% 99.6% 4910
ImageNet 77.2% 99.2% 24780
Table 1. Quantitative analysis of targeted adversarial attacks we perform on 1000 randomly chosen test images and randomly chosen target
classes. The attacks are limited to 1 million queries per image, and the adversarial perturbations are constrained with l1;=:05. The
same hyperparameters were used for all images in each dataset.
0 50000 100000 150000 200000
Queries Required050100150200250300350
Figure 4. Distribution of number of queries required to generate
an adversarial image with a randomly chosen target class for the
InceptionV3 ImageNet network over the 1000 test images.
maximizes the classiÔ¨Åer‚Äôs expected output probability of a
target classyover the distribution of inputs T(x0):
x0= arg max
x0;kx x0k1EtT[logP(yjt(x0))]
We use the PGD attack of [11] to solve the EOT optimiza-
tion problem, using NES to estimate the gradient of the clas-
siÔ¨Åer. Note that P(yj)is the classiÔ¨Åer‚Äôs output probability
for labelygiven an input. In our evaluation we randomly
choose 10 examples from the ImageNet validation set, and
for each example we randomly choose a target class. We
choose our distribution of transformations to be a degree
rotation forunif( 30;30), and set= 0:1. We use a
Ô¨Åxed set of hyperparameters across all attacks, and perform
the PGD attack until we achieve greater than 90% adversar-
iality on a random sample of 100 transformations.
Table 2 shows the results of our experiment. We achieve
a mean attack success rate (where attack success rate is
deÔ¨Åned for a single adversarial example as the percentage
of randomly transformed samples that classify as the tar-
get class) of 95.7% on our 10 attacks, and use a mean of
3780000 queries per example. Figure 5 shows samples of
the adversarial examples robust up to 30.4.3. Targeted partial-information adversarial ex-
amples
We evaluate the effectiveness of our partial-information
black-box attack in generating targeted adversarial exam-
ples for the InceptionV3 network when given access to only
the top 10 class probabilities out of the total of 1000 labels.
We randomly choose 1000 examples from the test set, and
for each example, we choose a random target class. For
each source-target pair, we Ô¨Ånd an example of the target
class in the test set, initialize with that image, and use our
partial-information attack to construct a targeted adversarial
example. We use PGD with NES gradient estimates, con-
straining to a maximum `1perturbation of = 0:05. We
use a Ô¨Åxed set of hyperparameters across all attacks, and we
run the attack until we produce an adversarial example or
until we time out (at a maximum of 1 million queries).
The targeted partial-information attack achieves a 95.5%
success rate with a mean of 104342 queries to the black-box
classiÔ¨Åer.
4.4. Attacking Google Cloud Vision
In order to demonstrate the relevance and applicability
of our approach to real-world system, we attack the Google
Cloud Vision API, a commercially available computer vi-
sion suite offered by Google. In particular, we attack the
most general object labeling classiÔ¨Åer, which performs n-
way classiÔ¨Åcation on any given image. This case is consid-
erably more challenging than even the typical black-box set-
ting. The number of classes is large and unknown ‚Äî a full
enumeration of labels is unavailable. The classiÔ¨Åer returns
‚ÄúconÔ¨Ådence scores‚Äù for each label it assigns to an image,
which seem to be neither probabilities nor logits. The clas-
siÔ¨Åer does not return scores for all labels, but instead returns
an unspeciÔ¨Åed-length list of labels that varies based on im-
age. Despite these challenges, we successfully demonstrate
the ability of the system to generate black-box adversarial
examples, in both an untargeted attack and a targeted attack.
4.4.1 Untargeted attack
Figure 6 shows an unperturbed image being correctly
labeled as several riÔ¨Çe/riÔ¨Çe-related classes, including
‚Äúweapon‚Äù and ‚ÄúÔ¨Årearm.‚Äù We run the algorithm presented
in this work, but rather than maximizing the probability of
a target class, we write the following loss function basedOriginal Top-1 Accuracy Mean Attack Success Rate Mean Queries
80.0% 95.7% 3780000
Table 2. Mean attack success rate and mean required queries across the 10 generated adversarial examples on InceptionV3 robust up to
30rotations.
Original: hard disc
P(true): 100%
P(adv): 0%
P(true): 100%
P(adv): 0%
P(true): 100%
P(adv): 0%
P(true): 100%
P(adv): 0%
Adversarial: bearskin
P(true): 0%
P(adv): 70%
P(true): 3%
P(adv): 45%
P(true): 1%
P(adv): 41%
P(true): 1%
P(adv): 62%
Figure 5. Random sample of adversarial examples robust up to 30rotations along with classiÔ¨Åer probabilities of the adversarial and
natural classes.
on the classiÔ¨Åcation C(x)to minimize the maximum score
assigned to any label semantically similar to ‚Äúgun‚Äù:
F(x) = max
C(x)[‚Äúlabel‚Äù]\gun00C(x)[‚Äúscore‚Äù]
Note that we expand the deÔ¨Ånition of ‚ÄúmisclassiÔ¨Åcation‚Äù
to encompass semantic similarity‚Äîthat is, we are uninter-
ested in a modiÔ¨Åcation that induces a classiÔ¨Åcation of ‚Äúper-
sian cat‚Äù on a ‚Äútabby cat.‚Äù Applying the presented algorithm
to this loss function with = 0:1yields the adversarial ex-
ample shown in Figure 7, deÔ¨Ånitively demonstrating the ap-
plicability of our method to real-world commercial systems.
4.4.2 Targeted attack
Figure 8 shows an unperturbed image being correctly la-
beled as several skiing-related classes, including ‚Äúskiing‚Äù
and ‚Äúski‚Äù. We run our partial-information attack to force
this image to be classiÔ¨Åed as ‚Äúdog‚Äù. Note that the label
‚Äúdog‚Äù does not appear in the output for the unperturbed im-
age. We initialize the algorithm with a photograph of a dog
(classiÔ¨Åed by GCV as a dog) and use our partial-information
attack to synthesize an image that looks like the skiers but
is classiÔ¨Åed as ‚Äúdog‚Äù.5. Related work
Szegedy et al. (2014) [17] Ô¨Årst demonstrated that neural
networks are vulnerable to adversarial examples. A number
of techniques have been developed to generate adversarial
examples in the white-box case [6, 3, 9], where an attacker
is assumed to have full access to the model parameters and
architecture.
Previous work has shown that adversarial examples can
be generated in the black-box case by training a substi-
tute model, and then exploiting white-box techniques on
the substitute [13, 14]. However, this attack is unreliable
because it assumes the adversarial examples can transfer to
the target model. At best, the adversarial images are less ef-
fective, and in some cases, attacks may entirely fail to trans-
fer and ensembles of substitute models need to be used [10].
Our attack does not require the transferability assumption.
Recent attempts use Ô¨Ånite differences to estimate the gra-
dient instead of training substitute models [4]. However,
even with various query reduction techniques, a large num-
ber of queries are required to generate a single attack image,
potentially rendering real-world attacks and transformation-
tolerant adversarial examples intractable. In comparison,
our method uses several orders of magnitude fewer queries.
Prior work has demonstrated that black-box methods
can feasibly attack real-world, commercially deployed sys-
tems, including image classiÔ¨Åcation APIs from Clarifai,Figure 6. The Google Cloud Vision Demo labelling on the unper-
turbed image.
Figure 7. The Google Cloud Vision Demo labelling on the ad-
versarial image generated with `1bounded perturbation with
= 0:1: the original class is no longer a returned label.
Metamind, Google, and Amazon [10, 14, 8], and a speech
recognition system from Google [2]. Our work advances
prior work on machine learning systems deployed in the
real world by demonstrating a highly effective and query-
efÔ¨Åcient attack against the Google Cloud Vision API in the
partial-information setting, a scenario that has not been ex-
plored in prior work.
6. Conclusion
In this work, we present an algorithm based on natural
evolutionary strategies (NES) which allows for the genera-
tion of adversarial examples in the black-box setting with-
out training a substitute network. We also introduce the
partial-information setting, a more restricted black-box sit-
uation that better models large-scale commercial systems,
and we present an algorithm for crafting targeted adversar-
ial examples for this setting. We motivate our algorithm
through the formulation of NES as a set of Ô¨Ånite differences
over a random normal projection, and demonstrate the em-
pirical efÔ¨Åcacy of the method by generating black-box ad-
versarial examples orders of magnitude more efÔ¨Åcient (in
terms of number of queries) than previous work on both the
CIFAR-10 and ImageNet datasets. Using a combination of
the described algorithm and the EOT algorithm, we gener-
Figure 8. The Google Cloud Vision Demo labelling on the unper-
turbed image.
Figure 9. The Google Cloud Vision Demo labelling on the ad-
versarial image generated with `1bounded perturbation with
= 0:1: the image is labeled as the target class.
ate the Ô¨Årst robust black-box adversarial examples, which
constitutes a step towards attacking real-world systems. We
also demonstrate the efÔ¨Åcacy of our partial-information at-
tack. Finally, we synthesize targeted adversarial examples
for the commercial Google Cloud Vision API, demonstrat-
ing the Ô¨Årst targeted attack against a partial-information sys-
tem. Our results point to a promising new method for efÔ¨Å-
ciently and reliably generating black-box adversarial exam-
ples.
Acknowledgements
Special thanks to Nat Friedman and Daniel Gross.
References
[1] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok. Synthesiz-
ing robust adversarial examples. 2017.
[2] N. Carlini, P. Mishra, T. Vaidya, Y . Zhang, M. Sherr,
C. Shields, D. Wagner, and W. Zhou. Hidden voice com-
mands. In 25th USENIX Security Symposium (USENIX Se-
curity 16), Austin, TX , 2016.
[3] N. Carlini and D. Wagner. Towards evaluating the robust-
ness of neural networks. In IEEE Symposium on Security &
Privacy , 2017.
[4] P.-Y . Chen, H. Zhang, Y . Sharma, J. Yi, and C.-J. Hsieh. Zoo:
Zeroth order optimization based black-box attacks to deepneural networks without training substitute models. In Pro-
ceedings of the 10th ACM Workshop on ArtiÔ¨Åcial Intelligence
and Security , AISec ‚Äô17, pages 15‚Äì26, New York, NY , USA,
2017. ACM.
[5] S. Dasgupta, D. Hsu, and N. Verma. A concentration theo-
rem for projections. In Conference on Uncertainty in ArtiÔ¨Å-
cial Intelligence , 2006.
[6] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and
harnessing adversarial examples. In Proceedings of the In-
ternational Conference on Learning Representations (ICLR) ,
2015.
[7] A. N. Gorban, I. Y . Tyukin, D. V . Prokhorov, and K. I.
Sofeikov. Approximation with random bases. Inf. Sci. ,
364(C):129‚Äì145, Oct. 2016.
[8] J. Hayes and G. Danezis. Machine learning as an adversarial
service: Learning black-box adversarial examples. 2017.
[9] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial ex-
amples in the physical world. 2016.
[10] Y . Liu, X. Chen, C. Liu, and D. Song. Delving into trans-
ferable adversarial examples and black-box attacks. In Pro-
ceedings of the International Conference on Learning Rep-
resentations (ICLR) , 2017.
[11] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and
A. Vladu. Towards deep learning models resistant to adver-
sarial attacks. 2017.
[12] Y . Nesterov and V . Spokoiny. Random gradient-free min-
imization of convex functions. Found. Comput. Math. ,
17(2):527‚Äì566, Apr. 2017.
[13] N. Papernot, P. McDaniel, and I. Goodfellow. Transferability
in machine learning: from phenomena to black-box attacks
using adversarial samples. 2016.
[14] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik,
and A. Swami. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia Con-
ference on Computer and Communications Security , ASIA
CCS ‚Äô17, pages 506‚Äì519, New York, NY , USA, 2017. ACM.
[15] T. Salimans, J. Ho, X. Chen, and I. Sutskever. Evolution
strategies as a scalable alternative to reinforcement learning.
CoRR , abs/1703.03864, 2017.
[16] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision.
2015.
[17] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus. Intriguing properties of neural
networks. 2013.
[18] D. Wierstra, T. Schaul, T. Glasmachers, Y . Sun, J. Peters,
and J. Schmidhuber. Natural evolution strategies. J. Mach.
Learn. Res. , 15(1):949‚Äì980, Jan. 2014.