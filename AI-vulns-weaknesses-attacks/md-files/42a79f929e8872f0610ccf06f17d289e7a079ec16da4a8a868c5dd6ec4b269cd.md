To Appear in 2021 ACM SIGSAC Conference on Computer and Communications Security, November 2021
Membership Leakage in Label-Only Exposures
Zheng Li and Yang Zhang
CISPA Helmholtz Center for Information Security
Abstract
Machine learning (ML) has been widely adopted in various
privacy-critical applications, e.g., face recognition and medi-
cal image analysis. However, recent research has shown that
ML models are vulnerable to attacks against their training
data. Membership inference is one major attack in this do-
main: Given a data sample and model, an adversary aims to
determine whether the sample is part of the model’s training
set. Existing membership inference attacks leverage the con-
ﬁdence scores returned by the model as their inputs (score-
based attacks). However, these attacks can be easily miti-
gated if the model only exposes the predicted label, i.e., the
ﬁnal model decision.
In this paper, we propose decision-based membership in-
ference attacks and demonstrate that label-only exposures are
also vulnerable to membership leakage. In particular, we de-
velop two types of decision-based attacks, namely transfer
attack and boundary attack. Empirical evaluation shows that
our decision-based attacks can achieve remarkable perfor-
mance, and even outperform the previous score-based attacks
in some cases. We further present new insights on the suc-
cess of membership inference based on quantitative and qual-
itative analysis, i.e., member samples of a model are more
distant to the model’s decision boundary than non-member
samples. Finally, we evaluate multiple defense mechanisms
against our decision-based attacks and show that our two
types of attacks can bypass most of these defenses.1
1 Introduction
Machine learning (ML) has witnessed tremendous progress
over the past decade and has been applied across a wide
range of privacy-critical applications, such as face recogni-
tion [28, 61] and medical image analysis [9, 29, 51]. Such
developments rely on not only novel training algorithms and
architectures, but also access to sensitive and private data,
such as health data. Various recent research [23, 25, 31, 35,
36, 45, 46, 48, 49, 54, 57, 60] has shown that ML models are
vulnerable to privacy attacks. One major attack in this do-
main is membership inference: An adversary aims to deter-
mine whether or not a data sample is used to train a target
ML model.
Existing membership inference attacks [25, 31, 35, 46, 48,
1Our code is available at https://github.com/zhenglisec/Decision-
based-MIA .
Cat
Cat0.1
0.8
0.1
0.0
0.2
0.5
0.1
0.2
Decision -based
Score -based
Non-memMemberFigure 1: An illustration of accessible components of the tar-
get model for each of the two threat models. A score-based
threat model assumes access to the output layer; a decision-
based threat model assumes access to the predicted label alone.
49, 57] rely on the conﬁdence scores (e.g. class probabilities
or logits) returned by a target ML model as their inputs. The
success of membership inference is due to the inherent over-
ﬁtting property of ML models, i.e., an ML model is more
conﬁdent facing a data sample it was trained on, and this
conﬁdence is reﬂected in the model’s output scores. See Fig-
ure 1 for an illustration of accessible components of an ML
model for such score-based threat model. A major drawback
for these score-based attacks is that they can be trivially mit-
igated if the model only exposes the predicted label, i.e., the
ﬁnal model decision, instead of conﬁdence scores. The fact
that score-based attacks can be easily averted makes it more
difﬁcult to evaluate whether a model is truly vulnerable to
membership inference or not, which may lead to premature
claims of privacy for ML models.
This motivates us to focus on a new category of member-
ship inference attacks that has so far received fairly little at-
tention, namely Decision-based attacks . Here, the adversary
solely relies on the ﬁnal decision of the target model, i.e., the
top-1 predicted label, as their attack model’s input. It is more
realistic to evaluate the vulnerability of a machine learning
system under the decision-based attacks with sole access to
the model’s ﬁnal decision. First, compared to score-based at-
tacks, decision-based attacks are much more relevant in real-
world applications where conﬁdence scores are rarely acces-
sible. Furthermore, decision-based attacks have the potential
to be much more robust to the state-of-the-art defenses, such
as conﬁdence score perturbation [27, 38, 56]. In label-only
exposure, a naive baseline attack [57] infers that a candi-
date sample is a member of a target model if it is predicted
correctly by the model. However, this baseline attack can-
not distinguish between members and non-members that are
both correctly classiﬁed as shown in Figure 1.
In this paper, we propose two types of decision-based at-
1arXiv:2007.15528v3 [cs.LG] 17 Sep 2021tacks under different scenarios, namely transfer attack and
boundary attack . In the following, we abstractly introduce
each of them.
Transfer Attack. We assume the adversary has an auxiliary
dataset (namely shadow dataset) that comes from the same
distribution as the target model’s training set. The assump-
tion also holds for previous score-based attacks [35, 46, 48,
49]. The adversary ﬁrst queries the target model in a man-
ner analog to cryptographic oracle, thereby relabeling the
shadow dataset by the target model’s predicted labels. Then,
the adversary can use the relabeled shadow dataset to con-
struct a local shadow model to mimic the behavior of the
target model. In this way, the relabeled shadow dataset con-
tains sufﬁcient information from the target model, and mem-
bership information can also be transferred to the shadow
model. Finally, the adversary can leverage the shadow model
to launch a score-based membership inference attack locally.
Boundary Attack. Collecting data, especially sensitive and
private data, is a non-trivial task. Thus, we consider a more
difﬁcult and realistic scenario in which there is no shadow
dataset and shadow model available. To compensate for the
lack of information in this scenario, we shift the focus from
the target model’s output to the input. Here, our key intu-
ition is that it is harder to perturb member data samples to
different classes than non-member data samples. The adver-
sary queries the target model on candidate data samples, and
perturb them to change the model’s predicted labels. Then
the adversary can exploit the magnitude of the perturbation
to differentiate member and non-member data samples.
Extensive experimental evaluation shows that both of our
attacks achieve strong performance. In particular, our bound-
ary attack in some cases even outperforms the previous
score-based attacks. This demonstrates the severe mem-
bership risks stemming from ML models. In addition, we
present a new perspective on the success of current member-
ship inference and show that the distance between a sample
and an ML model’s decision boundary is strongly correlated
with the sample’s membership status.
Finally, we evaluate our attacks on multiple defense mech-
anisms: generalization enhancement [46, 50, 54], privacy en-
hancement [4] and conﬁdence score perturbation [27,38,56].
The results show that our attacks can bypass most of the
defenses, unless heavy regularization is applied. However
heavy regularization can lead to a signiﬁcant degradation of
the model accuracy.
In general, our contributions can be summarized as the fol-
lowing:
• We perform a systematic investigation on membership
leakage in label-only exposures of ML models, and in-
troduce decision-based membership inference attacks,
which is highly relevant for real-world applications and
important to gauge model privacy.
• We propose two types of decision-based attacks under
different scenarios, including transfer attack and bound-
ary attack. Extensive experiments demonstrate that our
two types of attacks achieve better performances thanthe baseline attack, and even outperform the previous
score-based attacks in some cases.
• We propose a new perspective on the reasons for the
success of membership inference, and perform a quanti-
tative and qualitative analysis to demonstrate that mem-
bers of an ML model are more distant from the model’s
decision boundary than non-members.
• We evaluate multiple defenses against our decision-
based attacks and show that our novel attacks can still
achieve reasonable performance unless heavy regular-
ization is applied.
Paper Organization. The rest of this paper is organized
as follows. Section 2 presents the deﬁnitions of membership
inference for the ML models, threat models, datasets, and
model architectures used in this paper. Section 3 and Sec-
tion 4 introduce our two attack methods and evaluation meth-
ods. In Section 5, we provide an in-depth analysis of the suc-
cess of membership inference. Section 6 provides multiple
defenses against decision-based attacks. Section 7 presents
related work, and Section 8 concludes the paper.
2 Preliminaries
2.1 Membership Leakage in Machine Learn-
ing Models
Membership leakage in ML models emerges when an adver-
sary aims to determine whether a candidate data sample is
used to train a certain ML model. More formally, given a
candidate data sample x, a trained ML model M, and exter-
nal knowledge of an adversary, denoted by W, a membership
inference attack Acan be deﬁned as the following function.
A:x;M;W!f0;1g:
Here, 0 means xis not a member of M’s training set and 1
otherwise. The attack model Ais essentially a binary classi-
ﬁer. Depending on the assumptions, it can be constructed in
different ways, which will be presented in later sections.
2.2 Threat Model
Here, we outline the threat models considered in this paper.
The threat models are summarized in Table 1. There are two
existing categories of attacks, i.e., score-based attacks and
decision-based attacks. The general idea of score-based at-
tacks is to exploit the detailed output (i.e., conﬁdence score)
of the target model to launch an attack. In decision-based
attacks, an adversary cannot access to conﬁdence scores, but
relies on the ﬁnal predictions of the target model launch an
attack. The baseline attack predicts a data sample as a mem-
ber of the training set when the model classiﬁes it correctly.
However, this naive and simple approach does not work at
all in the case shown in Figure 1. In the following, we intro-
duce the adversarial knowledge that an adversary has in our
decision-based attacks.
2Table 1: An overview of membership inference threat models. “ X” means the adversary needs the knowledge and “-” indicates the
knowledge is not necessary.
Attack Category AttacksTarget Model’s Training Data Shadow Detailed Model Prediction Final Model Prediction
Structure Distribution Model (e.g. probabilities or logits) (e.g. max class label)
Score-based [25, 31, 35, 46, 48, 49, 57] Xor - Xor - Xor - X X
Decision-basedBaseline attack [57] - X - - X
Transfer attack - X X - X
Boundary attack - - - - X
Adversarial Knowledge. For our decision-based attacks,
the adversary only has black-box access to the target model,
i.e., they are not able to extract a candidate data sample’s
membership status from the conﬁdence scores. Concretely,
our threat model comprises the following entities. (1) Final
decision of the target model M, i.e., predicted label. (2) A
shadow dataset Dshadow drawn from the same distribution as
target model’s dataset Dtarget. (3) A local shadow model S
trained using the shadow dataset Dshadow . For boundary at-
tack, the adversary only has the knowledge of (1).
2.3 Datasets and Target Model Architecture
Datasets. We consider four benchmark datasets of different
size and complexity, namely CIFAR-10 [1], CIFAR-100 [1],
GTSRB [2], and Face [3], to conduct our experiments. Since
the images in GTSRB are of different sizes, we resize them to
6464 pixels. For the Face dataset, we only consider people
with more than 40 images, which leaves us with 19 people’s
data, i.e., 19 classes. We describe them in detail in Appendix
Section A.1.
Target Model Architecture. Typically, for image classiﬁ-
cation tasks, we use neural networks which is adopted across
a wide of applications. In this work, we build the target
model using 4 convolutional layers and 4 pooling layers with
2 hidden layers containing 256 units each at last. The target
models are trained for 200 training epochs, iteratively using
Adam algorithm with a batch-size of 128 and a ﬁxed learning
rate of 0.001.
3 Transfer Attack
In this section, we present the ﬁrst type of decision-based
attacks, i.e., transfer attack. We start by introducing our key
intuition. Then, we describe the attack methodology. Finally,
we present the evaluation results.
3.1 Key Intuition
The intuition of this attack is that the transferability property
holds between shadow model Sand target model M. Almost
all related works [15, 33, 37, 39] focus on the transferability
of adversarial examples, i.e., adversarial examples can trans-
fer between models trained for the same task. Unlike these
works, we focus on the transferability of membership infor-
mation for benign data samples, i.e., the member and non-
member data samples behaving differently in Mwill alsobehave differently in S. Then we can leverage the shadow
model to launch a score-based membership inference attack.
3.2 Methodology
The transfer attack’s methodology can be divided into four
stages, namely shadow dataset relabeling, shadow model ar-
chitecture selection, shadow model training, and member-
ship inference. The algorithm can be found in Appendix al-
gorithm 1.
Shadow Dataset Relabeling. As aforementioned, the ad-
versary has a shadow dataset Dshadow drawn from the same
distribution as the target model M’s dataset Dtarget. To train
a shadow model, the ﬁrst step is to relabel these data samples
using the target model Mas an oracle. In this way, the adver-
sary can establish a connection between the shadow dataset
and the target model, which facilitates the shadow model to
be more similar to the target model in the next step.
Shadow Model Architecture Selection. As the adversary
knows the main task of the target model, it can build the
shadow model using high-level knowledge of the classiﬁca-
tion task (e.g., convolutional networks are appropriate for vi-
sion). As in prior score-based attacks, we also use the same
architecture of target models to build our shadow models.
Note that we emphasize that the adversary does not have the
knowledge of the concrete architecture of the target model,
and in Section 3.4, we also show that a wide range of archi-
tecture choices yield similar attack performance.
Shadow Model Training. The adversary trains the shadow
model Swith the relabeled shadow dataset Dshadow in con-
junction with classical training techniques.
Membership Inference. Finally, the adversary feeds a can-
didate data sample into the shadow model Sto calculate its
cross-entropy loss with the ground truth label.
CELoss = K
å
i=01ylog(pi); (1)
where 1yis the one-hot encoding of the ground truth label
y,piis the probability that the candidate sample belongs to
class i, and Kis the number of classes. If the loss value is
smaller than a threshold, the adversary then determines the
sample being a member and vice versa. The adversary can
pick a suitable threshold depending on their requirements, as
in many machine learning applications. [7,19,22,27,43,46].
In our evaluation, we mainly use area under the ROC curve
(AUC) which is threshold independent as our evaluation met-
ric.
30 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC
Baseline attack.
Transfer attack(a) CIFAR-10
0 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC (b) CIFAR-100
0 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC (c) GTSRB
0 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC (d) Face
Figure 2: Comparison of our transfer attack performance with the baseline attack by Yeom et al. [57]. The x-axis represents the target
model being attacked and the y-axis represents the AUC score.
5000 6000 700010000 15000 20000 30000 35000 42000
Shadow Dataset Size4.66M
418.88M
4.21M
305.5M
4.10M
248.55M
4.07M
210.54M
3.84M
153.78M
3.18M
97.0M
1.71M
59.17M
0.97M
40.25M
0.86M
26.01MShadow Model Complexity0.78 0.79 0.78 0.79 0.79 0.8 0.8 0.8 0.81
0.79 0.79 0.79 0.79 0.8 0.8 0.8 0.81 0.8
0.79 0.79 0.79 0.79 0.8 0.8 0.79 0.8 0.81
0.78 0.79 0.79 0.79 0.8 0.8 0.81 0.8 0.8
0.78 0.79 0.79 0.79 0.8 0.79 0.8 0.8 0.8
0.78 0.79 0.79 0.79 0.8 0.79 0.8 0.8 0.8
0.78 0.79 0.78 0.79 0.79 0.79 0.8 0.8 0.8
0.78 0.79 0.79 0.79 0.8 0.8 0.8 0.79 0.8
0.78 0.79 0.79 0.79 0.79 0.79 0.78 0.79 0.79
0.760.770.780.790.800.81
Figure 3: Attack AUC under the effect of changing the dataset
size and shadow model complexity (upper is the number of pa-
rameters, lower is the computational complexity FLOPs). The
target model ( M-0, CIFAR-100)’s training set size is 7,000, and
complexity is 3.84M parameters and 153.78M FLOPs.
3.3 Experimental Setup
Following the attack strategy, we split each dataset into
Dtarget andDshadow : One is used to train and test the target
model, and the other is used to train the shadow model Safter
relabeled by the target model. For evaluation, Dtarget is also
split into two: One is used to train the target model M, i.e.,
Dtrain, and serves as the member samples of the target model,
while the other Dtestserves as the non-member samples.
It is well known that the inherent overﬁtting drives ML
models to be vulnerable to membership leakage [46, 48]. To
show the variation of the attack performance on each dataset,
we train 6 target models M-0,M-1, ..., M-5 using differ-
ent size of the training set Dtrain, exactly as performed in
the prior work by Shokri et al. [48] and many subsequent
works [35, 46, 49, 54]. The sizes of Dtrain,Dtest, andDshadow
are summarized in Appendix Table 7.
We execute the evaluation on randomly reshufﬂed data
samples from Dtarget, and select sets of the same size (i.e,
equal number of members and non-members) to maximize
the uncertainty of inference, thus the baseline performance is
equivalent to random guessing. We adopt AUC as our evalu-
ation metric which is threshold independent. In addition, we
further discuss methods to pick threshold for our attack later
in this section.3.4 Results
Attack AUC Performance. Figure 2 depicts the perfor-
mance of our transfer attack and baseline attack. First, we
can observe that our transfer attack performs at least on-par
with the baseline attack. More encouragingly, on the CIFAR-
10 and GTSRB datasets, our transfer attack achieves better
performance than the baseline attack. For example, in Fig-
ure 2 (M-5, CIFAR-10), the AUC score of the transfer attack
is 0.94, while that of the baseline attack is 0.815. The rea-
son why our transfer attack outperforms the baseline attack
on CIFAR-10 and GTSRB rather than on CIFAR-100 and
Face, is that the size of the shadow dataset for the ﬁrst two
datasets is relatively larger than that of the latter two, com-
pared to the size of each dataset (see Appendix Table 7). In
the next experiments, we make the same observation that a
larger shadow dataset implies better attack performance.
Effects of the Shadow Dataset and Model. We further
investigate the effects of shadow dataset size and shadow
model complexity (structure and hyper-parameter) on the at-
tack performance. More concretely, for the target model
(M-0, CIFAR-100), we vary the size of the shadow dataset
Dshadow from 5,000 to 42,000, where the target training
setDtrain is 7,000. We also vary the complexity of the
shadow model from 0.86M (number of parameters) and
26.01M (FLOPs,2computational complexity) to 4.86M and
418.88M, where the complexity of the target model is 3.84M
and 153.78M, respectively. We conduct extensive exper-
iments to simultaneously tune these two hyper-parameters
and report the results in Figure 3. Through investigation, we
make the following observations.
• Larger shadow dataset implies more queries to the target
model which leads to better attack performance.
• Even simpler shadow models and fewer shadow datasets
(bottom left part) can achieve strong attack perfor-
mance.
• In general, the transfer attack is robust even if the
shadow model is much different from the target model.
2FLOPs represent the theoretical amount of ﬂoating-point arithmetic needed
when feeding a sample into the model.
40 5 10 15 20 25
Cross-Entropy Loss100101102103NumbersMember
Non-Member(a) CIFAR-10, M-0
0 10 20 30
Cross-Entropy Loss100101102NumbersMember
Non-Member (b) CIFAR-10, M-5
0 5 10 15 20 25
Cross-Entropy Loss100101102103NumbersMember
Non-Member (c) CIFAR-100, M-0
0 10 20 30
Cross-Entropy Loss100101102103NumbersMember
Non-Member (d) CIFAR-100, M-5
Figure 4: The cross entropy loss distribution obtained from the shadow model. The x-axis represents the loss value and the y-axis
represents the number of the loss.
0 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC
Entropy
Maximum
CELoss
(a) CIFAR-10
0 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC (b) CIFAR-100
Figure 5: Attack AUC for three different statistical measures.
The x-axis represents the target model being attacked and the
y-axis represents the AUC score.
Effects of Statistical Metrics. As prior works [46, 48] also
use other statistical metrics, i.e., maximum conﬁdence scores
Max(pi)and normalized entropy 1
log(K)åipilog(pi). Here,
we also conduct experiments with these statistical metrics.
Figure 5 reports the AUC on the CIFAR-10 and CIFAR-100
datasets. We can observe that the loss metric achieves the
highest performance with respect to the different target mod-
els. Meanwhile, the AUC score is very close between maxi-
mum conﬁdence score and entropy. This indicates that the
loss metric contains the strongest signal on differentiating
member and non-member samples. We will give an in-depth
discussion on this in Section 5.2.
Loss Distribution of Membership. To explain why our
transfer attack works, Figure 4 further shows the loss distri-
bution of member and non-member samples from the target
model calculated on the shadow model ( M-0 and M-5 on
CIFAR-10 and CIFAR-100). Though both member and non-
member samples are never used to train the shadow model,
we still observe a clear difference between their loss distri-
bution. This veriﬁes our key intuition aforementioned: The
transferability of membership information holds between
shadow model Sand target model M, i.e., the member and
non-member samples behaving differently in Mwill also be-
have differently in S.
Threshold Choosing. As mentioned before, in the member-
ship inference stage, the adversary needs to make a manual
decision on which threshold to use. For the transfer attack,
since we assume that the adversary has a dataset that comesfrom the same distribution as the target model’s dataset, it
can rely on the shadow dataset to estimate a threshold by
sampling certain part of that dataset as non-member samples.
4 Boundary-Attack
After demonstrating our transfer attack, we now present our
second attack, i.e., boundary attack. Since curating auxil-
iary data requires signiﬁcant time and monetary investment.
Thus, we relax this assumption in this attack. The adver-
sary does not have a shadow dataset to train a shadow model.
All they could rely on is the predicted label from the target
model. To the best of our knowledge, this is by far the most
strict setting for membership inference against ML models.
In the following section, we start with the key intuition de-
scription. Then, we introduce the attack methodology. In the
end, we present the evaluation results.
4.1 Key Intuition
Our intuition behind this attack follows a general observation
of the overﬁtting nature of ML models. Concretely, an ML
model is more conﬁdent in predicting data samples that it is
trained on. In contrast to the prior score-based attacks [25,
31, 35, 46, 48, 49, 57] that directly exploit conﬁdence scores
as analysis objects, we place our focus on the antithesis of
this observation, i.e., since the ML model is more conﬁdent
on member data samples, it should be much harder to change
its mind.
Intuitively, Figure 6 depicts the conﬁdence scores for two
randomly selected member data samples (Figure 6a, Fig-
ure 6c) and non-member data samples (Figure 6b, Figure 6d)
with respect to M-0 trained on CIFAR-10. We can observe
that the maximal score for member samples is indeed much
higher than the one of non-member samples. We further use
cross entropy (Equation 1) to quantify the difﬁculty for an
ML model to change its predicted label for a data sample to
other labels.
Table 2 shows the cross entropy between the conﬁdence
scores and other labels for these samples. We can see that
member samples’ cross entropy is signiﬁcantly larger than
non-member samples. This leads to the following observa-
tion on membership information.
Observation. Given an ML model and a set of data sam-
5Table 2: The cross entropy between the conﬁdence scores and other labels except for the predicted label. ACE represent the Average
Cross Entropy.
Truth Predicted Cross Entropy
Status Label Label 0 1 2 3 4 5 6 7 8 9 ACE
(a) Member 6 6 7.8156 8.3803 4.1979 1.0942 4.1367 4.3492 - 7.6328 1.5522 1.2923 4.4946
(b) Non-member 8 8 2.3274 0.8761 0.8239 2.0793 1.2275 0.9791 1.2373 1.1152 - 5.0451 1.2218
(c) Member 3 3 1.2995 5.2842 5.4212 - 1.5130 4.8059 4.5897 7.1547 3.2411 4.7910 4.2334
(d) Non-member 7 9 2.8686 1.8325 3.6480 0.5352 1.8722 1.1689 4.0124 0.6866 3.1071 - 2.1766
0123456789
Class Label0.00.20.40.60.81.0Confidence Score
(a) Member data sample
0123456789
Class Label0.00.20.40.60.81.0Confidence Score (b) Non-member data sample
0123456789
Class Label0.00.20.40.60.81.0Confidence Score
(c) Member data sample
0123456789
Class Label0.00.20.40.60.81.0Confidence Score (d) Non-member data sample
Figure 6: The probability distribution of the target model ( M-0,
CIFAR-10) on member samples and non-member samples.
ples, the cost of changing the target model’s predicted labels
for member samples is larger than the cost for non-member
samples. Furthermore, consider the label-only exposures in
a black-box ML model, which means the adversary can only
perturb the data samples to change the target model’s pre-
dicted labels, thus the perturbation needed to change a mem-
ber sample’s predicted label is larger than non-members.
Then, the adversary can exploit the magnitude of the per-
turbation to determine whether the sample is a member or
not.
4.2 Methodology
Our attack methodology consists of the following three
stages, i.e., decision change, perturbation measurement, and
membership inference. The algorithm can be found in Ap-
pendix algorithm 2.
Decision Change. The goal of changing the ﬁnal model
decision, i.e., predicted label, is similar to that of adversar-
ial attack [8, 10, 41, 42, 47, 52], For simplicity, we utilize ad-
versarial example techniques to perturb the input to mislead
the target model. Speciﬁcally, we utilize two state-of-the-
art black-box adversarial attacks, namely HopSkipJump [12]
and QEBA [30], which only require access to the model’spredicted labels.
Perturbation Measurement. Once the ﬁnal model decision
has changed, we measure the magnitude of the perturbations
added to the candidate input samples. In general, adversarial
attack techniques typically use Lpdistance (or Minkowski
Distance), e.g., L0,L1,L2, and L¥, to measure the perceptual
similarity between a perturbed sample and its original one.
Thus, we use Lpdistance to measure the perturbation.
Membership Inference. After obtaining the magnitude of
the perturbations, the adversary simply considers a candidate
sample with perturbations larger than a threshold as a mem-
ber sample, and vice versa. Similar to the transfer attack, we
mainly use AUC as our evaluation metric. We also provide a
general and simple method for choosing a threshold in Sec-
tion 4.4.
4.3 Experiment Setup
We use the same experimental setup as presented in Sec-
tion 3.3, such as the dataset splitting strategy and 6 target
models trained on different size of training set Dtrain. In the
decision change stage, we use the implementation of a popu-
lar python library (ART3) for HopSkipJump, and the authors’
source code4for QEBA. Note that we only apply untargeted
decision change, i.e., changing the initial decision of the tar-
get model to any other random decision. Besides, both Hop-
SkipJump and QEBA require multiple queries to perturb data
samples to change their predicted labels. We set 15,000 for
HopSkipJump and 7,000 for QEBA. We further study the in-
ﬂuence of the number of queries on the attack performance.
For space reasons, we report the results of HopSkipJump
scheme in the main body of our paper. Results of QEBA
scheme can be found in Appendix Figure 14 and Figure 15.
4.4 Results
Distribution of Perturbation. First, we show the distri-
bution of perturbation between a perturbed sample and its
original one for member and non-member samples in Fig-
ure 7. Both decision change schemes, i.e., HopSkipJump
and QEBA, apply L2distance to limit the magnitude of per-
turbation, thus we report results of L2distance as well. As
expected, the magnitude of the perturbation on member sam-
ples is indeed larger than that on non-member samples. For
3https://github.com/Trusted-AI/adversarial-robustness-
toolbox
4https://github.com/AI-secure/QEBA
60 1 2 3 4 5
Target Model 
0.01.02.03.04.0L2 Distance
Member
Non-Member(a) CIFAR-10
0 1 2 3 4 5
Target Model 
0.01.02.03.0L2 Distance
 (b) CIFAR-100
0 1 2 3 4 5
Target Model 
0.02.04.06.0L2 Distance
 (c) GTSRB
0 1 2 3 4 5
Target Model 
0.01.02.03.04.05.06.0L2 Distance
HopSkipJump (d) Face
Figure 7: L2distance between the original sample and its perturbed samples generated by the HopSkipJump attack. The x-axis
represents the target model being attacked and the y-axis represents the L2distance.
0 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC
L0
L1
L2
L
(a) CIFAR-10
0 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC (b) CIFAR-100
0 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC (c) GTSRB
0 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC
HopSkipJump (d) Face
Figure 8: Attack AUC for four different Lpdistances between the original samples and its perturbed samples generated by the Hop-
SkipJump attack. The x-axis represents the target model being attacked and the y-axis represents the AUC score.
instance in Figure 7 ( M-5, CIFAR-10), the average L2dis-
tance of the perturbation for member samples is 1.0755,
while that for non-member samples is 0.1102. In addition,
models with a larger training set, i.e., lower overﬁtting level,
require less perturbation to change the ﬁnal prediction. As
the overﬁtting level increases, the adversary needs to modify
more on the member sample. The reason is that an ML model
with a higher overﬁtting level has remembered its training
samples to a larger extent, thus it is much harder to change
their predicted labels, i.e., larger perturbation is required.
Attack AUC Performance. We report the AUC scores over
all datasets in Figure 8. In particular, we compare 4 differ-
ent distance metrics, i.e., L0,L1,L2, and L¥, for each de-
cision change scheme. From Figure 8, we can observe that
L1,L2andL¥metrics achieve the best performance across
all datasets. For instance in Figure 8 ( M-1, CIFAR-10), the
AUC scores for L1,L2, and L¥metrics are 0.8969, 0.8963,
and 0.9033, respectively, while the AUC score for L0metric
is 0.7405. From Figure 15 (in Appendix), we can also ob-
serve the same results of QEBA scheme: L1,L2andL¥met-
rics achieve the best performance across all datasets, while
L0metric performs the worst. Therefore, an adversary can
simply choose the same distance metric adopted by adver-
sarial attacks to measure the magnitude of the perturbation.
Effects of Number of Queries. To mount boundary attack
in real-world ML applications such as Machine Learning as a
Service (MLaaS), the adversary cannot issue as many queries
as they want to the target model, since a large number of
queries increases the cost of the attack and may raise the
suspicion of the model provider. Now, we evaluate the at-
0 2500 5000 7500 10000
Number of Queries0.50.60.70.80.91.0AUC
CIFAR-10, -5
CIFAR-100, -5
GTSRB, -5
Face, -5
Figure 9: Attack AUC under the effect of number of queries.
The x-axis represents the number of queries and the y-axis rep-
resents the AUC score for perturbation-based attack.
tack performance with different number of queries. Here, we
show the results of the HopSkipJump scheme for M-5 over
all datasets. We vary the number of queries from 0 to 15,000
and evaluate the attack performance based on the L2met-
ric. As we can see in Figure 9, the AUC increases sharply
as the number of queries increases in the beginning. After
2,500 queries, the attack performance becomes stable. From
the results, we argue that query limiting would likely not be
a suitable defense. For instance, when querying 131 times,
the AUC for CIFAR-10 is 0.8228 and CIFAR-100 is 0.9266.
At this time, though the perturbed sample is far away from
its origin’s decision boundary, the magnitude of perturbation
70 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC
Score-based
Baseline attack
Transfer attack
Boundary attack(a) CIFAR-10
0 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC (b) CIFAR-100
0 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC (c) GTSRB
0 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC (d) Face
Figure 10: Comparison of our two types of attacks with the baseline attack and score-based attack. The x-axis represents the target
model being attacked and the y-axis represents the AUC score.
20 40 60 80
Percentile 
0.50.60.70.80.91.0F1 Score
CIFAR-10, -5
CIFAR-100, -5
GTSRB, -5
Face, -5
Figure 11: The relation between the top tpercentile of the L2
distance, i.e., threshold, and the attack performance. The x-axis
represents the top tpercentile and the y-axis represents the F1
score.
for member samples is still relatively larger than that for non-
member samples. Thus, the adversary can still differentiate
member and non-member samples.
Threshold Choosing. Here, we focus on the threshold
choosing for our boundary attack where the adversary is not
equipped with a shadow dataset. We provide a simple and
general method for choosing a threshold. Concretely, we
generate a set of random samples in the feature space as
the target model’s training set. In the case of image clas-
siﬁcation, we sample each pixel for an image from a uniform
distribution. Next, we treat these randomly generated sam-
ples as non-members and query them to the target model.
Then, we apply adversarial attack techniques on these ran-
dom samples to change their initial predicted labels by the
target model. Finally, we use these samples’ perturbation to
estimate a threshold, i.e., ﬁnding a suitable top tpercentile
over these perturbations. The algorithm can be found in Ap-
pendix algorithm 3.
We experimentally generate 100 random samples for M-5
trained across all datasets, and adopt HopSkipJump in deci-
sion change stage. We again use the L2distance to measure
the magnitude of perturbation and F1 score as our evaluation
metric. From Figure 11, we make the following observations:
• The peak attack performance is bounded between t=0% and t=100%, which means the best threshold can
deﬁnitely be selected from these random samples’ per-
turbation.
• The powerful and similar attack performance ranges
from t=30% to t=80%, reaching half of the total per-
centile, which means that a suitable threshold can be
easily selected.
Therefore, we conclude that our threshold choosing method
is effective and can achieve excellent performance.
Comparison of Different Attacks. Now we compare the
performance of our two attacks and previous existing attacks.
In particular, we also compare our attacks against prior score-
based attacks. Following the score-based attack proposed by
Salem et al. [46], we train one shadow model using half of
Dshadow with its ground truth labels, and one attack model
in a supervised manner based on the shadow model’s output
scores. Here, we do not assume that the attacker knows the
exact training set size of the target model, which is actually
a strong assumption. Note that this is not a fair comparison,
as our decision-based attacks only access to the ﬁnal model’s
prediction, rather than the conﬁdence scores.
We report attack performance for our boundary attack us-
ingL2metric in HopSkipJump scheme. From Figure 10, we
can ﬁnd that our boundary attack achieves similar or even
better performance than the score-based attack in some cases.
This demonstrates the efﬁcacy of our proposed decision-
based attack, thereby the corresponding membership leakage
risks stemming from ML models are much more severe than
previously shown.
As for cost analysis, the attack logic is different for each
method, so it is difﬁcult to evaluate the cost with standard
metrics. Besides the adversarial knowledge acquired for each
attack, we mainly report training costs and query costs in Ta-
ble 4. We can ﬁnd the baseline attack only queries once for
a candidate sample. However, in our transfer attack, once
a shadow model is built, the adversary will only query the
shadow model for candidate samples without making any
other queries to the target model. Therefore, we cannot pre-
maturely claim that the baseline attack has the lowest cost,
but should consider the actual situation.
8Table 3: Average Certiﬁed Radius (ACR) of members and non-members for target models.
Target CIFAR-10 CIFAR-100 GTSRB Face
Model Member Non-mem Member Non-mem Member Non-mem Member Non-mem
M-0 0.1392 0.1201 0.0068 0.0033 0.0300 0.0210 0.0571 0.0607
M-1 0.1866 0.1447 0.0133 0.0079 0.0358 0.0215 0.0290 0.0190
M-2 0.1398 0.1170 0.0155 0.0079 0.0692 0.0463 0.0408 0.0313
M-3 0.1808 0.1190 0.0079 0.0074 0.0430 0.0348 0.1334 0.1143
M-4 0.1036 0.1032 0.0141 0.0116 0.0212 0.0176 0.0392 0.0292
M-5 0.1814 0.0909 0.0157 0.0080 0.0464 0.0385 0.1242 0.1110
Table 4: The cost of each attack. Query cost is the number of
queries to the target model.
Attack Shadow Model Query for Query for a
Type Training Epochs Dshadow candidate sample
score-based 200 - 1
baseline attack - - 1
transfer attack 200 |Dshadow | -
boundary attack - - Multiple
5 Membership Leakage Analysis
The above results fully demonstrate the effectiveness of our
decision-based attacks. Here, we delve more deeply into the
reasons for the success of membership inference. Our bound-
ary attack utilizes the magnitude of the perturbation to deter-
mine whether the sample is a member or not, and the key
to stop searching perturbations is the ﬁnal decision change
of the model. Here, the status of decision change actually
contains information about the decision boundary, i.e., the
perturbed sample crosses the decision boundary. This sug-
gests a new perspective on the relationship between member
samples and non-member samples, and we intend to analyze
membership leakage from this perspective. Since previous
experiments have veriﬁed our key intuition that the pertur-
bation required to change the predicted label of a member
sample is larger than that of a non-member, we argue that the
distance between the member sample and its decision bound-
ary is typically larger than that of the non-member sample.
Next, we will verify it both quantitatively and qualitatively.
5.1 Quantitative Analysis
We introduce the neighboring Lp-radius ball to investigate
the membership leakage of ML models. This neighboring
Lp-radius ball, also known as Robustness Radius , is deﬁned
as the Lprobustness of the target model at a data sample,
which represents the radius of the largest Lpball centered at
the data sample in which the target model does not change
its prediction, as shown in Figure 12d. Concretely, we in-
vestigate the L2robustness radius of the target model M
at a data sample x. Unfortunately, computing the robust-
ness radius of a ML model is a hard problem. Researchers
have proposed many certiﬁcation methods to derive a tight
lower bound of robustness radius R(M;x;y)for ML mod-
els. Here, we also derive a tight lower bound of robust-
ness radius, namely Certiﬁed Radius [58], which satisﬁes
0CR(M;x;y)R(M;x;y)for any M;xand its groundtruth label y2Y=f1;2;;Kg. More details about certi-
ﬁed radius can be found in Appendix Section A.2.
ACR of Members and Non-members. As we can see from
Theorem 1 (see Appendix Section A.2), the value of the certi-
ﬁed radius can be estimated by repeatedly sampling Gaussian
noises. For the target model Mand a data sample (x;y), we
can estimate the certiﬁed radius CR(M;x;y). Here, we use
theaverage certiﬁed radius (ACR) as a metric to estimate
the average certiﬁed radius for members and non-members
separately, i.e.,
ACR member =1
jDtrainjå
(x;y)2DtrainCR(M;x;y); (2)
ACR non member =1
jDtestjå
(x;y)2DtestCR(M;x;y): (3)
Table 5: Average Certiﬁed Radius (ACR) of members and non-
members for shadow models.
Shadow CIFAR-10 CIFAR-100
Model Member Non-mem Member Non-mem
M-0 0.1392 0.1301 0.0091 0.0039
M-1 0.1873 0.1516 0.0150 0.0071
M-2 0.1416 0.1463 0.0177 0.0068
M-3 0.1962 0.1452 0.0121 0.0047
M-4 0.1152 0.1046 0.0099 0.0092
M-5 0.1819 0.0846 0.0176 0.0087
We randomly select an equal number of members and non-
members for target models and report the results in Table 3.
Note that the certiﬁed radius is actually an estimated value
representing the lower bound of the robustness radius, not
the exact radius. Therefore, we analyze the results from a
macroscopic perspective and can draw the following obser-
vations.
• The ACR of member samples is generally larger than
the ACR of non-member samples, which means that
in the output space, the ML model maps member sam-
ples further away from its decision boundary than non-
member samples.
• As the level of overﬁtting increases, the macroscopic
trend of the gap between the ACR of members and
non-members is also larger, which exactly reﬂects the
increasing attack performance in the aforementioned
AUC results.
9Member
Non-member(a) Target Model, Zoom-out
Member
Non-member (b) Target Model, Zoom-in
Member
Non-member
Misclassification (c) Shadow Model, Zoom-in
Adversary 2 ：Reasoning
HopSkipJumpStarting image
QEBA 
Prediction unchanging 
regionRadius (d) Adversarial attack process
Figure 12: The visualization of decision boundary for target model (a, b) and shadow model (c), and the search process of perturbed
sample by HopSkipJump and QEBA (d).
30 40 50 60 70
Target Model Test Accuracy, %0.20.40.60.81.0AUCDefense Mechanism
None
Dropout
L1
L2
Differential Privacy
Data Augmentation
Adversarial Regularization
MemGuard
(a) Score-based attack
30 40 50 60 70
Target Model Test Accuracy, %0.20.40.60.81.0AUC (b) Baseline attack
30 40 50 60 70
Target Model Test Accuracy, %0.20.40.60.81.0AUC (c) Transfer attack
30 40 50 60 70
Target Model Test Accuracy, %0.20.40.60.81.0AUC (d) Boundary attack
Figure 13: Attack AUC of transfer attack and boundary attack against multiple defense mechanisms.
Furthermore, we also feed the equal member and non-
member samples into each corresponding shadow model and
obtain the ACR. Note that both member and non-member
samples are never used to train the shadow model. We report
the results in Table 5, and we can draw the same observa-
tions as for the target model. In other words, this again ver-
iﬁes our key intuition for transfer attack: The transferability
of membership information holds between shadow model S
and target model M, i.e., the member and non-member sam-
ples behaving differently in Mwill also behave differently
with high probability in S.
5.2 Qualitative Analysis
Next, we investigate the membership leakage of ML mod-
els from a visualization approach. We study the decision
boundary of the target model (CIFAR-10, M-3) with a given
set of data samples, including 1,000 member samples and
1,000 non-member samples. To better visualize the decision
boundary, there are two points to note:
• Both member and non-member samples are mapped
from the input space to the output space, which then
presents the membership signal. Thus, we visualize the
decision boundary in the output space, i.e., the trans-
formed space of the last hidden layer which is fully con-
nected with the ﬁnal model decision.
• Due to the limitation of the target dataset size, we fur-
ther sample a large number of random data points in the
output space and label them with different colors ac-
cording to their corresponding classes. This can clearlyvisualize the decision boundary that distinguishes be-
tween different class regions.
To this end, we map the given data samples into the trans-
formed space and embed the output logits or scores into a 2D
space using t-Distributed Stochastic Neighbor Embedding (t-
SNE) [16]. Figure 12a shows the results for 10 classes of
CIFAR-10. We can see that the given data samples have been
clearly classiﬁed into 10 classes and mapped to 10 different
regions. For the sake of analysis, we purposely zoom in four
different regions in the left of the whole space. From Fig-
ure 12b, we can make the following observations:
• The member samples and non-member samples belong-
ing to the same class are tightly divided into 2 clusters,
which explains why the previous score-based attacks
can achieve effective performance.
• More interestingly, we can see that the member sam-
ples are further away from the decision boundary than
the non-member samples, that is, the distance between
the members and the decision boundary is larger than
that of the non-members. Again, this validates our key
intuition.
Recall that in the decision change stage of boundary at-
tack, we apply black-box adversarial attack techniques to
change the ﬁnal model decision. Here, we give an intuitive
overview of how HopSkipJump and QEBA schemes work in
Figure 12d. As we can see, though these two schemes adopt
different strategies to ﬁnd the perturbed sample, there is one
10thing in common: The search ends at the tangent samples be-
tween the neighboring Lp-radius ball of the original sample
and its decision boundary. Only in this way they can mis-
lead the target model and also generate a small perturbation.
Combined with Figure 12b, we can ﬁnd that the magnitude
of perturbation is essentially a reﬂection of the distance from
the original sample to its decision boundary.
We again feed the 1,000 member samples and 1,000 non-
member samples to the shadow model (CIFAR-10, M-3),
and visualize its decision boundary in Figure 12c. In par-
ticular, we mark in red the misclassiﬁed samples from non-
members. First, looking at the correctly classiﬁed samples,
we can also ﬁnd that the member samples are relatively far
from the decision boundary, i.e., the loss is relatively lower
than that of non-member samples. As for the misclassiﬁed
samples, it is easy to see that their loss is much larger than
any other samples. Therefore, we can leverage the loss as
metric to differentiate members and non-members. How-
ever, we should also note that compared to Figure 12b, the
difference between members and non-members towards the
decision boundary is much smaller. Thus, if we do not
adopt loss metric which considers the ground truth label,
then the maximum conﬁdence scores Max(pi)and normal-
ized entropy 1
log(K)åipilog(pi)which are just based on self-
information will lead to a much lower difference between
members and non-members. This is the reason why the loss
metric achieves the highest performance.
Summarizing the above quantitative and qualitative anal-
ysis, we verify our argument that the distance between the
member sample and its decision boundary is larger than that
of the non-member sample, thus revealing the reasons for the
success of the membership inference, including score-based
and decision-based attacks. In addition, we verify that mem-
bership information remains transferable between the target
and shadow models. Last but not least, we also show the rea-
son why the loss metric of the transfer attack achieves the
best performance.
6 Defenses Evaluation
To mitigate the threat of membership leakage, a large body of
defense mechanisms have been proposed in the literature. In
this section, we evaluate the performance of current member-
ship inference attacks against the state-of-the-art defenses.
We summarize existing defenses in the following three broad
categories.
Generalization Enhancement. As overﬁtting is the ma-
jor reason for membership inference to be successful, multi-
ple approaches have been proposed with the aim of reducing
overﬁtting, which are ﬁrst introduced by the machine learn-
ing community to encourage generalization. The standard
generalization enhancement techniques, such as weight de-
cay (L1/L2 regularization) [46, 54], dropout [50], and data
augmentation, have been shown to limit overﬁtting effec-
tively, but may lead to a signiﬁcant decrease in model ac-
curacy.
Privacy Enhancement. Differential privacy [11, 17, 26] isa widely adopted for mitigating membership privacy. Many
differential privacy based defense techniques add noise to the
gradient to ensure the data privacy in the training process of
the ML model. A representative approach in this category
is DP-Adam [4], and we adopt an open-source version of its
implementation in our experiments.5
Conﬁdence Score Perturbation. Previous score-based at-
tacks have demonstrated that the conﬁdence score predicted
by the target model clearly presents membership signal.
Therefore, researchers have proposed several approaches to
alter the conﬁdence score. We focus on two representative
approaches in this category: MemGuard [27] and adversar-
ial regularization [38], which changes the output probability
distribution so that both members and non-members look like
similar examples to the inference model built by the adver-
sary. We adopt the original implementation of MemGuard,6
and an open-source version of the adversarial regularization.7
For each mechanism, we train 3 target models (CIFAR-10,
M 2) using different hyper-parameters. For example, in L2
regularization, the lused to constrain the regularization loss
is set to 0.01, 0.05, and 0.1, and the lin L1 regularization is
set to 0.0001, 0.001 and 0.005, respectively. In differential
privacy, the noise is randomly sampled from a Gaussian dis-
tribution N(e;b), wherein eis ﬁxed to 0 and bis set to 0.1,
0.5 and 1.1, respectively.
Table 6: Attack AUC performance under the defense of Mem-
Guard.
CIFAR-10, M-2 Face, M-2
Attack None MemGuard None MemGuard
score-based 0.8655 0.5151 0.755 0.513
baseline attack 0.705 0.705 0.665 0.665
transfer attack 0.7497 0.7497 0.6664 0.6664
boundary attack 0.8747 0.8747 0.8617 0.8617
We report the attack performance against models trained
with a wide variety of different defensive mechanisms in Fig-
ure 13, and we make the following observations.
• Our decision-based attacks. i.e., both transfer attack
and boundary attack, can bypass most types of defense
mechanisms.
• Strong differential privacy ( b=1.1), L1 regularization
(l=0:005) and L2 regularization ( l=0:1) can reduce
membership leakage but, as expected, lead to a signiﬁ-
cant degradation in the model’s accuracy. The reason is
that the decision boundary between members and non-
members is heavily blurred.
• Data augmentation can deﬁnitely reduce overﬁtting, but
it still does not reduce membership leakage. This is be-
cause data augmentation drives the model to strongly
remember both the original samples and their augmen-
tations.
5https://github.com/ebagdasa/pytorch-privacy
6https://github.com/jjy1994/MemGuard
7https://github.com/SPIN-UMass/ML-Privacy-Regulization
11In Table 6, we further compare the performance of all attacks
against MemGuard [27], which is the latest powerful defense
technique and can be easily deployed. We can ﬁnd that Mem-
Guard cannot defend against decision-based attacks at all,
but is very effective against previous score-based attacks.
7 Related Works
Various research has shown that machine learning models are
vulnerable to security and privacy attacks. In this section, we
mainly survey the domains that are most relevant to us.
Membership Inference. Membership inference attack has
been successfully performed in various data domains, rang-
ing form biomedical data [6, 22, 24] to mobility traces [43].
Shokri et al. [48] present the ﬁrst membership inference at-
tack against machine learning models. The general idea be-
hind this attack is to use multiple shadow models to gen-
erate data to train multiple attack models (one for each
class). These attack models take the target sample’s con-
ﬁdence scores as input and output its membership status,
i.e., member or non-member. Salem et al. [46] later present
another attack by gradually relaxing the assumptions made
by Shokri et al. [48] achieving a model and data inde-
pendent membership inference. In addition, there are sev-
eral other subsequent score-based membership inference at-
tacks [25, 31, 35, 49, 57]. In the area of decision-based at-
tacks, Yeom et al. [57] quantitatively analyzed the relation-
ship between attack performance and loss for training and
testing sets, and proposed the ﬁrst decision-based attack, i.e.,
baseline attack aforementioned. We also acknowledge that
a concurrent work [13] proposes an approach similar to our
boundary attack. Speciﬁcally, the concurrent work assumes
that an adversary has more knowledge of the target model,
including training knowledge (model architecture, training
algorithm, and training dataset size), and a shadow dataset
from the same distribution as the target dataset to estimate
the threshold. In our work, we relax all assumptions and
propose a general threshold-choosing method. We further
present a new perspective on the reasons for the success of
membership inference. In addition, we introduce a novel
transfer-attack.
Defenses Against Membership Inference. Researchers
have proposed to improve privacy against membership in-
ference via different types of generalization enhancement.
For example, Shokri et al. [48] adopted L2 regularization
with a polynomial in the model’s loss function to penal-
ize large parameters. Salem et al. [46] demonstrated two
effective method of defending MI attacks, namely dropout
and model stacking. Nasr et al. [38] introduced a defen-
sive conﬁdence score membership classiﬁer in a min-max
game mechanism to train models with membership privacy,
namely adversarial regularization. There are other existing
generalization enhancement method can be used to mitigate
membership leakage, such as L1 regularization and data aug-
mentation. Another direction is privacy enhancement. Many
differential privacy-based defenses [11, 17, 26] involve clip-
ping and adding noise to instance-level gradients and is de-signed to train a model to prevent it from memorizing train-
ing data or being susceptible to membership leakage. Shokri
et al. [48] designed a differential privacy method for collabo-
rative learning of DNNs. As for conﬁdence score alteration,
Jia et al. [27] introduce MemGuard, the ﬁrst defense with
formal utility-loss guarantees against membership inference.
The basic idea behind this work is to add carefully crafted
noise to conﬁdence scores of an ML model to mislead the
membership classiﬁer. Yang et al. [56] also propose a similar
defense in this direction.
Attacks against Machine Learning. Besides membership
inference attacks, there exist numerous other types of attacks
against ML models. A major attack type in this space is
adversarial examples [12, 30, 40–42, 52]. In this setting, an
adversary adds carefully crafted noise to samples aiming at
mislead the target classiﬁer. Ganju et al. [20] proposed a
property inference attack aiming at inferring general proper-
ties of the training data (such as the proportion of each class
in the training data). Model inversion attack [18, 19] focuses
on inferring the missing attributes of the target ML model.
A similar type of attacks is backdoor attack, where the ad-
versary as a model trainer embeds a trigger into the model
for her to exploit when the model is deployed [21, 34, 55].
Another line of work is model stealing, Tramèr et al. [53]
proposed the ﬁrst attack on inferring a model’s parame-
ters. Other works focus on protecting a model’s owner-
ship [5, 32, 44, 59].
8 Conclusion
In this paper, we perform a systematic investigation on mem-
bership leakage in label-only exposures of ML models, and
propose two novel decision-based membership inference at-
tacks, including transfer attack and boundary attack. Exten-
sive experiments demonstrate that our two attacks achieve
better performances than baseline attack, and even outper-
form prior score-based attacks in some cases. Furthermore,
we propose a new perspective on the reasons for the success
of membership inference and show that members samples are
further away from the decision boundary than non-members.
Finally, we evaluate multiple defense mechanisms against
our decision-based attacks and show that our novel attacks
can still achieve reasonable performance unless heavy reg-
ularization has been applied. In particular, our evaluation
demonstrates that conﬁdence score perturbation is an infea-
sible defense mechanism in label-only exposures.
Acknowledgements
This work is partially funded by the Helmholtz Association
within the project “Trustworthy Federated Data Analytics”
(TFDA) (funding number ZT-I-OO1 4).
12References
[1]https://www.cs.toronto.edu/~kriz/cifar.
html . 3, 16
[2]http://benchmark.ini.rub.de/?section=gtsrb .
3, 16
[3]http://vis-www.cs.umass.edu/lfw/ . 3, 16
[4] Martin Abadi, Andy Chu, Ian Goodfellow, Brendan
McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.
Deep Learning with Differential Privacy. In ACM
SIGSAC Conference on Computer and Communica-
tions Security (CCS) , pages 308–318. ACM, 2016. 2,
11
[5] Yossi Adi, Carsten Baum, Moustapha Cisse, Benny
Pinkas, and Joseph Keshet. Turning Your Weakness
Into a Strength: Watermarking Deep Neural Networks
by Backdooring. In USENIX Security Symposium
(USENIX Security) , pages 1615–1631. USENIX, 2018.
12
[6] Michael Backes, Pascal Berrang, Mathias Humbert,
and Praveen Manoharan. Membership Privacy in
MicroRNA-based Studies. In ACM SIGSAC Con-
ference on Computer and Communications Security
(CCS) , pages 319–330. ACM, 2016. 12
[7] Michael Backes, Mathias Humbert, Jun Pang, and Yang
Zhang. walk2friends: Inferring Social Links from
Mobility Proﬁles. In ACM SIGSAC Conference on
Computer and Communications Security (CCS) , pages
1943–1957. ACM, 2017. 3
[8] Battista Biggio, Igino Corona, Davide Maiorca, Blaine
Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giac-
into, and Fabio Roli. Evasion Attacks against Machine
Learning at Test Time. In European Conference on Ma-
chine Learning and Principles and Practice of Knowl-
edge Discovery in Databases (ECML/PKDD) , pages
387–402. Springer, 2013. 6
[9] Philippe Burlina, David E. Freund, B. Dupas, and
Neil M. Bressler. Automatic Screening of Age-related
Macular Degeneration and Retinal Abnormalities. In
Annual International Conference of the IEEE Engi-
neering in Medicine and Biology Society (EMBC) ,
pages 3962–3966. IEEE, 2011. 1
[10] Nicholas Carlini and David Wagner. Towards Evaluat-
ing the Robustness of Neural Networks. In IEEE Sym-
posium on Security and Privacy (S&P) , pages 39–57.
IEEE, 2017. 6
[11] Kamalika Chaudhuri, Claire Monteleoni, and Anand D
Sarwate. Differentially Private Empirical Risk Min-
imization. Journal of Machine Learning Research ,
2011. 11, 12
[12] Jianbo Chen, Michael I. Jordan, and Martin J. Wain-
wright. HopSkipJumpAttack: A Query-Efﬁcient
Decision-Based Attack. In IEEE Symposium on Secu-
rity and Privacy (S&P) , pages 1277–1294. IEEE, 2020.
6, 12[13] Christopher A. Choquette Choo, Florian Tramèr,
Nicholas Carlini, and Nicolas Papernot. Label-Only
Membership Inference Attacks. CoRR abs/2007.14321 ,
2020. 12
[14] Jeremy M. Cohen, Elan Rosenfeld, and J. Zico
Kolter. Certiﬁed Adversarial Robustness via Ran-
domized Smoothing. In International Conference on
Machine Learning (ICML) , pages 1310–1320. PMLR,
2019. 16
[15] Ambra Demontis, Marco Melis, Maura Pintor,
Matthew Jagielski, Battista Biggio, Alina Oprea,
Cristina Nita-Rotaru, and Fabio Roli. Why Do Ad-
versarial Attacks Transfer? Explaining Transferabil-
ity of Evasion and Poisoning Attacks. In USENIX Se-
curity Symposium (USENIX Security) , pages 321–338.
USENIX, 2019. 3
[16] Laurens Van der Maaten and Geoffrey Hinton. Visual-
izing Data Using t-SNE. Journal of Machine Learning
Research , 2008. 10
[17] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and
Adam Smith. Calibrating Noise to Sensitivity in Private
Data Analysis. In Theory of Cryptography Conference
(TCC) , pages 265–284. Springer, 2006. 11, 12
[18] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.
Model Inversion Attacks that Exploit Conﬁdence Infor-
mation and Basic Countermeasures. In ACM SIGSAC
Conference on Computer and Communications Secu-
rity (CCS) , pages 1322–1333. ACM, 2015. 12
[19] Matt Fredrikson, Eric Lantz, Somesh Jha, Simon Lin,
David Page, and Thomas Ristenpart. Privacy in Phar-
macogenetics: An End-to-End Case Study of Personal-
ized Warfarin Dosing. In USENIX Security Symposium
(USENIX Security) , pages 17–32. USENIX, 2014. 3,
12
[20] Karan Ganju, Qi Wang, Wei Yang, Carl A. Gunter, and
Nikita Borisov. Property Inference Attacks on Fully
Connected Neural Networks using Permutation Invari-
ant Representations. In ACM SIGSAC Conference on
Computer and Communications Security (CCS) , pages
619–633. ACM, 2018. 12
[21] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Grag.
Badnets: Identifying Vulnerabilities in the Machine
Learning Model Supply Chain. CoRR abs/1708.06733 ,
2017. 12
[22] Inken Hagestedt, Yang Zhang, Mathias Humbert, Pas-
cal Berrang, Haixu Tang, XiaoFeng Wang, and Michael
Backes. MBeacon: Privacy-Preserving Beacons for
DNA Methylation Data. In Network and Distributed
System Security Symposium (NDSS) . Internet Society,
2019. 3, 12
[23] Benjamin Hilprecht, Martin Härterich, and Daniel
Bernau. Monte Carlo and Reconstruction Membership
13Inference Attacks against Generative Models. Sympo-
sium on Privacy Enhancing Technologies Symposium ,
2019. 1
[24] Nils Homer, Szabolcs Szelinger, Margot Redman,
David Duggan, Waibhav Tembe, Jill Muehling, John V .
Pearson, Dietrich A. Stephan, Stanley F. Nelson, and
David W. Craig. Resolving Individuals Contribut-
ing Trace Amounts of DNA to Highly Complex Mix-
tures Using High-Density SNP Genotyping Microar-
rays. PLOS Genetics , 2008. 12
[25] Bo Hui, Yuchen Yang, Haolin Yuan, Philippe Burlina,
Neil Zhenqiang Gong, and Yinzhi Cao. Practical Blind
Membership Inference Attack via Differential Compar-
isons. In Network and Distributed System Security Sym-
posium (NDSS) . Internet Society, 2021. 1, 3, 5, 12
[26] Roger Iyengar, Joseph P. Near, Dawn Xiaodong Song,
Om Dipakbhai Thakkar, Abhradeep Thakurta, and Lun
Wang. Towards Practical Differentially Private Convex
Optimization. In IEEE Symposium on Security and Pri-
vacy (S&P) , pages 299–316. IEEE, 2019. 11, 12
[27] Jinyuan Jia, Ahmed Salem, Michael Backes, Yang
Zhang, and Neil Zhenqiang Gong. MemGuard: De-
fending against Black-Box Membership Inference At-
tacks via Adversarial Examples. In ACM SIGSAC
Conference on Computer and Communications Secu-
rity (CCS) , pages 259–274. ACM, 2019. 1, 2, 3, 11,
12
[28] Ira Kemelmacher-Shlizerman, Steven M. Seitz, Daniel
Miller, and Evan Brossard. The MegaFace Benchmark:
1 Million Faces for Recognition at Scale. In IEEE Con-
ference on Computer Vision and Pattern Recognition
(CVPR) , pages 4873–4882. IEEE, 2016. 1
[29] Konstantina Kourou, Themis P. Exarchos, Konstanti-
nos P. Exarchos, Michalis V . Karamouzis, and Dim-
itrios I. Fotiadis. Machine Learning Applications in
Cancer Prognosis and Prediction. Computational and
Structural Biotechnology Journal , 2015. 1
[30] Huichen Li, Xiaojun Xu, Xiaolu Zhang, Shuang Yang,
and Bo Li. QEBA: Query-Efﬁcient Boundary-Based
Blackbox Attack. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 1218–
1227. IEEE, 2020. 6, 12
[31] Jiacheng Li, Ninghui Li, and Bruno Ribeiro. Mem-
bership Inference Attacks and Defenses in Supervised
Learning via Generalization Gap. In ACM Conference
on Data and Application Security and Privacy (CO-
DASPY) , pages 5–16. ACM, 2021. 1, 3, 5, 12
[32] Zheng Li, Chengyu Hu, Yang Zhang, and Shanqing
Guo. How to Prove Your Model Belongs to You: A
Blind-Watermark based Framework to Protect Intellec-
tual Property of DNN. In Annual Computer Secu-
rity Applications Conference (ACSAC) , pages 126–137.
ACM, 2019. 12
[33] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song.
Delving into Transferable Adversarial Examples and
Black-box Attacks. CoRR abs/1611.02770 , 2016. 3[34] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan
Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang.
Trojaning Attack on Neural Networks. In Network and
Distributed System Security Symposium (NDSS) . Inter-
net Society, 2019. 12
[35] Yunhui Long, Vincent Bindschaedler, and Carl A.
Gunter. Towards Measuring Membership Privacy.
CoRR abs/1712.09136 , 2017. 1, 2, 3, 4, 5, 12
[36] Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue
Bu, Xiaofeng Wang, Haixu Tang, Carl A. Gunter,
and Kai Chen. Understanding Membership Infer-
ences on Well-Generalized Learning Models. CoRR
abs/1802.04889 , 2018. 1
[37] Muzammal Naseer, Salman H. Khan, Muham-
mad Haris Khan, Fahad Shahbaz Khan, and Fatih
Porikli. Cross-Domain Transferability of Adversarial
Perturbations. In Annual Conference on Neural Infor-
mation Processing Systems (NeurIPS) , pages 12885–
12895. NeurIPS, 2019. 3
[38] Milad Nasr, Reza Shokri, and Amir Houmansadr. Ma-
chine Learning with Membership Privacy using Adver-
sarial Regularization. In ACM SIGSAC Conference on
Computer and Communications Security (CCS) , pages
634–646. ACM, 2018. 1, 2, 11, 12
[39] Nicolas Papernot, Patrick McDaniel, and Ian Goodfel-
low. Transferability in Machine Learning: from Phe-
nomena to Black-Box Attacks using Adversarial Sam-
ples. CoRR abs/1605.07277 , 2016. 3
[40] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha,
and Michael Wellman. SoK: Towards the Science of
Security and Privacy in Machine Learning. In IEEE
European Symposium on Security and Privacy (Euro
S&P) , pages 399–414. IEEE, 2018. 12
[41] Nicolas Papernot, Patrick D. McDaniel, Ian Goodfel-
low, Somesh Jha, Z. Berkay Celik, and Ananthram
Swami. Practical Black-Box Attacks Against Machine
Learning. In ACM Asia Conference on Computer and
Communications Security (ASIACCS) , pages 506–519.
ACM, 2017. 6, 12
[42] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha,
Matt Fredrikson, Z. Berkay Celik, and Ananthram
Swami. The Limitations of Deep Learning in Adversar-
ial Settings. In IEEE European Symposium on Security
and Privacy (Euro S&P) , pages 372–387. IEEE, 2016.
6, 12
[43] Apostolos Pyrgelis, Carmela Troncoso, and Emil-
iano De Cristofaro. Knock Knock, Who’s There?
Membership Inference on Aggregate Location Data. In
Network and Distributed System Security Symposium
(NDSS) . Internet Society, 2018. 3, 12
[44] Bita Darvish Rouhani, Huili Chen, and Farinaz
Koushanfar. DeepSigns: A Generic Watermarking
Framework for IP Protection of Deep Learning Mod-
els.CoRR abs/1804.00750 , 2018. 12
14[45] Alexandre Sablayrolles, Matthijs Douze, Cordelia
Schmid, Yann Ollivier, and Hervé Jégou. White-box vs
Black-box: Bayes Optimal Strategies for Membership
Inference. In International Conference on Machine
Learning (ICML) , pages 5558–5567. PMLR, 2019. 1
[46] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal
Berrang, Mario Fritz, and Michael Backes. ML-Leaks:
Model and Data Independent Membership Inference
Attacks and Defenses on Machine Learning Models. In
Network and Distributed System Security Symposium
(NDSS) . Internet Society, 2019. 1, 2, 3, 4, 5, 8, 11, 12
[47] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octa-
vian Suciu, Christoph Studer, Tudor Dumitras, and Tom
Goldstein. Poison Frogs! Targeted Clean-Label Poison-
ing Attacks on Neural Networks. In Annual Conference
on Neural Information Processing Systems (NeurIPS) ,
pages 6103–6113. NeurIPS, 2018. 6
[48] Reza Shokri, Marco Stronati, Congzheng Song, and Vi-
taly Shmatikov. Membership Inference Attacks Against
Machine Learning Models. In IEEE Symposium on Se-
curity and Privacy (S&P) , pages 3–18. IEEE, 2017. 1,
2, 3, 4, 5, 12
[49] Liwei Song, Reza Shokri, and Prateek Mittal. Privacy
Risks of Securing Machine Learning Models against
Adversarial Examples. In ACM SIGSAC Conference on
Computer and Communications Security (CCS) , pages
241–257. ACM, 2019. 1, 2, 3, 4, 5, 12
[50] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A
Simple Way to Prevent Neural Networks from Overﬁt-
ting. Journal of Machine Learning Research , 2014. 2,
11
[51] Mary H. Stanﬁll, Margaret Williams, Susan H. Fenton,
Robert A. Jenders, and William R. Hersh. A System-
atic Literature Review of Automated Clinical Coding
and Classiﬁcation Systems. J. Am. Medical Informatics
Assoc. , 2010. 1
[52] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian
Goodfellow, Dan Boneh, and Patrick McDaniel. En-
semble Adversarial Training: Attacks and Defenses. In
International Conference on Learning Representations
(ICLR) , 2017. 6, 12
[53] Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Re-
iter, and Thomas Ristenpart. Stealing Machine Learn-ing Models via Prediction APIs. In USENIX Secu-
rity Symposium (USENIX Security) , pages 601–618.
USENIX, 2016. 12
[54] Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu,
and Wenqi Wei. Towards Demystifying Membership
Inference Attacks. CoRR abs/1807.09173 , 2018. 1, 2,
4, 11
[55] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li,
Bimal Viswanath, Haitao Zheng, and Ben Y . Zhao.
Neural Cleanse: Identifying and Mitigating Backdoor
Attacks in Neural Networks. In IEEE Symposium on
Security and Privacy (S&P) , pages 707–723. IEEE,
2019. 12
[56] Ziqi Yang, Bin Shao, Bohan Xuan, Ee-Chien Chang,
and Fan Zhang. Defending Model Inversion and Mem-
bership Inference Attacks via Prediction Puriﬁcation.
CoRR abs/2005.03915 , 2020. 1, 2, 12
[57] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and
Somesh Jha. Privacy Risk in Machine Learning: An-
alyzing the Connection to Overﬁtting. In IEEE Com-
puter Security Foundations Symposium (CSF) , pages
268–282. IEEE, 2018. 1, 3, 4, 5, 12
[58] Runtian Zhai, Chen Dan, Di He, Huan Zhang, Bo-
qing Gong, Pradeep Ravikumar, Cho-Jui Hsieh, and Li-
wei Wang. MACER: Attack-free and Scalable Robust
Training via Maximizing Certiﬁed Radius. In Interna-
tional Conference on Learning Representations (ICLR) ,
2020. 9
[59] Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu,
Marc Ph. Stoecklin, Heqing Huang, and Ian Molloy.
Protecting Intellectual Property of Deep Neural Net-
works with Watermarking. In ACM Asia Conference on
Computer and Communications Security (ASIACCS) ,
pages 159–172. ACM, 2018. 12
[60] Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao
Wang, Bo Li, and Dawn Song. The Secret Re-
vealer: Generative Model-Inversion Attacks Against
Deep Neural Networks. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages
250–258. IEEE, 2020. 1
[61] Tianyue Zheng, Weihong Deng, and Jiani Hu. Cross-
Age LFW: A Database for Studying Cross-Age Face
Recognition in Unconstrained Environments. CoRR
abs/1708.08197 , 2017. 1
15A Appendix
A.1 Datasets Description
CIFAR-10/CIFAR-100. CIFAR-10 [1] and CIFAR-100 [1]
are benchmark datasets used to evaluate image recognition
algorithms. CIFAR-10 is composed of 32 32 color images
in 10 classes, with 6000 images per class. In total, there
are 50000 training images and 10000 test images. CIFAR-
100 has the same format as CIFAR-10, but it has 100 classes
containing 600 images each. There are 500 training images
and 100 testing images per class.
GTSRB. The GTSRB [2] dataset is an image collection con-
sisting of 43 trafﬁc signs. Images vary in size and are RGB-
encoded. It consists of over 51,839 color images, whose di-
mensions range from 15 15 to 250250 pixels (not neces-
sarily square). Of these 51,839 images, 39,209 are used for
training, and 12,630 are used for testing. Due to the varying
sizes of the images, the images are resized to 64 64 before
being passed to the model for classiﬁcation.
Face. The Face [3] dataset consists of about 13,000 images
of human faces crawled from the web. It is collected from
1,680 participants with each participant having at least two
distinct images in the dataset. In our evaluation, we only
consider people with more than 40 images, which leaves us
with 19 people’s data, i.e., 19 classes. The Face dataset is
challenging for facial recognition, as the images are taken
from the web and not under a controlled environment, such
as a lab. It is also worth noting that this dataset is unbalanced.
A.2 Certiﬁed Radius
Randomized Smoothing. In this work, we apply a recent
technique, called randomized smoothing [14], which can be
extended to any architecture to obtain the certiﬁed radius of
smoothed deep neural networks. The core of randomized
smoothing is to use the smoothed version of M, which is
denoted by G, to make predictions. The formulation of Gis
deﬁned as follows.
Deﬁnition 1. For an arbitrary classiﬁer Mands>0, the
smoothed classiﬁer GofMis deﬁned as
G(x) =argmax
c2YPeN(0;s2I)(M(x+e) =c): (4)
In short, the smoothed classiﬁer Greturns the label most
likely to be returned by Mwhen its input is sampled from
a Gaussian distribution N(x;s2I)centered at x. Cohen et
al. [14] prove the following theorem, which provides an ana-
lytic form of certiﬁed radius:
Theorem 1. [14] Let M:x!y, andeN(0;s2I). Let the
smoothed classiﬁer Gbe deﬁned as in (4). Let the ground
truth of an input xbey. IfGclassiﬁes xcorrectly, i.e.,
Pe(M(x+e) =y)max
y06=yPe(M(x+e) =y0): (5)Then, Gis provably robust at x, with the certiﬁed radius
given by
CR(G;x;y) =s
2[F 1(Pe(M(x+e) =y))
 F 1(max
y06=yPe(M(x+e) =y0))]
=s
2[F 1(Ee1fM(x+e)=yg)
 F 1(max
y06=yEe1fM(x+e)=y0g)]; (6)
where Fis the c:d:f:of the standard Gaussian distribution.
Table 7: Dataset splitting strategy. Dtrain is used to train the
target model and serves as the members, while the other Dtest
serves as the non-members. Dshadow is used to train the shadow
model after relabelled by the target model.
Target CIFAR10 CIFAR100 GTSRB Face
Model DtrainDtestDtrainDtestDtrainDtestDtrainDtest
M-0 3000 1000 7000 1000 600 500 350 100
M-1 2000 1000 6000 1000 500 500 300 100
M-2 1500 1000 5000 1000 400 500 250 100
M-3 1000 1000 4000 1000 300 500 200 100
M-4 500 1000 3000 1000 200 500 150 100
M-5 100 1000 2000 1000 100 500 100 100
Shadow Dshadow
Model 46000 42000 38109 1417
Algorithm 1: Transfer attack algorithm.
Input: shadow dataset Dshadow , shadow model S,
target model M, a candidate sample (x;y),
threshold t, minibatch m, membership
indicator T;
Output: Trained shadow model S,xis member or
not;
1Initialize the parameters of shadow ;
2Relabel Dshadow by querying to M;
3fornumber of training epochs do
4 fori=1;ijDshadowj
m;i++ do
5 sample minibatch of msamples from Dshadow ;
6 update Sby descending its adam gradient
7 end
8end
9Feed xintoSto obtain pi;
10calculate loss: l= åK
i=01ylog(pi);
11ifltthen
12 T= 1; ; /\*xis a member \*/
13else
14 T= 0; ; /\*xis a non-member \*/
15end
16return S,T;
160 1 2 3 4 5
Target Model 
0.01.02.03.04.0L2 Distance
Member
Non-Member(a) CIFAR-10
0 1 2 3 4 5
Target Model 
0.01.02.03.0L2 Distance
 (b) CIFAR-100
0 1 2 3 4 5
Target Model 
0.02.04.06.0L2 Distance
 (c) GTSRB
0 1 2 3 4 5
Target Model 
0.01.02.03.04.05.06.0L2 Distance
QEBA (d) Face
Figure 14: L2distance between the original sample and its perturbed samples generated by the QEBA attack. The x-axis represents
the target model being attacked and the y-axis represents the L2distance.
0 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC
L0
L1
L2
L
(a) CIFAR-10
0 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC (b) CIFAR-100
0 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC (c) GTSRB
0 1 2 3 4 5
Target Model 
0.00.20.40.60.81.0AUC
QEBA (d) Face
Figure 15: Attack AUC for four different Lpdistances between the original sample and its perturbed samples generated by the QEBA
attack. The x-axis represents the target model being attacked and the y-axis represents the AUC score.
Algorithm 3: Threshold choosing for boundary at-
tack.
Input: adversarial attack technique HopSkipJump ,
target model M; Gaussian distribution
N(e;b), queue q, top tpercentile;
Output: threshold t;
1Initialize q;
2Sample multiple random samples XfromN(e;b)for
number of random samples do
3 Select one sample x2X;
4 Feed xintoMto obtain predicted label y;
5 fornumber of query do
6 Apply HopSkipJump to perturb xto obtain x0;
7 Feed x0intoMto obtain predicted label y0;
8 ify06=ythen
9 pushjx x0j2intoq; break;
10 else
11 x=x0;
12 end
13 end
14end
15sortqin descending order;
16t=q(t);
17return t;Algorithm 2: Boundary attack algorithm.
Input: adversarial attack technique HopSkipJump ,
target model M, a candidate sample (x;y),
threshold t, membership indicator T;
Output: xis member or not;
1fornumber of query do
2 Feed xintoMto obtain predicted label y0;
3 ify06=ythen
4 x0= x; ; /\* perturbed sample x0\*/
5 else
6 Apply HopSkipJump to perturb x;
7 end
8end
9calculate perturbation P=jx x0j2;
10ifPtthen
11 T= 0; ; /\*xis a non-member \*/
12else
13 T= 1; ; /\*xis a member \*/
14end
15return T;
17