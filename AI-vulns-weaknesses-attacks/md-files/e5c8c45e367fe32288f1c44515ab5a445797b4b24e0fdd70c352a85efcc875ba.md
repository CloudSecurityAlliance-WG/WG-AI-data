1
Adversarial Robustness Assessment:
Why both L0andL1Attacks Are Necessary
Shashank Kotyan and Danilo Vasconcellos Vargas
Abstract ‚ÄîThere exists a vast number of adversarial attacks
and defences for machine learning algorithms of various types
which makes assessing the robustness of algorithms a daunting
task. To make matters worse, there is an intrinsic bias in these
adversarial algorithms. Here, we organise the problems faced:
a) Model Dependence, b) InsufÔ¨Åcient Evaluation, c) False
Adversarial Samples, and d) Perturbation Dependent Results).
Based on this, we propose a model agnostic dual quality
assessment method, together with the concept of robustness
levels to tackle them. We validate the dual quality assessment
on state-of-the-art neural networks (WideResNet, ResNet,
AllConv, DenseNet, NIN, LeNet and CapsNet) as well as
adversarial defences for image classiÔ¨Åcation problem. We
further show that current networks and defences are
vulnerable at all levels of robustness. The proposed robustness
assessment reveals that depending on the metric used (i.e., L0
orL1), the robustness may vary signiÔ¨Åcantly. Hence, the
duality should be taken into account for a correct evaluation.
Moreover, a mathematical derivation, as well as a
counter-example, suggest that L1andL2metrics alone are not
sufÔ¨Åcient to avoid spurious adversarial samples. Interestingly,
the threshold attack of the proposed assessment is a novel L1
black-box adversarial method which requires even less
perturbation than the One-Pixel Attack (only 12% of One-Pixel
Attack‚Äôs amount of perturbation) to achieve similar results.
Index Terms ‚ÄîDeep Learning, Neural Networks, Adversarial
Attacks, Few-Pixel Attack, Threshold Attack
I. I NTRODUCTION
NEURAL networks have empowered us to obtain high
accuracy in several applications like speech recognition
and face recognition. Most of these applications are only
feasible by the aid of neural networks. Despite these
accomplishments, neural networks have been shown to
misclassify if small perturbations are added to original
samples, called adversarial samples. Further, these
adversarial samples exhibit that conventional neural network
architectures are not capable of understanding concepts or
high-level abstractions as we earlier speculated.
Security and safety risks created by these adversarial
samples is also prohibiting the use of neural networks in
many critical applications such as autonomous vehicles.
Therefore, it is of utmost signiÔ¨Åcance to formulate not only
accurate but robust neural networks. However, to do so, a
quality assessment is required, which would let robustness to
be evaluated efÔ¨Åciently without in-depth knowledge of
adversarial machine learning.
S. Kotyan and D.V . Vargas are with the Laboratory of Intelligent Systems,
Department of Informatics, Kyushu University, Japan. http://lis.inf.kyushu-u.
ac.jp/. E-mail:vargas@inf.kyushu-u.ac.jpRegarding the development of a quality assessment for
robustness, the Ô¨Åeld of adversarial machine learning has
provided some tools which could be useful for the
development. However, the sheer amount of scenarios,
adversarial attacking methods, defences and metrics ( L0,L1,
L2andL1) make the current state-of-the-art difÔ¨Åcult to
perceive. Moreover, most of the contemporary adversarial
attacks are white-box ones which can not be used to assess
hybrids, non-standard neural networks and other classiÔ¨Åers in
general. Giving the vast amount of possibilities and many
deÔ¨Ånitions with their exceptions and trade-offs, it turns out
that a simple robustness quality assessment is a daunting
task.
Moreover, adversarial samples point out to reasoning
shortcomings in machine learning. Improvements in
robustness should also result in learning systems that can
better reason over data as well as achieve a new level of
abstraction. Therefore, a quality assessment procedure would
also be helpful in this regard, checking for failures in both
reasoning and high-level abstractions.
Therefore, to create a quality assessment procedure, we
formalise some of the problems which must be tackled:
P1 Model Dependence: A model agnostic quality
assessment is crucial to enable neural networks to be
compared with other approaches which may be completely
different (logic hybrids and evolutionary hybrids).
P2 InsufÔ¨Åcient Evaluation: There are several types of
adversarial samples as well as potential attack variations
and scenarios each with their own bias. The attacks also
differ substantially depending on metrics optimized, namely
L0,L1,L2andL1. However, not all of them are vital for
the evaluation of robustness. A quality assessment should
have few but sufÔ¨Åcient tests to provide an in-depth analysis
without compromising its utility.
P3 False Adversarial Samples: Adversarial attacks are
known sometimes to produce misleading adversarial
samples (samples that can not be recognised even by a
human observer) seldomly. Such deceptive adversarial
samples can only be detected through inspection, which
causes the evaluation to be error-prone. Both the need for
inspection, together with the feasibility of fraudulent
adversarial samples, should not be present.
P4 Perturbation Dependent Results: Varying amount of
perturbation leads to varying adversarial accuracy.
Moreover, networks differ in their sensitivity to attacks
given a varied amount of perturbation. Consequently, this
might result in double standards or hide importantarXiv:1906.06026v3 [cs.LG] 16 Jul 20202
Few-Pixel ( L0) Attack
Threshold ( L1) Attack
Fig. 1: Adversarial samples found with Few-pixel ( L0) black-box attack and threshold ( L1) black-box attack.
information.
In this article, we propose a quality assessment to tackle the
problems mentioned above with the following features:
Non-gradient based Black-box Attack (Address P1):
Black-box attacks are desirable for a model agnostic
evaluation which does not depend on speciÔ¨Åc features of
the learning process such as gradients. Therefore, here, the
proposed quality assessment is based on black-box attacks,
one of which is a novel L1black-box attack. In fact, to
the knowledge of the authors, this is the Ô¨Årst L1black-boxAttack that does not make any assumptions over the target
machine learning system. Figure 1 show some adversarial
samples crafted with the L0andL1black-box Attacks
used in the quality assessment.
Dual Evaluation (Address P2 and P3): We propose to use
solely attacks based on L0andL1to avoid creating
adversarial samples which are not correctly classiÔ¨Åed by
human beings after modiÔ¨Åcation. These metrics impose a
constraint over the spatial distribution of noise which
guarantees the quality of the adversarial sample. In Section3
IV, this is explained mathematically as well as illustrated
with a counter-example.
Robustness Levels (Address P4): In this article, we deÔ¨Åne
robustness levels in terms of the constraint‚Äôs threshold th.
We then compare multiple robustness levels of results with
their respective values at the same robustness level.
Robustness levels constrain the comparison of equal
perturbation, avoiding the comparison of results with
different degrees of perturbation (Problem P4). In fact,
robustness levels add a concept which may aid in the
classiÔ¨Åcation of algorithms. For example, an algorithm
which is robust to One-Pixel Attack belongs to the
1-pixel-safe category.
II. R ELATED WORKS
Recently, it was exhibited that neural networks contain
many vulnerabilities. The Ô¨Årst article on the topic dates back
to2013 when it was revealed that neural networks behave
oddly for almost the same images [1]. Afterwards, a series
of vulnerabilities were found and exploited by the use of
adversarial attacks. In [2], the authors demonstrated that
neural networks show high conÔ¨Ådence when presented with
textures and random noise. Adversarial perturbations which
can be added to most of the samples to fool a neural
network was shown to be possible [3]. Patches can also
make them misclassify, and the addition of them in an image
turn it into a different class [4]. Moreover, an extreme attack
was shown to be effective in which it is possible to make
neural networks misclassify with a single-pixel change [5].
Many of these attacks can be easily made into real-world
threats by printing out adversarial samples, as shown in [6].
Moreover, carefully crafted glasses can also be made into
attacks [7]. Alternatively, even general 3D adversarial objects
were shown possible [8].
Regarding understanding the phenomenon, it is argued in
[9] that neural networks‚Äô linearity is one of the main reasons.
Another recent investigation proposes the conÔ¨Çicting saliency
added by adversarial samples as the reason for
misclassiÔ¨Åcation [10].
Many defensive systems and detection systems have also
been proposed to mitigate some of the problems. However,
there are still no current solutions or promising ones which
can negate the adversarial attacks consistently. Regarding
defensive systems, defensive distillation in which a smaller
neural network squeezes the content learned by the original
one was proposed as a defence [11]. However, it was shown
not to be robust enough in [12]. Adversarial training was
also proposed, in which adversarial samples are used to
augment the training dataset [9], [13], [14]. Augmentation of
the dataset is done in such a way that the neural network
should be able to classify the adversarial samples, increasing
its robustness. Although adversarial training can increase the
robustness slightly, the resulting neural network is still
vulnerable to attacks [15]. There are many recent variations
of defenses [16], [17], [18], [19], [20], [21], [22], [23] which
are carefully analysed and many of their shortcomings are
explained in [24], [25].Regarding detection systems, a study from [26]
demonstrated that indeed some adversarial samples have
different statistical properties which could be exploited for
detection. In [21], the authors proposed to compare the
prediction of a classiÔ¨Åer with the prediction of the same
input but ‚Äùsqueezed‚Äù. This technique allowed classiÔ¨Åers to
detect adversarial samples with small perturbations. Many
detection systems fail when adversarial samples deviate from
test conditions [27], [28]. Thus, the clear beneÔ¨Åts of
detection systems remain inconclusive.
III. A DVERSARIAL MACHINE LEARNING AS
OPTIMISATION PROBLEM
Adversarial machine learning can be perceived as a
constrained optimisation problem. Before deÔ¨Åning it, let us
formalise adversarial samples Ô¨Årst. Let f(x)2[ [0;1] ]be the
output of a machine learning algorithm in binary
classiÔ¨Åcation setting. Extrapolating the algorithm in
multi-label classiÔ¨Åcation setting, the output can be deÔ¨Åned as
f(x)2[ [1::N] ]. Here,x2Rkis the input of the algorithm
for the input of size kandNis the number of classes in
whichxcan be classiÔ¨Åed. An adversarial sample x0for an
original sample xcan be thus, deÔ¨Åned as follows:
x0=x+xsuch thatf(x0)6=f(x)
in whichx2Rkis a small perturbation added to the input.
Therefore, adversarial machine learning can be deÔ¨Åned as an
optimization problem1:
minimize
xg(x+x)csubject tokxkth
wherethis a pre-deÔ¨Åned threshold value and g()cis the soft-
label or the conÔ¨Ådence for the correct class csuch thatf(x) =
argmaxg(x).
The constraint in the optimisation problem has the
objective of disallowing perturbations which could make x
unrecognisable or change its correct class. Therefore, the
constraint is itself a mathematical deÔ¨Ånition of what
constitutes an imperceptible perturbation. Many different
norms are used in the literature (e.g., L0,L1,L2andL1).
Intuitively, the norms allow for different types of attacks.
For simplicity, we are narrowing the scope of this article to
the image classiÔ¨Åcation problem alone. However, the proposed
attacks and the quality assessment can be also be extended to
other problems as well.
IV. G UARANTEEING THE QUALITY OF ADVERSARIAL
SAMPLES
Constraining the perturbation is decisive in adversarial
samples to avoid producing samples that can not be
recognised by human beings or samples that have, by the
amount of perturbation, changed its correct class. However,
restraining the total amount of perturbation is not enough as
a small amount of perturbation concentrated in a few pixels
might be able to create false adversarial samples. Therefore,
1Here the deÔ¨Ånition will only concern untargeted attacks in classiÔ¨Åcation
setting but a similar optimization problem can be deÔ¨Åned for targeted attacks4
Fig. 2: Example of a false adversarial sample (right) and its respective
original sample (left). The false adversarial sample is built with few
total perturbations (i.e., low L1andL2) but with unrecognisable
Ô¨Ånal image (false adversarial sample). This is a result of the non-
constrained spatial distribution of perturbations which is prevented if
lowL0orL1is used. This hypothetical attack has a L2of merely
356, well below the maximum L2for the One-Pixel ( L01) Attack
(765).
a spatial constraint over the perturbation of pixels Pwould
be a desirable feature.
This can be achieved mathematically as follows: Given an
imagexand its perturbed counterpart x0, it is possible to
calculateL1norm between the original by the Manhattan
distance of both matrices: kx x0k1. Constraining L1to be
less than a certain number does not guarantee any spatial
distribution constraint. Let us deÔ¨Åne a set based on all
non-zero pixel perturbations as follows:
Nz=fPi:kPi P0
ik1>0gwherePiandP0
iare pixels
from respectively the original image xand the perturbed
imagex0andiis an image index. Both Nzand its
cardinalityjNzjhas information about the spatial distribution
of perturbations and constraining any of these values would
result in a spatially limited perturbation.
Provided that this low enough, a modiÔ¨Åcation preserving
the white noise of that intensity would bound jNzj< th .
Moreover,jNzjis preciseL0, demonstrating that L0is based
on the set Nz, which stores spatial information about the
differences. At the same time, L1uses the Manhattan norm,
which does not have this information. Similarly, the L1
norm can be rewritten as the following optimisation
constraint:8Pi2x;kPi P0
ik1thNotice that this
constraint is also deÔ¨Åned over the spatial distribution of
perturbations.
Figure 2 gives empirical evidence of misleading
adversarial sample of an image that is constrained by
L2765. Notice that this value is precisely the maximum
change of one pixel, i.e., the maximum possible perturbation
of the One-Pixel attack ( L01) which when no limits are
imposed over its spatial distribution may create false
adversarial samples.
The reasoning behind the L0andL1are as follows, without
altering much the original sample, attacks can perturb a few
pixels strongly ( L0), all pixels slightly ( L1) or a mix of both
(L1andL2). The hurdle is that L1andL2which mix both
strategies vary strongly with the size of images, if not used
with caution may cause unrecognisable adversarial samples
(Problem P3). Also, it is difÔ¨Åcult to compare between methods
usingL1andL2norm because the amount of perturbationswill often differ (Problem P4).
Threshold Attack ( L1black-box Attack): The threshold
attack optimizes the constrained optimization problem with
the constraintkxk1th, i.e., it uses the L1norm. The
algorithm search in Rkspace as the search space is the
same as the input space. This is because the variables can
be any variation of the input as long as the threshold is
respected. In image classiÔ¨Åcation problem k=mnc
wheremnis the size, and cis the number of channels of
the image.
Few-Pixel Attack ( L0black-box Attack): The few-pixel
attack is a variation of our previous proposed attack, the
One-Pixel Attack [5]. It optimizes the constrained
optimization problem by using the constraint kxk0th,
i.e., it uses the L0norm. The search variable is a
combination of pixel values (depending on channels cin
the image) and position ( 2values X, Y) for all of the pixels
(thpixels). Therefore, the search space is smaller than the
threshold attack deÔ¨Åned below with dimensions of
R(2+c)th.
Robustness Levels: Here we propose robustness levels, as
machine learning algorithms might perform differently to
varying amount of perturbations. Robustness levels evaluate
classiÔ¨Åers in a couple of ththresholds. Explicitly, we deÔ¨Åne
four levels of robustness 1;3;5;10for both of our L0Norm
Attack and L1Norm Attack. We then name them
respectively pixel and threshold robustness levels.
Algorithms that pass a level of robustness ( 0%adversarial
accuracy) are called level-threshold-safe or level-pixel-safe.
For example, an algorithm that passes the level-one in
threshold (L1) attack is called 1-threshold-safe.
V. E XPERIMENTAL RESULTS AND DISCUSSIONS
In this section, we aim to validate the dual quality
assessment2empirically as well as analyse the current
state-of-the-art neural networks in terms of robustness.
Preliminary Tests (Section V-B): Tests on two
state-of-the-art neural networks are presented (ResNet [29]
and CapsNet [30]). These tests are done to choose the
black-box optimisation algorithm to be used for the further
sections. The performance of both Differential Evolution
(DE) [31] and Covariance Matrix Adaptation Evolution
Strategy (CMA-ES) [32] are evaluated.
Evaluating Learning and Defense Systems (Section V-C):
Tests are extended to the seven different state-of-the-art
neural networks - WideResNet [33], DenseNet [34], ResNet
[29], Network in Network (NIN) [35], All Convolutional
Network (AllConv) [36], CapsNet [30], and LeNet [37]. We
also evaluate three adversarial defences applied to the
standard ResNet architecture - Adversarial training (AT)
[14], Total Variance Minimization (TVM) [19], and Feature
Squeezing (FS) [21]. We have chosen defences based on
entirely different principles to be tested. In this way, the
results achieved here can be extended to other similar types
of defences in the literature.
2Code is available at http://bit.ly/DualQualityAssessment5
Attack Parameters
FGM norm =L1,= 8,step= 2
BIM norm =L1,= 8,step= 2, iterations = 10
PGD norm =L1,= 8,step= 2, iterations = 20
DeepFool iterations = 100 ,= 0:000001
NewtonFool iterations = 100 , eta = 0:01
L0AttackCommon Parameter Size = 5,
DE NP= 400 , Number of Generations = 100 , CR = 1
CMA-ES Function Evaluations = 40000 ,= 31:75
L1AttackCommon Parameter Size = 3072 ,
DE NP= 3072 , Number of Generations = 100 , CR = 1
CMA-ES Function Evaluations = 39200 ,=th=4
TABLE I: Description of various parameters of different adversarial
attacks.
Evaluating Other Adversarial Attacks (Section V-D): The
evaluated learning systems are tested against other existing
white-box and black-box adversarial attacks such as - Fast
Gradient Method (FGM) [9], Basic Iterative Method (BIM)
[6], Projected Gradient Descent Method (PGD) [14],
DeepFool [38], and NewtonFool [39]. This analysis further
helps to demonstrate the necessity of duality in quality
assessment.
Extremely Fast Quality Assessment (Section V-F): In this
section, we apply and evaluate the principle of
transferability of adversarial samples. We verify the
possibility of a speedy version of the proposed quality
assessment. We implement this by using already crafted
adversarial samples to fool neural networks, instead of a
full-Ô¨Çedged optimisation. This would enable attacks to have
aO(1)time complexity, being signiÔ¨Åcantly faster.
Quality Assessment‚Äôs Attack Distribution (Section V-G):
Here, we assess the dual-attack distribution (Few-Pixel
Attack and Threshold Attack). The analysis of the
distribution demonstrates the necessity of such duality. The
distribution of successful attacks are shown, and previous
attacks are analysed in this perspective.
Effect of threshold (Section V-H): We analyse the
complete behaviour of the adversarial accuracy of our
black-box attacks without restricting the threshold‚Äôs th
value. Using this analysis, we prove the results using a
Ô¨Åxedthin robustness levels is a reasonable approximation
for our proposed quality assessment.
A. Experimental Settings
We use CIFAR-10 dataset [40] to evaluate our dual quality
assessment. Table I gives the parameter description of
various adversarial attacks used. All the pre-existing
adversarial attacks used in the article have been evaluated
using Adversarial Robustness 360 Toolbox (ART v1.2.0)
[41].
For ourL0andL1Attacks, we use the canonical
versions of the DE and CMA-ES algorithms to have a clear
standard. DE uses a repair method in which values that go
beyond range are set to random points within the valid
range. While in CMA-ES, to satisfy the constraints, a simple
repair method is employed in which pixels that surpass the
minimum/maximum are brought back to the
minimum/maximum value. Moreover, a clipping function isModelAttack Adversarial Accuracy
Optimiser th=1 th =3 th =5 th =10
Few-Pixel ( L0) Attack
ResNetDE 24% 70% 75% 79%
CMA-ES 12% 52% 73% 85%
CapsNetDE 21% 37% 49% 57%
CMA-ES 20% 39% 40% 41%
Threshold ( L1) Attack
ResNetDE 5% 23% 53% 82%
CMA-ES 33% 71% 76% 83%
CapsNetDE 11% 13% 15% 23%
CMA-ES 13% 34% 72% 97%
TABLE II: Adversarial accuracy results for Few-Pixel ( L0) and
Threshold ( L1) Attacks with DE and CMA-ES
used to keep values inside the feasible region. The constraint
is always satisÔ¨Åed because the number of parameters is itself
modelled after the constraint. In other words, when searching
for one pixel perturbation, the number of variables are Ô¨Åxed
to pixel values (three values) plus position values (two
values). Therefore it will always modify only one pixel,
respecting the constraint. Since the optimisation is done in
real values, to force the values to be within range, a simple
clipping function is used for pixel values. For position
values, a modulo operation is executed.
B. Preliminary Tests: Choosing the Optimization Algorithm
Table II shows the adversarial accuracy results performed
over 100 random samples. Here adversarial accuracy
corresponds to the accuracy of the adversarial attack to
create adversarial samples to fool neural networks. Both
black-box attacks can craft adversarial samples in all levels
of robustness. This fact demonstrates that without knowing
anything about the learning system and in a constrained
setting, black-box attacks are still able to reach more than
80% adversarial accuracy in state-of-the-art neural networks.
Concerning the comparison of CMA-ES and DE, the
outcomes favour the choice of CMA-ES for the quality
assessment. Both CMA-ES and DE perform likewise for the
Few-Fixel Attack, with both DE and CMA-ES having the
same number of wins. However, for the Threshold Attack,
the performance varies signiÔ¨Åcantly. CMA-ES this time
always wins (eight wins) against DE (no win). This
domination of CMA-ES is expected since the Threshold
Attack has a high dimensional search space which is more
suitable for CMA-ES. This happens in part because DE‚Äôs
operators may allow some variables to converge prematurely.
CMA-ES, on the other hand, is always generating slightly
different solutions while evolving a distribution.
In these preliminary tests, CapsNet was shown overall
superior to ResNet. Few-pixel ( L0) Attack reach 85%
adversarial accuracy for ResNet when ten pixels are
modiÔ¨Åed. CapsNet, on the other hand, is more robust to
Few-Pixel Attacks, allowing them to reach only 52% and
41% adversarial accuracy when ten pixels are modiÔ¨Åed for
DE and CMA-ES respectively. CapsNet is less robust than
ResNet to the Threshold Attack with th= 10 in which6
Model and Adversarial Accuracy
Standard Accuracy th=1 th =3 th =5 th =10
Few-Pixel ( L0) Attack
WideResNet 95.12% 11% 55% 75% 94%
DenseNet 94.54% 9% 43% 66% 78%
ResNet 92.67% 12% 52% 73% 85%
NIN 90.87% 18% 62% 81% 90%
AllConv 88.46% 11% 31% 57% 77%
CapsNet 79.03% 21% 37% 49% 57%
LeNet 73.57% 58% 86% 94% 99%
AT 87.11% 22% 52% 66% 86%
TVM 47.55% 16% 12% 20% 24%
FS 92.37% 17% 49% 69% 78%
Threshold ( L1) Attack
WideResNet 95.12% 15% 97% 98% 100%
DenseNet 94.54% 23% 68% 72% 74%
ResNet 92.67% 33% 71% 76% 83%
NIN 90.87% 11% 86% 88% 92%
AllConv 88.46% 9% 70% 73% 75%
CapsNet 79.03% 13% 34% 72% 97%
LeNet 73.57% 44% 96% 100% 100%
AT 87.11% 3% 12% 25% 57%
TVM 47.55% 4% 4% 6% 14%
FS 92.37% 26% 63% 66% 74%
TABLE III: Adversarial accuracy results for L0andL1Attacks over
100 random samples
almost all images were vulnerable ( 97%). At the same time,
CapsNet is reasonably robust to 1-threshold-safe (only 13%
adversarial accuracy). ResNet is almost equally not robust
throughout, with low robustness even when th= 3, losing to
CapsNet in robustness in all other values of thof the
threshold attack. These preliminary tests also show that
different networks have different robustness. This is not only
regarding the type of attacks ( L0andL1) but also with the
degree of attack (e.g., 1-threshold and 10-threshold attacks
have very different results on CapsNet).
C. Evaluating Learning and Defense Systems
Table III extends the CMA-ES attacks on various neural
networks: WideResNet [33], DenseNet [34], ResNet [29],
Network in Network (NIN) [35], All Convolutional Network
(AllConv) [36], CapsNet [30], and LeNet [37]. We also
evaluate with three contemporary defences: Adversarial
training (AT) [14], Total Variance Minimization (TVM) [19],
and Feature Squeezing (FS) [21].
Results in bold (Only for learning systems and not
defensive systems) are the lowest adversarial accuracy and
other results which are within a distance of Ô¨Åve from the
lowest one. For CapsNet only 88samples could be attacked
with maximum th= 127 forL0Attack. Twelve samples
could not be overwhelmed when the th < 128. Here, taking
into account an existing variance of results, we consider
results within Ô¨Åve of the lowest to be equally good. If we
consider the number of bold results for each of the neural
networks, a qualitative measure of robustness CapsNet and
AllConv can be considered the most robust with Ô¨Åve bold
results. The third place in robustness achieves only threebold results and consequently is far away from the prime
performers.
Regarding the adversarial training, it is easier to attack
with the Few-Pixel Attack than with Threshold Attack. This
result should derive from the fact that the adversarial
samples used in adversarial training contained images from
Projected Gradient Descent (PGD) Attack, which is L1type
of attack. Therefore, it suggests that given an attack bias
that differs from the invariance bias used to train the
networks, the attack can easily succeed . Regarding TVM, the
attacks were less successful. We trained a ResNet on TVM
modiÔ¨Åed images and, albeit many trials with different
hyper-parameters, we were able to craft a classiÔ¨Åer with at
best47:55% accuracy. This is a steep drop from the 92:37%
accuracy of the original ResNet and happens because TVM
was initially conceived for Imagenet and did not scale well
to CIFAR-10. However, as the original accuracy of the
model trained with TVM is also not high; therefore, even
with a small attack percentage of 24%, the resulting model
accuracy is 35%. Attacks on Feature Squeezing had
relatively high adversarial accuracy both L0andL1attacks.
Moreover, both types of attacks had similar accuracy,
revealing a lack of bias in the defence system.
Notice that none of the neural networks was able to
reduce low thattacks to zero. This illustrates that although
robustness may differ between current neural networks, none
of them can effectively overcome even the lowest level of
perturbation feasible. Moreover, since a th= 5 is enough to
achieve around 70% accuracy in many settings, this suggests
that achieving 100% adversarial accuracy may depend more
on a few samples which are harder to attack, such as
samples far away from the decision boundary. Consequently,
the focus on 100% adversarial accuracy rather than the
amount of threshold might give preference to methods which
set a couple of input projections far away from others
without improving the accuracy overall. An example can be
examined by making some input projections far away
enough to make them harder to attack.
The difference in the behaviour of L0andL1Norm Attacks
shows that the robustness is achieved with some trade-offs.
This further justiÔ¨Åes the importance of using both metrics to
evaluate neural networks.
D. Evaluating Other Adversarial Attacks
We evaluated our assessed neural networks further against
well-known adversarial attacks such as Fast Gradient Method
(FGM) [9], Basic Iterative Method (BIM) [6], Projected
Gradient Descent Method (PGD) [14], DeepFool [38], and
NewtonFool [39]. Please, note that for FGM, BIM, PGD
attacks= 8(Default Value)th= 10 ofL1Attack on
our robustness scales. While DeepFool and NewtonFool do
not explicitly control the robustness scale. Table IV
compares the existing white-box attacks and black-box
attacks with our proposed attacks. Notice that, although all
the existing attacks are capable of fooling neural networks.
We notice some peculiar results, like DeepFool Attack, was
less successful against the LeNet, which was most vulnerable7
Adversarial Attacks WideResNet DenseNet ResNet NIN AllConv CapsNet LeNet
FGM 69% (159.88) 50% (120.03) 52% (124.70) 72% (140.46) 67% (155.95) 70% (208.89) 84% (152.37)
BIM 89% (208.44) 52% (160.34) 55% (164.64) 74% (216.97) 69% (273.90) 82% (361.63) 89% (345.27)
PGD 89% (208.49) 52% (160.38) 55% (164.64) 74% (216.96) 69% (274.15) 84% (370.90) 89% (357.34)
DeepFool 60% (613.14) 60% (478.03) 58% (458.57) 59% (492.90) 51% (487.46) 87% (258.08) 31% (132.32)
NewtonFool 82% (63.13) 50% (53.89) 54% (51.56) 66% (54.78) 61% (61.05) 90% (1680.83) 84% (49.61)
Few-Pixel (L0) Attackth= 1 20% (181.43) 20% (179.48) 29% (191.73) 28% (185.09) 24% (172.01) 29% (177.86) 61% (191.69)
th= 3 54% (276.47) 50% (270.50) 63% (275.57) 62% (274.91) 49% (262.66) 43% (247.97) 89% (248.21)
th= 5 75% (326.14) 68% (315.53) 79% (314.27) 81% (318.71) 67% (318.99) 52% (300.19) 96% (265.18)
th= 10 91% (366.60) 81% (354.42) 90% (342.56) 93% (354.61) 81% (365.10) 63% (359.55) 98% (271.90)
Threshold (L1) Attackth= 1 30% (39.24) 38% (39.24) 43% (39.27) 23% (39.23) 23% (39.21) 13% (39.09) 47% (39.28)
th= 3 92% (65.07) 69% (53.89) 74% (52.82) 81% (72.29) 72% (68.11) 34% (70.79) 96% (62.86)
th= 5 95% (67.84) 72% (56.81) 77% (55.38) 85% (77.09) 76% (72.45) 72% (130.80) 99% (66.42)
th= 10 98% (70.70) 78% (67.63) 83% (64.50) 90% (84.20) 79% (77.76) 97% (184.93) 100% (66.65)
TABLE IV: Adversarial accuracy of the proposed L0andL1black-box Attacks used in the dual quality assessment and their comparison
with other methods from the literature. The value in the brackets represents the Mean L2score of the adversarial sample with the original
sample. The results were drawn by attacking a different set of samples from previous tests. Therefore the accuracy results may differ slightly
from previous tables.
Fig. 3: Adversarial accuracy from Table III across classes. The two diagrams at left and right are respectively L0andL1attacks. The top
diagrams used th= 10 while the bottom ones used th= 1.
to our proposed attacks (Table III). Moreover, ResNet and
DenseNet had much better robustness for the existing attacks
compared to our attacks.
The objective of this article is not to propose better or
more effective attacking methods but rather to propose an
assessment methodology, and its related duality conjecture
(the necessity of evaluating both L0andL1Attacks).
However, the proposed Threshold L1Attack in the
assessment methodology is more accurate than other attacks
while requiring less amount of perturbation. The Threshold
Attack requires less perturbation than the One-Pixel attack
(only circa 12% of the amount of perturbation of theOne-Pixel Attack th= 1) which was already considered one
of the most extreme attacks needing less perturbation to fool
neural networks. This sets up an even lower threshold to the
perturbation, which is inevitable to fool neural networks.
Notice that, the behaviour of the existing attacks is similar
to our Threshold L1Attack (Table IV). This suggests that
the current evaluations of the neural networks focus on
increasing the robustness based on L1Norm. However, our
study shows that behaviour of L0Norm differs from the L1
Norm (Table IV), and the robustness for the L1Norm may
not be sufÔ¨Åcient to study the robustness and vulnerabilities
of the neural networks as a whole.8
E. Dependency Of Proposed Adversarial Attacks On Classes
We further separated the adversarial accuracy (Table III)
into classes (Figure 3). This is to evaluate the dependence
of proposed adversarial attacks on speciÔ¨Åc classes, Figure 3
shows an already known feature that some classes are more
natural to attack than others. For example, the columns for
bird and cat classes are visually darker than frog and truck
classes for all diagrams. This happens because classes with
similar features and therefore, closer decision boundaries are
more natural to attack.
Interestingly, the Figure 3 reveals that neural networks tend
to be harder to attack in only a few classes. This may suggest
that these networks encode some classes far away from others
(e.g., projection of the features of these classes into a different
vector). Consequently, the reason for their relative robustness
may lie on a simple construction of the decision boundary
with a few distinct and sharply separated classes.
F . Extremely Fast Quality Assessment: Transferability of
Adversarial Samples
If adversarial samples from one model can be used to attack
different models and defences, it would be possible to create
an ultra-fast quality assessment. Figure 4 shows that indeed,
it is possible to qualitatively assess a neural network based on
the transferability of adversarial samples.
Beyond being a faster method, the transferability of
samples has the beneÔ¨Åt of ignoring any masking of gradients
which makes hard to search but not to transfer. This shows
that the vulnerability in neural networks is still there but
hidden. Interestingly, the transferability is mostly
independent on the type of attack ( L0orL1), with most of
the previously discussed differences disappearing. There are
some differences like L0attacks are less accurate than most
of theL1ones. This suggests that positions of pixel and
their variance are relatively more model-speciÔ¨Åc than small
changes in the whole image.
Generally speaking, transferability is a quick assessment
method which, when used with many different types of
adversarial samples, gives an approximation of the model‚Äôs
robustness. This approximation is not better or worse but
different. It differs from usual attacks because (a) it is not
affected by how difÔ¨Åcult it is to search adversarial samples,
taking into account only their existence, and (b) it measures
the accuracy to commonly found adversarial samples rather
than all searchable ones.
Therefore, in the case of low thvalues, transferability can
be used as a qualitative measure of robustness. However, its
values are not equivalent to or close to real adversarial
accuracy. Thus, it serves only as a lower bound.
G. Adversarial Sample Distribution of Quality Assessment
To understand the importance of the duality for the
proposed quality assessment. We analyse the distribution of
our proposed attacks across samples. In some cases, the
distribution of samples for L0andL1can be easily veriÔ¨Åed
by the difference in adversarial accuracy. For example,Model L0Attack L1Attack
WideResNet 425.0 141.5
DenseNet 989.5 696.0
ResNet 674.0 575.5
NIN 528.0 364.0
AllConv 1123.5 849.0
CapsNet 2493.0 404.5
LeNet 137.5 104.0
TABLE V: Area under the curve (AUC) for both Few-Pixel ( L0) and
Threshold ( L1) black-box Attacks
CapsNet is more susceptible to L1thanL0types of attacks
while for adversarial training [14] the opposite is true (Table
III). Naturally, adversarial training depends strongly on the
adversarial samples used in training, Therefore, different
robustness could be acquired depending on the type of
adversarial samples used.
Moreover, the distribution shows here that even when
adversarial accuracy seems close, the distribution of L0and
L1Attacks may differ. For example, the adversarial
accuracy on ResNet for both L0andL1withth= 10 differ
by mere 2%. However, the distribution of adversarial
samples shows that around 17% of the samples can only be
attacked by either one of the attack types (Figure 5). Thus,
the evaluation of both L0andL1are essential to verify the
robustness of a given neural network or adversarial defence.
Moreover, this is true even when a similar adversarial
accuracy is observed.
H. Analysing effect of threshold thon learning systems
To evaluate how networks behave with the increase in
threshold, we plot here the adversarial accuracy with the
increase of th(Figure 6). These plots reveal an even more
evident difference of behaviour for the same method when
attacked with either L0orL1norm of attacks. It shows that
the curve inclination itself is different. Therefore, L0and
L1Attacks scale differently.
From Figure 6, two classes of curves can be seen. CapsNet
behaves on a class of its own while the other networks behave
similarly. CapsNet, which has an entirely different architecture
with dynamic routing, shows that a very different robustness
behaviour is achieved. LeNet is justiÔ¨Åably lower because of
its lower accuracy and complexity.
To assess the quality of the algorithms in relation to their
curves, the Area Under the Curve (AUC) is calculated by the
trapezoidal rule. deÔ¨Åned as:
AUC = na th1
2+th2+th3+:::+thn 1+thn
2
where
nais the number of images attacked and th1;th 2;:::thnare
different values of ththreshold for a maximum of n= 127 .
Table V shows a quantitative evaluation of Figure 6 by
calculating the Area Under the Curve (AUC).
There is no network which is robust in both attacks. CapsNet
is the most robust neural network for L0attacks while AllConv
wins while being followed closely by other neural networks for
L1. Although requiring a lot more resources to be drawn, the
curves here result in the same conclusion achieved by Table9
Fig. 4: Accuracy of adversarial samples when transferring from the a given source model (row) to a target model (column) for both L1
black-box Attacks (left) and L0black-box Attacks (right). The source of the adversarial samples is on the y-axis with the target model on
the x-axis. The adversarial samples were acquired from 100original images attacked with thvarying mostly from one to ten. The maximum
value of this set to 127.
Fig. 5: Distribution of adversarial samples found on DenseNet (left)
and ResNet (right) using th= 10 with both few-pixel ( L0) and
threshold ( L1) Attacks.
III. Therefore, the previous results are a good approximation
of the behaviour promptly.
VI. C ONCLUSIONS
In this article, we propose a model agnostic dual quality
assessment for adversarial machine learning, especially for
neural networks. By investigating the various state-of-the-art
neural networks as well as arguably the contemporary
adversarial defences, it was possible to: (a) show that
robustness to L0andL1Norm Attacks differ signiÔ¨Åcantly,
which is why the duality should be taken into consideration.
(b) verify that current methods and defences, in general, are
vulnerable even for L0andL1black-box Attacks of low
thresholdth, and (c) validate the dual quality assessment
with robustness level as a good and efÔ¨Åcient approximation
to the full accuracy per threshold curve. Interestingly, the
evaluation of the proposed method (Threshold Attack) was
shown to require surprisingly less amount of perturbation.
This novelL1black-box Attack based on CMA-ES required
only circa 12% of the amount of perturbation used by the
One-Pixel Attack while achieving similar accuracy. Thus,this article analyses the robustness of neural networks and
defences by elucidating the problems as well as proposing
solutions to them. Hopefully, the proposed dual quality
assessment and analysis on current neural networks‚Äô
robustness will aid the development of more robust neural
networks and hybrids alike.
ACKNOWLEDGMENTS
This work was supported by JST, ACT-I Grant Number
JP-50243 and JSPS KAKENHI Grant Number JP20241216.
Additionally, we would like to thank Prof. Junichi Murata
for the kind support without which it would not be possible
to conduct this research.
REFERENCES
[1] C. e. a. Szegedy, ‚ÄúIntriguing properties of neural networks,‚Äù in In ICLR .
Citeseer, 2014.
[2] A. Nguyen, J. Yosinski, and J. Clune, ‚ÄúDeep neural networks are
easily fooled: High conÔ¨Ådence predictions for unrecognizable images,‚Äù
inProceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , 2015, pp. 427‚Äì436.
[3] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard,
‚ÄúUniversal adversarial perturbations,‚Äù in 2017 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) . Ieee, 2017, pp.
86‚Äì94.
[4] T. B. Brown, D. Man ¬¥e, A. Roy, M. Abadi, and J. Gilmer, ‚ÄúAdversarial
patch,‚Äù arXiv preprint arXiv:1712.09665 , 2017.
[5] J. Su, D. V . Vargas, and K. Sakurai, ‚ÄúOne pixel attack for fooling deep
neural networks,‚Äù IEEE Transactions on Evolutionary Computation ,
vol. 23, no. 5, pp. 828‚Äì841, 2019.
[6] A. Kurakin, I. Goodfellow, and S. Bengio, ‚ÄúAdversarial examples in the
physical world,‚Äù arXiv preprint arXiv:1607.02533 , 2016.
[7] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, ‚ÄúAccessorize to
a crime: Real and stealthy attacks on state-of-the-art face recognition,‚Äù
inProceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security . Acm, 2016, pp. 1528‚Äì1540.
[8] A. Athalye and I. Sutskever, ‚ÄúSynthesizing robust adversarial examples,‚Äù
inIcml, 2018.10
Fig. 6: Adversarial accuracy per thforL0andL1Attack.
[9] I. J. Goodfellow, J. Shlens, and C. Szegedy, ‚ÄúExplaining and harnessing
adversarial examples,‚Äù arXiv preprint arXiv:1412.6572 , 2014.
[10] D. V . Vargas and J. Su, ‚ÄúUnderstanding the one-pixel attack: Propagation
maps and locality analysis,‚Äù arXiv preprint arXiv:1902.02947 , 2019.
[11] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, ‚ÄúDistillation
as a defense to adversarial perturbations against deep neural networks,‚Äù
in2016 IEEE Symposium on Security and Privacy (SP) . Ieee, 2016,
pp. 582‚Äì597.
[12] N. Carlini and D. Wagner, ‚ÄúTowards evaluating the robustness of neural
networks,‚Äù in 2017 IEEE Symposium on Security and Privacy (SP) .
Ieee, 2017, pp. 39‚Äì57.
[13] R. Huang, B. Xu, D. Schuurmans, and C. Szepesv ¬¥ari, ‚ÄúLearning with a
strong adversary,‚Äù arXiv preprint arXiv:1511.03034 , 2015.
[14] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, ‚ÄúTowards
deep learning models resistant to adversarial attacks,‚Äù in Iclr, 2018.
[15] F. Tram `er, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and
P. McDaniel, ‚ÄúEnsemble adversarial training: Attacks and defenses,‚Äù
arXiv preprint arXiv:1705.07204 , 2017.
[16] G. K. Dziugaite, Z. Ghahramani, and D. M. Roy, ‚ÄúA study of the
effect of jpg compression on adversarial images,‚Äù arXiv preprint
arXiv:1608.00853 , 2016.
[17] T. Hazan, G. Papandreou, and D. Tarlow, Perturbations, Optimization,
and Statistics . MIT Press, 2016.
[18] N. Das, M. Shanbhogue, S.-T. Chen, F. Hohman, L. Chen, M. E.
Kounavis, and D. H. Chau, ‚ÄúKeeping the bad guys out: Protecting
and vaccinating deep learning with jpeg compression,‚Äù arXiv preprint
arXiv:1705.02900 , 2017.
[19] C. Guo, M. Rana, M. Cisse, and L. van der Maaten, ‚ÄúCountering
adversarial images using input transformations,‚Äù in Iclr, 2018.
[20] Y . Song, T. Kim, S. Nowozin, S. Ermon, and N. Kushman, ‚ÄúPixeldefend:
Leveraging generative models to understand and defend against
adversarial examples,‚Äù in Iclr, 2018.
[21] W. Xu, D. Evans, and Y . Qi, ‚ÄúFeature squeezing: Detecting adversarial
examples in deep neural networks,‚Äù arXiv preprint arXiv:1704.01155 ,
2017.
[22] X. Ma, B. Li, Y . Wang, S. M. Erfani, S. Wijewickrema,
G. Schoenebeck, D. Song, M. E. Houle, and J. Bailey, ‚ÄúCharacterizing
adversarial subspaces using local intrinsic dimensionality,‚Äù arXiv
preprint arXiv:1801.02613 , 2018.
[23] J. Buckman, A. Roy, C. Raffel, and I. Goodfellow, ‚ÄúThermometer
encoding: One hot way to resist adversarial examples,‚Äù Iclr, 2018.
[24] A. Athalye, N. Carlini, and D. Wagner, ‚ÄúObfuscated gradients give a
false sense of security: Circumventing defenses to adversarial examples,‚Äù
inIcml, 2018.
[25] J. Uesato, B. O‚ÄôDonoghue, P. Kohli, and A. Oord, ‚ÄúAdversarial riskand the dangers of evaluating against weak attacks,‚Äù in International
Conference on Machine Learning , 2018, pp. 5032‚Äì5041.
[26] K. Grosse, P. Manoharan, N. Papernot, M. Backes, and P. McDaniel,
‚ÄúOn the (statistical) detection of adversarial examples,‚Äù arXiv preprint
arXiv:1702.06280 , 2017.
[27] N. Carlini and D. Wagner, ‚ÄúAdversarial examples are not easily detected:
Bypassing ten detection methods,‚Äù in Proceedings of the 10th ACM
Workshop on ArtiÔ¨Åcial Intelligence and Security . Acm, 2017, pp. 3‚Äì14.
[28] ‚Äî‚Äî, ‚ÄúMagnet and‚Äù efÔ¨Åcient defenses against adversarial attacks‚Äù are
not robust to adversarial examples,‚Äù arXiv preprint arXiv:1711.08478 ,
2017.
[29] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image
recognition,‚Äù in Proceedings of the IEEE conference on computer vision
and pattern recognition , 2016, pp. 770‚Äì778.
[30] S. Sabour, N. Frosst, and G. E. Hinton, ‚ÄúDynamic routing between
capsules,‚Äù in Advances in neural information processing systems , 2017,
pp. 3856‚Äì3866.
[31] R. Storn and K. Price, ‚ÄúDifferential evolution‚Äìa simple and efÔ¨Åcient
heuristic for global optimization over continuous spaces,‚Äù Journal of
global optimization , vol. 11, no. 4, pp. 341‚Äì359, 1997.
[32] N. Hansen, S. D. M ¬®uller, and P. Koumoutsakos, ‚ÄúReducing the time
complexity of the derandomized evolution strategy with covariance
matrix adaptation (cma-es),‚Äù Evolutionary computation , vol. 11, no. 1,
pp. 1‚Äì18, 2003.
[33] S. Zagoruyko and N. Komodakis, ‚ÄúWide residual networks,‚Äù arXiv
preprint arXiv:1605.07146 , 2016.
[34] F. Iandola, M. Moskewicz, S. Karayev, R. Girshick, T. Darrell,
and K. Keutzer, ‚ÄúDensenet: Implementing efÔ¨Åcient convnet descriptor
pyramids,‚Äù arXiv preprint arXiv:1404.1869 , 2014.
[35] M. Lin, Q. Chen, and S. Yan, ‚ÄúNetwork in network,‚Äù arXiv preprint
arXiv:1312.4400 , 2013.
[36] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller,
‚ÄúStriving for simplicity: The all convolutional net,‚Äù arXiv preprint
arXiv:1412.6806 , 2014.
[37] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner, ‚ÄúGradient-based learning
applied to document recognition,‚Äù Proceedings of the IEEE , vol. 86,
no. 11, pp. 2278‚Äì2324, 1998.
[38] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, ‚ÄúDeepfool: a simple
and accurate method to fool deep neural networks,‚Äù in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition , 2016,
pp. 2574‚Äì2582.
[39] U. Jang, X. Wu, and S. Jha, ‚ÄúObjective metrics and gradient
descent algorithms for adversarial examples in machine learning,‚Äù
inProceedings of the 33rd Annual Computer Security Applications
Conference . Acm, 2017, pp. 262‚Äì277.11
[40] A. Krizhevsky, G. Hinton et al. , ‚ÄúLearning multiple layers of features
from tiny images,‚Äù Tech. Rep., 2009.
[41] M.-I. Nicolae, M. Sinn, M. N. Tran, B. Buesser, A. Rawat, M. Wistuba,
V . Zantedeschi, N. Baracaldo, B. Chen, H. Ludwig, I. Molloy, andB. Edwards, ‚ÄúAdversarial robustness toolbox v1.1.0,‚Äù CoRR , vol.
1807.01069, 2018. [Online]. Available: https://arxiv.org/pdf/1807.01069