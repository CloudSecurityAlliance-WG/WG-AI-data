Exploring the Landscape of Spatial Robustness
Logan Engstrom
MIT
engstrom@mit.eduBrandon Tran
MIT
btran115@mit.eduDimitris Tsipras
MIT
tsipras@mit.edu
Ludwig Schmidt
MIT
ludwigs@mit.eduAleksander M ˛ adry
MIT
madry@mit.edu
Abstract
The study of adversarial robustness has so far largely focused on perturbations bound in `p-norms.
However, state-of-the-art models turn out to be also vulnerable to other, more natural classes of perturbations
such as translations and rotations. In this work, we thoroughly investigate the vulnerability of neural
network–based classiﬁers to rotations and translations. While data augmentation offers relatively small
robustness, we use ideas from robust optimization and test-time input aggregation to signiﬁcantly improve
robustness. Finally we ﬁnd that, in contrast to the `p-norm case, ﬁrst-order methods cannot reliably ﬁnd
worst-case perturbations. This highlights spatial robustness as a fundamentally different setting requiring
additional study.1
1 Introduction
Neural networks are now widely embraced as dominant solutions in computer vision (Krizhevsky et al.,
2012; He et al., 2016), speech recognition (Graves et al., 2013), and natural language processing (Collobert &
Weston, 2008). However, while their accuracy scores often match (and sometimes go beyond) human-level
performance on key benchmarks (He et al., 2015; Taigman et al., 2014), models experience severe performance
degradation in the worst case.
In this case, models are vulnerable to so-called adversarial examples , or slightly perturbed inputs that
are almost indistinguishable from natural data to a human but cause state-of-the-art classiﬁers to make
incorrect predictions (Szegedy et al., 2014; Goodfellow et al., 2015). This raises concerns about the use of
neural networks in contexts where reliability, dependability, and security are important.
There is a long line of work on methods for constructing adversarial perturbations in various set-
tings (Szegedy et al., 2014; Goodfellow et al., 2015; Kurakin et al., 2017; Sharif et al., 2016; Moosavi-Dezfooli
et al., 2016; Carlini & Wagner, 2017; Papernot et al., 2017; Madry et al., 2018; Athalye et al., 2018). However,
these methods largely rely on perturbations that are quite contrived and hence unlikely to exist in the real
world. For example, in the context of image recognition, a large body of work has focused on fooling models
with carefully crafted, `p-bounded perturbations in pixel-space.
As such, one may suspect that adversarial examples are a problem only in the presence of a truly malicious
attacker, and are unlikely to arise in more benign environments. However, recent work has shown that
neural network–based vision classiﬁers are vulnerable to input images that have been spatially transformed
through small rotations, translations, shearing, scaling, and other natural transformations (Fawzi & Frossard,
Equal Contribution
1Code for CIFAR10 available at https://github.com/MadryLab/adversarial\_spatial (TensorFlow) and for ImageNet at https:
//github.com/MadryLab/spatial-pytorch (PyTorch).
1arXiv:1712.02779v4 [cs.LG] 16 Sep 20192015; Kanbak et al., 2018; Xiao et al., 2018; Tramèr & Boneh, 2017). Such transformations are pervasive in
vision applications and hence quite likely to naturally occur in practice. The vulnerability of neural networks
to such transformations raises a natural question:
How can we build spatially robust classiﬁers?
We address this question by ﬁrst performing an in-depth study of neural network–based classiﬁer
robustness to two basic image transformations: translations and rotations. While these transformations
appear natural to a human, we show that small rotations and translations alone can signiﬁcantly degrade
accuracy. These transformations are particularly relevant for computer vision applications since real-world
objects do not always appear perfectly centered.
Natural Adversarial Natural Adversarial Natural Adversarial
“revolver” “mousetrap” “vulture” “orangutan” “ship” “dog”
Figure 1: Examples of adversarial transformations and their predictions in the standard, "black canvas", and
reﬂection padding setting.
1.1 Our Methodology and Results
Our goal is to obtain a ﬁne-grained understanding of the spatial robustness of standard, near state-of-the-art
image classiﬁers for the MNIST (LeCun, 1998), CIFAR10 (Krizhevsky, 2009), and ImageNet (Russakovsky
et al., 2015) datasets.
Classiﬁer brittleness. We ﬁnd that small rotations and translations consistently and signiﬁcantly degrade
accuracy of image classiﬁers on a number of tasks, as illustrated in Figure 1. Our results suggest that
classiﬁers are highly brittle: even small random transformations can degrade accuracy by up to 30%. Such
brittleness to random transformations suggests that these models might be unreliable even in benign settings.
Relative adversary strength. We then perform a thorough analysis comparing the abilities of various
adversaries—ﬁrst-order, random, and grid-based—to fool models with small rotations and translations. In
particular, we ﬁnd that exhaustive grid search-based adversaries are much more powerful than ﬁrst-order
adversaries. This is in stark contrast to results in the `p-bounded adversarial example literature, where
ﬁrst-order methods can consistently ﬁnd approximately worst-case inputs (Carlini & Wagner, 2017; Madry
et al., 2018).
Spatial loss landscape. To understand why such a difference occurs, we delve deeper into the classiﬁers
to try and understand the failure modes induced by such natural transformations. We ﬁnd that the loss
landscape of classiﬁers with respect to rotations and translations is highly non-concave and contains many
spurious maxima. This is, again, in contrast to the `p-bounded setting, in which, experimentally, the value of
different maxima tend to concentrate well (Madry et al., 2018). Our loss landscape results thus demonstrate
that any adversary relying on ﬁrst order information might be unable to reliably ﬁnd misclassiﬁcations.
Consequently, rigorous evaluation of model robustness in this spatial setting requires techniques that that go
beyond what was needed to induce `p-based adversarial robustness.
2Improving spatial robustness. We then develop methods for alleviating these vulnerabilities using in-
sights from our study. As a natural baseline, we augment the training procedure with rotations and
translations. While this does largely mitigate the problem on MNIST, additional data augmentation only
marginally increases robustness on CIFAR10 and ImageNet. We thus propose two natural methods for
further increasing the robustness of these models, based on robust optimization and aggregation of random
input transformations. These methods offer signiﬁcant improvements in classiﬁcation accuracy against both
adaptive and random attackers when compared to both standard models and those trained with additional
data augmentation. In particular, on ImageNet, our best model attains a top1 accuracy of 56% against the
strongest adversary, versus 34% for a standard network with additional data augmentation.
Combining spatial and `¥-bounded attacks. Finally, we examine the interplay between spatial and `¥-
based perturbations. We observe that robustness to these two classes of input perturbations is largely
orthogonal. In particular, pixel-based robustness does not imply spatial robustness, while combining spatial
and `¥-bounded transformations seems to have a cumulative effect in reducing classiﬁcation accuracy. This
emphasizes the need to broaden the notions of image similarity in the adversarial examples literature beyond
the common `p-balls.
2 Related Work
The fact that small rotations and translation can fool neural networks on MNIST and CIFAR10 was, to the best
of our knowledge, ﬁrst observed in (Fawzi & Frossard, 2015). They compute the minimum transformation
required to fool the model and use it as a measure for a quantitative comparison of different architectures
and training procedures. The main difference to our work is that we focus on the optimization aspect of
the problem. We show that a few random queries usually sufﬁce for a successful attack, while ﬁrst-order
methods are ineffective. Moreover, we go beyond standard data augmentation and evaluate the effectiveness
of natural baseline defenses.
The concurrent work of Kanbak et al. (2018) proposes a different ﬁrst-order method to evaluate the
robustness of classiﬁers based on geodesic distances on a manifold. This metric is harder to interpret than our
parametrized attack space. Moreover, given our ﬁndings on the non-concavity of the optimization landscape,
it is unclear how close their method is to the ground truth (exhaustive enumeration). While they perform a
limited study of defenses (adversarial ﬁne-tuning) using their method, it appears to be less effective than our
baseline worst-of-10 training. We attribute this difference to the inherent obstacles ﬁrst-order methods face
in this optimization landscape.
Recently, Xiao et al. (2018) and Tramèr & Boneh (2017) observed independently that it is possible to use
various spatial transformations to construct adversarial examples for naturally and `¥-adversarially trained
models. The main difference from our work is that we show even very simple transformations (translations
and rotations) are sufﬁcient to break a variety of classiﬁers, while the transformations employed in (Xiao
et al., 2018) and (Tramèr & Boneh, 2017) are more involved. The transformation in (Xiao et al., 2018) is
based on performing a displacement of individual pixels in the original image constrained to be globally
smooth and then optimized for misclassiﬁcation probability. Tramèr & Boneh (2017) consider an `¥-bounded
pixel-wise perturbation of a version of the original image that has been slightly rotated and in which a few
random pixels have been ﬂipped. Both of these methods require direct access to the attacked model (or a
surrogate) to compute (or at least estimate) the gradient of the loss function with respect to the model’s input.
In contrast, our attacks can be implemented using only a small number of random, non-adaptive inputs.
3 Adversarial Rotations and Translations
Recall that in the context of image classiﬁcation, an adversarial example for a given input image xand a
classiﬁer Cis an image x0that satisﬁes two properties: (i) on the one hand, the adversarial example x0causes
3the classiﬁer Cto output a different label on x0than on x, i.e., we have C(x)6=C(x0). (ii) On the other hand,
the adversarial example x0is “visually similar” to x.
Clearly, the notion of visual similarity is not precisely deﬁned here. In fact, providing a precise and
rigorous deﬁnition is extraordinarily difﬁcult as it would require formally capturing the notion of human
perception. Consequently, previous work largely settled on the assumption that x0is a valid adversarial
example for xif and only ifkx x0kp#for some p2[0,¥]and#small enough. This convention is
based on the fact that two images are indeed visually similar when they are close enough in some `p-norm.
However, the converse is not necessarily true. A small rotation or translation of an image usually appears
visually similar to a human, yet can lead to a large change when measured in an `p-norm. We aim to
expand the range of similarity measures considered in the adversarial examples literature by investigating
robustness to small rotations and translations.
Attack methods. Our ﬁrst goal is to develop sufﬁciently strong methods for generating adversarial rotations
and translations. In the context of pixel-wise `p-bounded perturbations, the most successful approach for
constructing adversarial examples so far has been to employ optimization methods on a suitable loss
function (Szegedy et al., 2014; Goodfellow et al., 2015; Carlini & Wagner, 2017). Following this approach, we
parametrize our attack method with a set of tunable parameters and then optimize over these parameters.
First, we deﬁne the exact range of attacks we want to optimize over. For the case of rotation and
translation attacks, we wish to ﬁnd parameters (du,dv,q)such that rotating the original image by qdegrees
around the center and then translating it by (du,dv)pixels causes the classiﬁer to make a wrong prediction.
Formally, the pixel at position (u,v)is moved to the following position (assuming the point (0, 0)is the
center of the image):
u0
v0
=
cosq sinq
sinq cosq

u
v
+
du
dv
.
We implement this transformation in a differentiable manner using the spatial transformer blocks of (Jader-
berg et al., 2015)2. In order to handle pixels that are mapped to non-integer coordinates, the transformer
units include a differentiable bilinear interpolation routine. Since our loss function is differentiable with
respect to the input and the transformation is in turn differentiable with respect to its parameters, we can
obtain gradients of the model’s loss function w.r.t. the perturbation parameters. This enables us to apply a
ﬁrst-order optimization method to our problem.
By deﬁning the spatial transformation for some xasT(x;du,dv,q), we construct an adversarial perturba-
tion for xby solving the problem
max
du,dv,qL(x0,y), for x0=T(x;du,dv,q), (1)
whereLis the loss function of the neural network3, and yis the correct label for x.
We compute the perturbation from Equation 1 in three distinct ways:
First-Order Method (FO): Starting from a random choice of parameters, we iteratively take steps in
the direction of the gradient of the loss function. This is the direction that locally maximizes the loss of
the classiﬁer (as a surrogate for misclassiﬁcation probability). Since the maximization problem we are
optimizing is non-concave, there are no guarantees for global optimality, but the hope is that the local
maximum solution closely approximates the global optimum. Note that unlike the `p-norm case, we
are not optimizing in the pixel space but in the latent space of rotation and translation parameters.
Grid Search: We discretize the parameter space and exhaustively examine every possible parametriza-
tion of the attack to ﬁnd one that causes the classiﬁer to give a wrong prediction (if such a parametriza-
tion exists). Since our parameter space is low-dimensional enough, this method is computationally
feasible (in contrast to a grid search for `p-based adversaries).
2We used the open source implementation found here: https://github.com/tensorflow/models/tree/master/research/
transformer .
3The lossLof the classiﬁer is a function from images to real numbers that expresses the performance of the network on the particular
example x(e.g., the cross-entropy between predicted and correct distributions).
4Worst-of- k:We randomly sample kdifferent choices of attack parameters and choose the one on which
the model performs worst. As we increase k, this attack interpolates between a random choice and
grid search.
We remark that while a ﬁrst-order attack requires full knowledge of the model to compute the gradient of
the loss with respect to the input, the other two attacks do not. They only require the outputs corresponding
to chosen inputs, which can be done with only query access to the target model.
4 Improving Invariance to Spatial Transformations
Translation LR3
2
1
012
3Rotation
30
20
10
0102030Xent Loss
0246810MNIST
Translation LR3
2
1
012
3Rotation
30
20
10
0102030Xent Loss
01234567CIFAR-10
Translation LR20
10
0
10
20Rotation
30
20
10
0102030Xent Loss
0.00.51.01.52.02.5ImageNet
Figure 2: Loss landscape of a random example for each dataset when performing left-right translations and
rotations. Translations and rotations are restricted to 10% of the image pixels and 30, respectively. We
observe that the landscape is signiﬁcantly non-concave, rendering ﬁrst-order methods to generate adversarial
example ineffective. Figure 12 in the appendix shows additional examples.
As we will see in Section 5, augmenting the training set with random rotations and translations does
improve the robustness of the model against such random transformations. However, data augmentation
does not signiﬁcantly improve the robustness against worst-case attacks and sometimes leads to a drop in
accuracy on unperturbed images. To address these issues, we explore two simple baselines that turn out to
be surprisingly effective.
Robust Optimization. Instead of performing standard empirical risk minimization to train the classiﬁ-
cation model, we utilize ideas from robust optimization. Robust optimization has a rich history (Ben-Tal
et al., 2009) and has recently been applied successfully in the context of defending neural networks against
adversarial examples (Madry et al., 2018; Sinha et al., 2018; Raghunathan et al., 2018; Wong & Kolter, 2018).
The main barrier to applying robust optimization for spatial transformations is the lack of an efﬁcient proce-
dure to compute the worst-case perturbation of a given example. Performing a grid search (as described in
Section 3) is prohibitive as this would increase the training time by a factor close to the grid size, which can
easily be a factor 100 or 1,000. Moreover, the non-convexity of the loss landscape prevents potentially more
efﬁcient ﬁrst-order methods from discovering (approximately) worst-case transformations (see Section 5 for
details).
Given that we cannot fully optimize over the space of translations and rotations, we instead use a coarse
approximation provided by the worst-of-10 adversary (as described in Section 3). So each time we use an
example during training, we ﬁrst sample 10 transformations of the example uniformly at random from the
space of allowed transformations. We then evaluate the model on each of these transformations and train
on the one perturbation with the highest loss. This corresponds to approximately minimizing a min-max
formulation of robust accuracy similar to (Madry et al., 2018). Training against such an adversary increases
the overall time by a factor of roughly six.4
4We need to perform 10 forward passes and one backwards pass instead of one forward and one backward pass required for
5Aggregating Random Transformations. As Section 5 shows, the accuracy against a random transformation
is signiﬁcantly higher than the accuracy against the worst transformation in the allowed attack space. This
motivates the following inference procedure: compute a (typically small) number of random transformations
of the input image and output the label that occurs the most in the resulting set of predictions. We constrain
these random transformations to be within 5%of the input image size in each translation direction and up
to15of rotation.5The training procedure and model can remain unchanged while the inference time is
increased by a small factor (equal to the number of transformations we evaluate on).
Combining Both Methods. The two methods outlined above are orthogonal and in some sense comple-
mentary. We can therefore combine robust training (using a worst-of-k adversary) and majority inference to
further increase the robustness of our models.
5 Experiments
Standard Adv. Trained Data Aug. 30 Data Aug. 40020406080100Percent of Test Set Fooled
Standard Adv. Trained No Crop Data Aug. 30 Data Aug. 40020406080Percent of Test Set Fooled
Standard No Crop Data Aug. 30 Data Aug. 400204060Percent of Test Set FooledAny
Only Trans
Only Rot
Both
None
MNIST CIFAR10 ImageNet
Figure 3: Fine-grained dataset analysis. For each model, we visualize what percent of the test set can be
fooled via various methods. We compute how many examples can be fooled with either translations or
rotations ("any"), how many can be fooled only by one of these, and how many require a combination to be
fooled ("both").
We evaluate standard image classiﬁers for the MNIST (LeCun, 1998), CIFAR10 (Krizhevsky, 2009) and
ImageNet (Russakovsky et al., 2015) datasets. In order to determine the extent to which misclassiﬁcation is
caused by insufﬁcient data augmentation during training, we examine various data augmentation methods.
We begin with a description of our experimental setup.
Model Architecture. For MNIST, we use a convolutional neural network derived from the TensorFlow
Tutorial (Abadi et al., 2016). In order to obtain a fully convolutional version of the network, we replace the
fully-connected layer by two convolutional layers with 128 and 256 ﬁlters each, followed by a global average
pooling. For CIFAR10, we consider a standard ResNet (He et al., 2016) model with 4 groups of residual
layers with ﬁlter sizes [16, 16, 32, 64] and 5 residual units each. We use standard and `¥-adversarially trained
models similar to those studied by Madry et al. (2018).6,7For ImageNet, we use a ResNet-50 (He et al., 2016)
architecture implemented in the tensorpack repository (Wu et al., 2016). We did not modify the model
architectures or training procedures.
Attack Space. In order to maintain the visual similarity of images to the natural ones we restrict the space
of allowed perturbations to be relatively small. We consider rotations of at most 30and translations of at
standard training.
5Note that if an adversary rotates an image by 30(a valid attack in our threat model), we may end up evaluating the image on
rotations of up to 45.
6https://github.com/MadryLab/cifar10\_challenge
7https://github.com/MadryLab/mnist\_challenge
6most (roughly) 10% percent of the image size in each direction. This corresponds to 3 pixels for MNIST
(image size 2828) and CIFAR10 (image size 3232), and 24 pixels for ImageNet (image size 299299).
For grid search attacks, we consider 5 values per translation direction and 31 values for rotations, equally
spaced. For ﬁrst-order attacks, we use 200steps of projected gradient descent of step size 0.01 times the
parameter range. When rotating and translating the images, we ﬁll the empty space with zeros (black pixels).
Data Augmentation. We consider ﬁve variants of training for our models.
Standard training: The standard training procedure for the respective model architecture.
`¥-bounded adversarial training: The classiﬁer is trained on `¥-bounded adversarial examples that
are generated with projected gradient descent.
No random cropping: Standard training for CIFAR-10 and ImageNet includes data augmentation via
random crops. We investigate the effect of this data augmentation scheme by also training a model
without random crops.
Random rotations and translations: At each training step, we perform a uniformly random perturbation
from the attack space on each training example.
Random rotations and translations from larger intervals: As before, we perform uniformly random
perturbations, but now from a superset of the attack space (40,13% pixels).
5.1 Evaluating Model Robustness
We evaluate all models against random and grid search adversaries with rotations and translations considered
both separately and together. We report the results in Table 1. We visualize a random subset of successful
attacks in Figures 5, 6, and 7 of Appendix A.
Despite the high accuracy of standard models on unperturbed examples and their reasonable performance
on random perturbations, a grid search can signiﬁcantly lower the classiﬁers’ accuracy on the test set. For
the standard models, accuracy drops from 99% to 26% on MNIST, 93% to 3% on CIFAR10, and 76% to 31%
on ImageNet (Top 1 accuracy).
The addition of random rotations and translations during training greatly improves both the random
and adversarial accuracy of the classiﬁer for MNIST and CIFAR10, but less so for ImageNet. For the ﬁrst two
datasets, data augmentation increases the accuracy against a grid adversary by 60% to 70%, while the same
data augmentation technique adds less than 3% accuracy on ImageNet.
We perform a ﬁne-grained investigation of our ﬁndings:
In Figure 3 we examine how many examples can be fooled by (i) rotations only, (ii) translations only,
(iii) neither transformation, or (iv) both.
We visualize the set of fooling angles for a random sample of the rotations-only grid in Figure 4 on
ImageNet, and provide more examples in the appendix in Figure 10. We observe that the set of fooling
angles is nonconvex and not contiguous.
To investigate how many transformations are adversarial per image, we analyze the percentage of
misclassiﬁed grid points for each example in Figure 11. While the majority of images has only a small
number of adversarial transformations, a signiﬁcant fraction of images is fooled by 20% or more of the
transformations.
7Table 1: Accuracy of different classiﬁers against rotation and translation adversaries on MNIST, CIFAR10,
and ImageNet. The allowed transformations are translations by (roughly) 10% of the image size and 30
rotations. The attack parameters are chosen through random sampling or grid search with rotations and
translations considered both together (“Rand.”, “Grid”) and separately (“Rand. T.” and “Grid T.” for
transformations, “Rand R.” and “Grid R.” for rotations). We consider networks that are trained with (i) the
respective standard setup, (ii) no data augmentation (if data augmentation is present in standard setup), (iii)
with an `¥adversary, (iv) with data augmentation corresponding to the attack space ( 3px,30) and an
enlarged space (4px,40), and (v) with worst-of-10 training for both types of augmentations.
Model Nat. Rand. Grid Rand. T. Grid T. Rand. R. Grid R.MNISTStandard 99.31% 94.23% 26.02 % 98.61% 89.80% 95.68% 70.98%
`¥-Adv 98.65% 88.02% 1.20% 93.72% 34.13% 95.27% 72.03%
Aug. 30 99.53% 99.35% 95.79 % 99.47% 98.66% 99.34% 98.23%
Aug. 40 99.34% 99.31% 96.95 % 99.39% 98.65% 99.40% 98.49%
W-10 (30) 99.48% 99.37% 97.32 % 99.50% 99.01% 99.39% 98.62%
W-10 (40) 99.42% 99.39% 97.88 % 99.45% 98.89% 99.36% 98.85%CIFAR10Standard 92.62% 60.93% 2.80% 88.54% 66.17% 75.36% 24.71%
No Crop 90.34% 54.64% 1.86% 81.95% 46.07% 69.23% 18.34%
`¥-Adv 80.21% 58.33% 6.02% 78.15% 59.02% 62.85% 20.98%
Aug. 30 90.02% 90.92% 58.90 % 91.76% 79.01% 91.14% 76.33%
Aug. 40 88.83% 91.18% 61.69 % 91.53% 77.42% 91.10% 76.80%
W-10 (30) 91.34% 92.35% 69.17 % 92.43% 83.01% 92.33% 81.82%
W-10 (40) 91.00% 92.11% 71.15 % 92.28% 82.15% 92.53% 82.25%ImageNetStandard 75.96% 63.39% 31.42 % 73.24% 60.42% 67.90% 44.98%
No Crop 70.81% 59.09% 16.52 % 66.75% 45.17% 62.78% 34.17%
Aug. 30 65.96% 68.60% 32.90 % 70.27% 45.72% 69.28% 47.25%
Aug. 40 66.19% 67.58% 33.86 % 69.50% 44.60% 68.88% 48.72%
W-10 (30) 76.14% 73.19% 52.76 % 74.42% 61.18% 73.74% 61.06%
W-10 (40) 74.64% 71.36% 50.23 % 72.86% 59.34% 71.95% 59.23%
8-30 -15 0 15 30
AngleExamplesStandard
-30 -15 0 15 30
AngleNo Crop
-30 -15 0 15 30
AngleData Aug. 30
-30 -15 0 15 30
AngleData Aug. 40Figure 4: Visualizing which angles fool ImageNet classiﬁers for 50 random examples. For each dataset and
model, we visualize one example per row. Red corresponds to misclassiﬁcation of the images. We observe
that the angles fooling the models form a highly non-convex set. Figure 10 in the appendix shows additional
examples for CIFAR10 and MNIST.
Padding Experiments. A natural question is whether the reduced accuracy of the models is due to the
cropping applied during the transformation. We verify that this is not the case by applying zero and reﬂection
padding to the image datasets. We note that the zero padding creates a “black canvas” version of the dataset,
ensuring that no information from the original image is lost after a transformation. We show a random set
of adversarial examples in this setting in Figure 8 and a full evaluation in Table 4. We also provide more
details regarding reﬂection padding in Section B and provide an evaluation in Table 6. All of these are in
Appendix A.
5.2 Comparing Attack Methods
In Table 2 we compare different attack methods on various classiﬁers and datasets. We observe that worst-of-
10 is a powerful adversary despite its limited interaction with the target classiﬁer. The ﬁrst-order adversary
performs signiﬁcantly worse. It fails to approximate the ground-truth accuracy of the models and performs
signiﬁcantly worse than the grid adversary and even the worst-of-10 adversary.
Table 2: Comparison of attack methods across datasets and models. Worst-of-10 is very effective and
signiﬁcantly reduces the model accuracy despite the limited interaction. The ﬁrst-order (FO) adversary
performs poorly, despite the large number of steps allowed. We compare standard training to Augmentation
(3px,30). For the full table, see Figure 3 of Appendix A.
MNIST CIFAR-10 ImageNet
Standard Aug. Standard Aug. Standard Aug.
Natural 99.31% 99.53% 92.62% 90.02% 75.96% 65.96%
Worst-of-10 73.32% 98.33% 20.13% 79.92% 47.83% 50.62%
First-Order 79.84% 98.78% 62.69% 85.92% 63.12% 66.05%
Grid 26.02 % 95.79 % 2.80% 58.92 % 31.42 % 32.90 %
9Understanding the Failure of First-Order Methods. The fact that ﬁrst-order methods fail to reliably ﬁnd
adversarial rotations and translations is in sharp contrast to previous work on `p-robustness (Carlini &
Wagner, 2017; Madry et al., 2018). For `p-bounded perturbations parametrized directly in pixel space, prior
work found the optimization landscape to be well-behaved which allowed ﬁrst-order methods to consistently
ﬁnd maxima with high loss. In the case of spatial perturbations, we observe that the non-concavity of the
problem is a signiﬁcant barrier for ﬁrst-order methods. We investigate this issue by visualizing the loss
landscape. For a few random examples from the three datasets, we plot the cross-entropy loss of the
examples as a function of translation and rotation. Figure 2 shows one example for each dataset and
additional examples are visualized in Figure 12 of the appendix. The plots show that the loss landscape is
indeed non-concave and contains many local maxima of low value. The low-dimensional problem structure
seems to make non-concavity a crucial obstacle. Even for MNIST, where we observe fewer local maxima, the
large ﬂat regions prevent ﬁrst-order methods from ﬁnding transformations of high loss.
Relation to Black-Box Attacks. Given its limited interaction with the model, the worst-of-10 adversary
achieves a signiﬁcant reduction in classiﬁcation accuracy. It performs only 10 random ,non-adaptive queries to
the model and is still able to ﬁnd adversarial examples for a large fraction of the inputs (see Table 2). The
low query complexity is an important baseline for black-box attacks on neural networks, which recently
gained signiﬁcant interest (Papernot et al., 2017; Chen et al., 2017; Bhagoji et al., 2017; Ilyas et al., 2018).
Black-box attacks rely only function evaluations of the target classiﬁer, without additional information such
as gradients. The main challenge is to construct an adversarial example from a small number of queries.
Our results show that it is possible to ﬁnd adversarial rotations and translations for a signiﬁcant fraction of
inputs with very few queries.
Combining Spatial and `¥-Bounded Perturbations Table 1 shows that models trained to be robust to
`¥-bounded perturbations do not achieve higher robustness to spatial perturbations. This provides evidence
that the two families of perturbation are orthogonal to each other. We further investigate this possibility
by considering a combined adversary that utilizes `¥-bounded perturbations on top of rotations and
translations. The results are shown in Figure 13. We indeed observe that these combined attacks reduce
classiﬁcation accuracy in an (approximately) additive manner.
5.3 Evaluating Our Defense Methods.
As we see in Table 1, training with a worst-of-10 adversary signiﬁcantly increases the spatial robustness of the
model, also compared to data augmentation with random transformations. We conjecture that using more
reliable methods to compute the worst-case transformations will further improve these results. Unfortunately,
increasing the number of random transformations per training example quickly becomes computationally
expensive. And as pointed out above, current ﬁrst-order methods also appear to be insufﬁcient for ﬁnding
worst-case transformations efﬁciently.
Our results for majority-based inference are presented in Table 5 of Appendix A. By combining these two
defenses, we improve the worst-case performance of the models from 26% to 98% on MNIST, from 3% to
82% on CIFAR10, and from 31% to 56% on ImageNet (Top 1).
6 Conclusions
We examined the robustness of state-of-the-art image classiﬁers to translations and rotations. We observed
that even a small number of randomly chosen perturbations of the input are sufﬁcient to considerably
degrade the classiﬁer’s performance.
The fact that common neural networks are vulnerable to simple and naturally occurring spatial transfor-
mations (and that these transformations can be found easily from just a few random tries) indicates that
adversarial robustness should be a concern not only in a fully worst-case security setting. We conjecture
10that additional techniques need to be incorporated in the architecture and training procedures of modern
classiﬁers to achieve worst-case spatial robustness. Also, our results underline the need to consider broader
notions of similarity than only pixel-wise distances when studying adversarial misclassiﬁcation attacks. In
particular, we view combining the pixel-wise distances with rotations and translations as a next step towards
the “right” notion of similarity in the context of images.
Acknowledgements
Dimitris Tsipras was supported in part by the NSF grant CCF-1553428 and the NSF Frontier grant CNS-
1413920. Aleksander M ˛ adry was supported in part by an Alfred P . Sloan Research Fellowship, a Google
Research Award, and the NSF grants CCF-1553428 and CNS-1815221.
References
Abadi, M., Barham, P ., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M.,
et al. Tensorﬂow: A system for large-scale machine learning. In USENIX Symposium on Operating Systems
Design and Implementation (OSDI) , 2016.
Athalye, A., Engstrom, L., Ilyas, A., and Kwok, K. Synthesizing robust adversarial examples. In International
Conference on Machine Learning (ICML) , 2018.
Ben-Tal, A., El Ghaoui, L., and Nemirovski, A. Robust optimization . Princeton University Press, 2009.
Bhagoji, A. N., He, W., Li, B., and Song, D. Exploring the space of black-box attacks on deep neural networks.
InArXiv preprint arXiv:1712.09491 , 2017.
Carlini, N. and Wagner, D. Towards evaluating the robustness of neural networks. In Symposium on Security
and Privacy (SP) , 2017.
Chen, P .-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J. Zoo: Zeroth order optimization based black-box
attacks to deep neural networks without training substitute models. In Workshop on Artiﬁcial Intelligence
and Security , 2017.
Collobert, R. and Weston, J. A uniﬁed architecture for natural language processing: Deep neural networks
with multitask learning. In Proceedings of the 25th international conference on Machine learning , pp. 160–167,
2008.
Fawzi, A. and Frossard, P . Manitest: Are classiﬁers really invariant? In British Machine Vision Conference
(BMVC) , 2015.
Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. In International
Conference on Learning Representations (ICLR) , 2015.
Graves, A., Mohamed, A.-r., and Hinton, G. Speech recognition with deep recurrent neural networks. In
International Conference on Acoustics, Speech, and Signal Processing (ICASSP) , 2013.
He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectiﬁers: Surpassing human-level performance on
imagenet classiﬁcation. In international conference on computer vision (ICCV) , 2015.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Conference on Computer
Vision and Pattern Recognition (CVPR) , 2016.
Ilyas, A., Engstrom, L., Athalye, A., and Lin, J. Black-box adversarial attacks with limited queries and
information. In International Conference on Machine Learning (ICML) , 2018.
11Jaderberg, M., Simonyan, K., Zisserman, A., et al. Spatial transformer networks. In neural information
processing systems (NeurIPS) , 2015.
Kanbak, C., Moosavi-Dezfooli, S.-M., and Frossard, P . Geometric robustness of deep networks: analysis and
improvement. In Conference on Computer Vision and Pattern Recognition (CVPR) , 2018.
Krizhevsky, A. Learning multiple layers of features from tiny images. In Technical report , 2009.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional neural
networks. In Advances in Neural Information Processing Systems (NeurIPS) , 2012.
Kurakin, A., Goodfellow, I. J., and Bengio, S. Adversarial machine learning at scale. In International Conference
on Learning Representations (ICLR) , 2017.
LeCun, Y. The mnist database of handwritten digits. In Technical report , 1998.
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to
adversarial attacks. In International Conference on Learning Representations (ICLR) , 2018.
Moosavi-Dezfooli, S., Fawzi, A., and Frossard, P . Deepfool: a simple and accurate method to fool deep
neural networks. In Computer Vision and Pattern Recognition (CVPR) , 2016.
Papernot, N., McDaniel, P ., Goodfellow, I., Jha, S., Celik, Z. B., and Swami, A. Practical black-box attacks
against machine learning. In Asia Conference on Computer and Communications Security , 2017.
Raghunathan, A., Steinhardt, J., and Liang, P . Certiﬁed defenses against adversarial examples. In International
Conference on Learning Representations (ICLR) , 2018.
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A.,
Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. In
International Journal of Computer Vision (IJCV) , 2015.
Sharif, M., Bhagavatula, S., Bauer, L., and Reiter, M. K. Accessorize to a crime: Real and stealthy attacks
on state-of-the-art face recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security, Vienna, Austria, October 24-28, 2016 , pp. 1528–1540, 2016.
Sinha, A., Namkoong, H., and Duchi, J. Certiﬁable distributional robustness with principled adversarial
training. In International Conference on Learning Representations (ICLR) , 2018.
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. Intriguing
properties of neural networks. In International Conference on Learning Representations (ICLR) , 2014.
Taigman, Y., Yang, M., Ranzato, M., and Wolf, L. Deepface: Closing the gap to human-level performance in
face veriﬁcation. In Computer Vision and Pattern Recognition (CVPR) , 2014.
Tramèr, F. and Boneh, D. Personal communication, 2017.
Wong, E. and Kolter, J. Z. Provable defenses against adversarial examples via the convex outer adversarial
polytope. In International Conference on Machine Learning (ICML) , 2018.
Wu, Y. et al. Tensorpack. https://github.com/tensorpack/ , 2016.
Xiao, C., Zhu, J., Li, B., He, W., Liu, M., and Song, D. Spatially transformed adversarial examples. In
International Conference on Learning Representations (ICLR) , 2018.
12A Omitted Tables and Figures
Table 3: Comparison of attack methods across datasets and models.
Model Natural Worst-of-10 FO GridMNISTStandard 99.31% 73.32% 79.84% 26.02 %
`¥-Adversarially Trained 98.65% 51.18% 81.23% 1.20%
Aug. 30 (3px,30) 99.53% 98.33% 98.78% 95.79 %
Aug. 40 (4px,40) 99.34% 98.49% 98.74% 96.95 %CIFAR10Standard 92.62% 20.13% 62.69% 2.80%
No Crop 90.34% 15.04% 52.27% 1.86%
`¥-Adversarially Trained 80.21% 19.38% 33.24% 6.02%
Aug. 30 (3px,30) 90.02% 79.92% 85.92% 58.92 %
Aug. 40 (4px,40) 88.83% 80.47% 85.48% 61.69 %ImageNetStandard 75.96% 47.83% 63.12% 31.42 %
No Crop 70.81% 35.52% 55.93% 16.52 %
Aug. 30 (24px,30) 65.96% 50.62% 66.05% 32.90 %
Aug. 40 (32px,40) 66.19% 51.11% 66.14% 33.86 %
Table 4: Evaluation of a subset of Table 1 in the “black-canvas” setting (images are zero-padded to avoid
cropping due to rotations and translations). The models are trained on padded images.
Natural Random Worst-of-10 Grid Trans. Grid Rot. GridCIFAR10Standard 91.81% 70.23% 25.51% 6.55% 83.38% 12.44%
No Crop 89.70% 52.86% 14.14% 1.17% 47.94% 9.46%
Aug. 30 (3px,30) 91.45% 90.82% 80.53% 63.64 % 82.28% 76.32%
Aug. 40 (4px,40) 91.24% 91.00% 81.81% 66.64 % 81.75% 78.57%ImageNetStandard 73.60% 46.59% 29.51% 15.38 % 28.03% 23.81%
No Crop 66.28% 38.70% 14.17% 3.43% 8.87% 10.97%
Aug. 30 (24px,30)64.60% 67.75% 47.32% 28.51 % 45.33% 39.33%
Aug. 40 (32px,40)49.20% 57.69% 38.36% 22.10 % 32.84% 32.95%
13OriginalStandard
7
Perturbed
9
OriginalAdv. Trained
7
Perturbed
9
OriginalData Aug. 30
9
Perturbed
2
OriginalData Aug. 40
5
Perturbed
6
2
 9
 2
 7
 5
 6
 3
 8
1
 7
 1
 7
 3
 8
 2
 1
0
 5
 0
 5
 6
 4
 9
 4
4
 0
 4
 0
 4
 2
 4
 8
1
 7
 1
 7
 7
 2
 7
 1
4
 9
 4
 9
 7
 2
 4
 9
9
 6
 9
 2
 2
 1
 2
 7
5
 6
 5
 6
 3
 7
 2
 9Figure 5: MNIST. Successful adversarial examples for the models studied in Section 5. Rotations are restricted
to be within 30of the original image and translations up to 3pixels per direction (image size 2828). Each
example is visualized along with its predicted label in the original and perturbed versions.
14OriginalStandard
ship
Perturbed
automobile
OriginalAdv. Trained
airplane
Perturbed
ship
OriginalData Aug. 30
ship
Perturbed
automobile
OriginalData Aug. 40
ship
Perturbed
automobile
ship
 dog
 automobile
 truck
 airplane
 automobile
 airplane
 automobile
frog
 dog
 frog
 bird
 frog
 dog
 frog
 cat
frog
 bird
 automobile
 dog
 automobile
 truck
 automobile
 cat
automobile
 dog
 airplane
 dog
 dog
 cat
 automobile
 frog
truck
 cat
 truck
 frog
 ship
 airplane
 airplane
 dog
ship
 airplane
 horse
 cat
 dog
 cat
 dog
 horse
ship
 cat
 ship
 automobile
 horse
 truck
 ship
 airplane
airplane
 bird
 airplane
 bird
 horse
 deer
 horse
 dogFigure 6: CIFAR10. Successful adversarial examples for the models studied in Section 5. Rotations are
restricted to be within 30of the original and translations up to 3pixels per directions (image size 3232).
Each example is visualized along with its predicted label in the original and perturbed version.
15OriginalStandard
Petri dish
Perturbed
tray
OriginalAdv. Trained
Petri dish
Perturbed
bell pepper
OriginalData Aug. 30
Petri dish
Perturbed
tray
OriginalData Aug. 40
Petri dish
Perturbed
cucumber
boathouse
 guillotine
 boathouse
 umbrella
 boathouse
 guillotine
 boathouse
 plastic bag
totem pole
 wallet
 wallet
 envelope
 wallet
 envelope
 totem pole
 swimming trunks
golfcart
 trailer truck
 goblet
 red wine
 golfcart
 car mirror
 wallet
 refrigerator
goblet
 red wine
 toyshop
 perfume
 goblet
 red wine
 capuchin
 titi
toyshop
 confectionery
 garter snake
 alligator lizard
 garter snake
 African crocodile
 golfcart
 minivan
garter snake
 African crocodile
 hyena
 brown bear
 hyena
 platypus
 goblet
 red wine
hyena
 cheetah
 china cabinet
 thimble
 damselfly
 dragonfly
 toyshop
 face powder
damselfly
 dragonfly
 barrow
 barrel
 china cabinet
 golf ball
 water snake
 night snakeFigure 7: ImageNet. Successful adversarial examples for the models studied in Section 5. Rotations are
restricted to be within 30of the original and translations up to 24pixels per directions (image size 299299).
Each example is visualized along with its predicted label in the original and perturbed version.
16OriginalCifar
airplane
Perturbed
ship
OriginalImageNet
harvester
Perturbed
screen
automobile
 horse
 vulture
 orangutan
airplane
 dog
 sea snake
 slug
dog
 cat
 crane
 pier
horse
 dog
 soup bowl
 eggnog
airplane
 bird
 bakery
 packetFigure 8: Sample adversarial transformations for the "black-canvas" setting for the standard models on
CIFAR10 and ImageNet.
17OriginalCifar
ship
Perturbed
dog
airplane
 bird
frog
 cat
automobile
 cat
automobile
 dog
truck
 dogFigure 9: Sample adversarial transformations for the reﬂection padding setting for the standard models on
CIFAR10.
18MNIST
-30 -15 0 15 30
AngleExamplesStandard
-30 -15 0 15 30
AngleAdv. Trained
-30 -15 0 15 30
AngleData Aug. 30
-30 -15 0 15 30
AngleData Aug. 40
CIFAR10
-30 -15 0 15 30
AngleExamplesStandard
-30 -15 0 15 30
AngleAdv. Trained
-30 -15 0 15 30
AngleData Aug. 30
-30 -15 0 15 30
AngleData Aug. 40
Figure 10: Visualizing which angles fool the classiﬁer for 50 random examples on CIFAR and MNIST. For
each dataset and model, we visualize one example per row. Red corresponds to misclassiﬁcation of the images.
We observe that the angles fooling the models form a highly non-convex set.
19MNIST
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of transformations
 fooling the model020406080100% of originally correct
 examples misclassifiedTranslations and Rotations
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of transformations
 fooling the model020406080100Translations Only
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of transformations
 fooling the model020406080100Rotations Only
Standard
Adv. Trained
Data Aug. 30
Data Aug. 40
CIFAR10
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of transformations
 fooling the model020406080100% of originally correct
 examples misclassifiedTranslations and Rotations
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of transformations
 fooling the model020406080100Translations Only
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of transformations
 fooling the model020406080100Rotations Only
Standard
Adv. Trained
No Crop
Data Aug. 30
Data Aug. 40
ImageNet
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of transformations
 fooling the model020406080% of originally correct
 examples misclassifiedTranslations and Rotations
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of transformations
 fooling the model020406080Translations Only
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of transformations
 fooling the model020406080Rotations Only
Standard
No Crop
Data Aug. 30
Data Aug. 40
Figure 11: Cumulative Density Function plots. For each fraction of grid points p, we plot the percentage of
correctly classiﬁed test set examples that are fooled by at least pof the grid points. For instance, we can see
from the ﬁrst plot, MNIST Translations and Rotations, that approximately 10% of the correctly classiﬁed
natural examples are misclassiﬁed under 1/5 of the grid points transformations.
20Translation LR3
2
1
0
1
2
3Rotation
30
20
10
0102030Xent Loss1e3
0.00.20.40.60.81.01.21.41.6MNIST
Translation LR3
2
1
0
1
2
3Rotation
30
20
10
0102030Xent Loss
012345678CIFAR-10
Translation LR20
10
0
10
20Rotation
30
20
10
0102030Xent Loss1e3
0.00.51.01.52.02.53.03.5ImageNet
Translation LR3
2
1
0
1
2
3Rotation
30
20
10
0102030Xent Loss
02468101214
Translation LR3
2
1
0
1
2
3Rotation
30
20
10
0102030Xent Loss
0.02.55.07.510.012.515.017.520.0
Translation LR20
10
0
10
20Rotation
30
20
10
0102030Xent Loss
0.00.51.01.52.02.5
Translation LR3
2
1
0
1
2
3Rotation
30
20
10
0102030Xent Loss1e3
0.000.250.500.751.001.251.501.752.00
Translation LR3
2
1
0
1
2
3Rotation
30
20
10
0102030Xent Loss
012345678
Translation LR20
10
0
10
20Rotation
30
20
10
0102030Xent Loss1e1
0.00.20.40.60.81.01.21.4
Translation LR3
2
1
0
1
2
3Rotation
30
20
10
0102030Xent Loss
01234
Translation LR3
2
1
0
1
2
3Rotation
30
20
10
0102030Xent Loss
0.00.20.40.60.81.01.21.41.6
Translation LR20
10
0
10
20Rotation
30
20
10
0102030Xent Loss
01234Figure 12: Loss landscape of 4random examples for each dataset when performing left-right translations
and rotations. Translations and rotations are restricted to 10% of the image pixels and 30respectively.
We observe that the landscape is signiﬁcantly non-concave, making rendering FO methods for adversarial
example generation powerless.21MNIST
Standard Adv. Trained Data Aug. 30 Data Aug. 40
0.15 0.1 0.05 0020406080100
#Accuracy
0.15 0.1 0.05 0020406080100
#0.15 0.1 0.05 0020406080100
#0.15 0.1 0.05 0020406080100
#`¥random + `¥grid + `¥
CIFAR10
Standard Adv. Trained Data Aug. 30 Data Aug. 40
00.5 11.5 2020406080100
#Accuracy
00.5 11.5 2020406080100
#00.5 11.5 2020406080100
#00.5 11.5 2020406080100
#`¥random + `¥grid + `¥
ImageNet
Standard No Crop Data Aug. 30 Data Aug. 40
00.10.20.30.40.5020406080100
#Accuracy
00.10.20.30.40.5020406080100
#00.10.20.30.40.5020406080100
#00.10.20.30.40.5020406080100
#`¥random + `¥grid + `¥
Figure 13: Accuracy of different classiﬁers against `¥-bounded adversaries with various values of #and
spatial transformations. For each value of #, we perform PGD to ﬁnd the most adversarial `¥-bounded
perturbation. Additionally, we combine PGD with random rotations and translations and with a grid search
over rotations and translations in order to ﬁnd the transformation that combines with PGD in the most
adversarial way.
B Mirror Padding
In the experiments of Section 5, we ﬁlled the remaining pixels of rotated and translated images with black
(also known as zero or constant padding). This is the standard approach used when performing random
cropping for data augmentation purposes. We brieﬂy examined the effect of mirror padding, that is replacing
empty pixels by reﬂecting the image around the border8. The results are shown in Table 6. We observed that
training with one padding method and evaluating using the other resulted in a signiﬁcant drop in accuracy.
Training using one of these methods randomly for each example resulted in a model which roughly matched
the best-case of the two individual cases.
8https://www.tensorflow.org/api\_docs/python/tf/pad
22Table 5: Majority Defense. Accuracy of different models on the natural evaluation set and against a combined
rotation and translation adversary using aggregation of multiple random transformations.
Natural Acc. Grid Acc.
Model Stand. Vote Stand. VoteMNISTStandard 99.31% 98.71% 26.02% 18.80%
Aug 30. 99.53 % 99.41% 95.79% 95.32%
Aug 40. 99.34% 99.25% 96.95% 97.65%
W-10 (30) 99.48% 99.40% 97.32% 96.95%
W-10 (40) 99.42% 99.41% 97.88% 98.47 %CIFAR10Standard 92.62% 80.37% 2.82% 7.85%
Aug 30. 90.02% 92.70% 58.90% 69.65%
Aug 40. 88.83% 92.50% 61.69% 76.54%
W-10 (30) 91.34% 93.38% 69.17% 77.33%
W-10 (40) 91.00% 93.40 %71.15% 81.52 %ImageNetStandard 75.96% 73.19% 31.42% 40.21%
Aug 30. 65.96% 72.44% 32.90% 44.46%
Aug 40. 66.19% 71.46% 33.86% 46.98%
W-10 (30) 76.14 % 74.92% 52.76% 56.45 %
W-10 (40) 74.64% 73.38% 50.23% 56.23%
NaturalRandom
(Zero)Random
(Mirror)Grid Search
(Zero)Grid Search
(Mirror)
Standard Nat 92.62% 60.76% 66.42% 8.08% 5.37%
Standard Adv 80.21% 59.79% 67.12% 7.20% 12.89%
Aug. A, Zero 90.25% 91.09% 87.67% 59.87% 40.55%
Aug. B, Zero 89.55% 91.40% 87.94% 62.42% 42.37%
Aug. A, Mirror 92.25% 88.43% 91.05% 41.46% 53.95%
Aug. B, Mirror 92.03% 88.58% 91.34% 45.44% 57.97%
Aug. A, Both 91.80% 90.98% 91.28% 56.95% 52.60%
Aug. B, Both 91.57% 91.87% 91.11% 60.46% 56.13%
Table 6: CIFAR10: The effect of using reﬂection or zero padding when training a model. The experimental
setup matches that of Section 5. Zero padding refers to ﬁlling the empty pixels caused by translations and
rotations with black. Mirror padding corresponds to using a reﬂection of the images. "Both" refers to training
using both methods and alternating randomly between them for each training example.
23