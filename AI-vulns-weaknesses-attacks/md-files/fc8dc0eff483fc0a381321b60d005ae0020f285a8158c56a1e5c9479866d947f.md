HopSkipJumpAttack: A Query-EfÔ¨Åcient
Decision-Based Attack
Jianbo ChenMichael I. JordanMartin J. Wainwright;y
University of California, BerkeleyV oleon Groupy
fjianbochen@, jordan@cs., wainwrig@ gberkeley.edu
Abstract ‚ÄîThe goal of a decision-based adversarial attack on a
trained model is to generate adversarial examples based solely
on observing output labels returned by the targeted model. We
develop HopSkipJumpAttack, a family of algorithms based on
a novel estimate of the gradient direction using binary infor-
mation at the decision boundary. The proposed family includes
both untargeted and targeted attacks optimized for `2and `1
similarity metrics respectively. Theoretical analysis is provided
for the proposed algorithms and the gradient direction estimate.
Experiments show HopSkipJumpAttack requires signiÔ¨Åcantly
fewer model queries than several state-of-the-art decision-based
adversarial attacks. It also achieves competitive performance in
attacking several widely-used defense mechanisms.
I. I NTRODUCTION
Although deep neural networks have achieved state-of-the-art
performance on a variety of tasks, they have been shown to
be vulnerable to adversarial examples ‚Äîthat is, maliciously
perturbed examples that are almost identical to original sam-
ples in human perception, but cause models to make in-
correct decisions [1]. The vulnerability of neural networks
to adversarial examples implies a security risk in applica-
tions with real-world consequences, such as self-driving cars,
robotics, Ô¨Ånancial services, and criminal justice; in addition,
it highlights fundamental differences between human learning
and existing machine-based systems. The study of adversarial
examples is thus necessary to identify the limitation of current
machine learning algorithms, provide a metric for robustness,
investigate the potential risk, and suggest ways to improve the
robustness of models.
Recent years have witnessed a Ô¨Çurry of research on the design
of new algorithms for generating adversarial examples [1‚Äì16].
Adversarial examples can be categorized according to at least
three different criteria: the similarity metric, the attack goal,
and the threat model. Commonly used similarity metrics are
`p-distances between adversarial and original examples with
p2 f0;2;1g. The goal of attack is either untargeted or
targeted. The goal of an untargeted attack is to perturb the
input so as to cause any type of misclassiÔ¨Åcation, whereas the
goal of a targeted attack is to alter the decision of the model to
a pre-speciÔ¨Åc target class. Changing the loss function allows
for switching between two types of attacks [3, 5, 6].
Perhaps the most important criterion in practice is the threat
Figure 1: An illustration of accessible components of the target
model for each of the three threat models. A white-box threat
model assumes access to the whole model; a score-based threat
model assumes access to the output layer; a decision-based
threat model assumes access to the predicted label alone.
model , of which there are two primary types: white-box and
black-box. In the white-box setting, an attacker has complete
access to the model, including its structure and weights. Under
this setting, the generation of adversarial examples is often
formulated as an optimization problem, which is solved either
via treating misclassiÔ¨Åcation loss as a regularization [1, 6] or
via tackling the dual as a constrained optimization problem
[2, 3, 7]. In the black-box setting, an attacker can only
access outputs of the target model. Based on whether one has
access to the full probability or the label of a given input,
black-box attacks are further divided into score-based and
decision-based. See Figure 1 for an illustration of accessible
components of the target model for each of the three threat
models. Chen et al. [8] and Ilyas et al. [9, 10] introduced
score-based methods using zeroth-order gradient estimation to
craft adversarial examples.
The most practical threat model is that in which an attacker
has access to decisions alone. A widely studied type of the
decision-based attack is transfer-based attack. Liu et al. [11]
showed that adversarial examples generated on an ensemble
of deep neural networks from a white-box attack can be
transferred to an unseen neural network. Papernot et al.
[12, 13] proposed to train a substitute model by querying
the target model. However, transfer-based attack often re-
quires a carefully-designed substitute model, or even access
to part of the training data. Moreover, they can be defended
against via training on a data set augmented by adversarial
examples from multiple static pre-trained models [17]. InarXiv:1904.02144v5 [cs.LG] 28 Apr 2020recent work, Brendel et al. [14] proposed Boundary Attack,
which generates adversarial examples via rejection sampling.
While relying neither on training data nor on the assumption
of transferability, this attack method achieves comparable
performance with state-of-the-art white-box attacks such as
C&W attack [6]. One limitation of Boundary Attack, however,
is that it was formulated only for `2-distance. Moreover, it
requires a relatively large number of model queries, rendering
it impractical for real-world applications.
It is more realistic to evaluate the vulnerability of a machine
learning system under the decision-based attack with a limited
budget of model queries. Online image classiÔ¨Åcation platforms
often set a limit on the allowed number of queries within
a certain time period. For example, the cloud vision API
from Google currently allow 1,800 requests per minute. Query
inefÔ¨Åciency thus leads to clock-time inefÔ¨Åciency and prevents
an attacker from carrying out large-scale attacks. A system
may also be set to recognize the behavior of feeding a large
number of similar queries within a small amount of time as
a fraud, which will automatically Ô¨Ålter out query-inefÔ¨Åcient
decision-based attacks. Last but not least, a smaller query
budget directly implies less cost in evaluation and research.
Query-efÔ¨Åcient algorithms help save the cost of evaluating the
robustness of public platforms, which incur a cost for each
query made by the attacker. It also helps facilitate research
in adversarial vulnerability, as such a decision-based attack
which does not require access to model details may be used
as a simple and efÔ¨Åcient Ô¨Årst step in evaluating new defense
mechanisms, as we will see in Section V-B and Appendix C.
In this paper, we study decision-based attacks under an opti-
mization framework, and propose a novel family of algorithms
for generating both targeted and untargeted adversarial exam-
ples that are optimized for minimum distance with respect to
either the`2-distance or `1distance. The family of algorithms
is iterative in nature, with each iteration involving three steps:
estimation of the gradient direction, step-size search via geo-
metric progression, and Boundary search via a binary search.
Theoretical analysis has been carried out for the optimization
framework and the gradient direction estimate, which not
only provides insights for choosing hyperparamters, but also
motivating essential steps in the proposed algorithms. We
refer to the algorithm as HopSkipJumpAttack1. In summary,
our contributions are the following:
We propose a novel unbiased estimate of gradient direction
at the decision boundary based solely on access to model
decisions, and propose ways to control the error from
deviation from the boundary.
We design a family of algorithms, HopSkipJumpAttack,
based on the proposed estimate and our analysis, which
is hyperparameter-free, query-efÔ¨Åcient and equipped with a
1A hop, skip, and a jump originally referred to an exercise or game
involving these movements dating from the early 1700s, but by the mid-1800s
it was also being used Ô¨Åguratively for the short distance so covered.convergence analysis.
We demonstrate the superior efÔ¨Åciency of our algorithm
over several state-of-the-art decision-based attacks through
extensive experiments.
Through the evaluation of several defense mechanisms such
as defensive distillation, region-based classiÔ¨Åcation, adver-
sarial training and input binarization, we suggest our attack
can be used as a simple and efÔ¨Åcient Ô¨Årst step for researchers
to evaluate new defense mechanisms.
Roadmap. In Section II, we describe previous work on
decision-based adversarial attacks and their relationship to our
algorithm. We also discuss the connection of our algorithm
to zeroth-order optimization. In Section III, we propose and
analyze a novel iterative algorithm which requires access to
the gradient information. Each step carries out a gradient
update from the boundary, and then projects back to the
boundary again. In Section IV, we introduce a novel asymp-
totically unbiased gradient-direction estimate at the boundary,
and a binary-search procedure to approach the boundary. We
also discuss how to control errors with deviation from the
boundary. The analysis motivates a decision-based algorithm,
HopSkipJumpAttack (Algorithm 2). Experimental results are
provided in Section V. We conclude in Section VI with a
discussion of future work.
II. R ELATED WORK
A. Decision-based attacks
Most related to our work is the Boundary Attack method
introduced by Brendel et al. [14]. Boundary Attack is an
iterative algorithm based on rejective sampling, initialized
at an image that lies in the target class. At each step, a
perturbation is sampled from a proposal distribution, which
reduces the distance of the perturbed image towards the
original input. If the perturbed image still lies in the target
class, the perturbation is kept. Otherwise, the perturbation is
dropped. Boundary Attack achieves performance comparable
to state-of-the-art white-box attacks on deep neural networks
for image classiÔ¨Åcation. The key obstacle to its practical
application is, however, the demand for a large number of
model queries. In practice, the required number of model
queries for crafting an adversarial example directly determines
the level of the threat imposed by a decision-based attack.
One source of inefÔ¨Åciency in Boundary Attack is the rejection
of perturbations which deviate from the target class. In our
algorithm, the perturbations are used for estimation of a
gradient direction.
Several other decision-based attacks have been proposed to
improve efÔ¨Åciency. Brunner et al. [15] introduced Biased
Boundary Attack, which biases the sampling procedure by
combining low-frequency random noise with the gradientfrom a substitute model. Biased Boundary Attack is able to
signiÔ¨Åcantly reduce the number of model queries. However,
it relies on the transferability between the substitute model
and the target model, as with other transfer-based attacks.
Our algorithm does not rely on the additional assumption of
transferability. Instead, it achieves a signiÔ¨Åcant improvement
over Boundary Attack through the exploitation of discarded
information into the gradient-direction estimation. Ilyas et al.
[9] proposed Limited attack in the label-only setting, which
directly performs projected gradient descent by estimating
gradients based on novel proxy scores. Cheng et al. [16]
introduced Opt attack, which transforms the original prob-
lem to a continuous version, and solves the new problem
via randomized zeroth-order gradient update. Our algorithm
approaches the original problem directly via a novel gradient-
direction estimate, leading to improved query efÔ¨Åciency over
both Limited Attack and Opt Attack. The majority of model
queries in HopSkipJumpAttack come in mini-batches, which
also leads to improved clock-time efÔ¨Åciency over Boundary
Attack.
B. Zeroth-order optimization
Zeroth-order optimization refers to the problem of optimizing
a functionfbased only on access to function values f(x),
as opposed to gradient values rf(x). Such problems have
been extensively studied in the convex optimization and bandit
literatures. Flaxman et al. [18] studied one-point randomized
estimate of gradient for bandit convex optimization. Agarwal
et al. [19] and Nesterov and Spokoiny [20] demonstrated that
faster convergence can be achieved by using two function
evaluations for estimating the gradient. Duchi et al. [21]
established optimal rates of convex zeroth-order optimization
via mirror descent with two-point gradient estimates. Zeroth-
order algorithms have been applied to the generation of ad-
versarial examples under the score-based threat model [8‚Äì10].
Subsequent work [22] proposed and analyzed an algorithm
based on variance-reduced stochastic gradient estimates.
We formulate decision-based attack as an optimization prob-
lem. A core component of our proposed algorithm is a
gradient-direction estimate, the design of which is moti-
vated by zeroth-order optimization. However, the problem
of decision-based attack is more challenging than zeroth-
order optimization, essentially because we only have binary
information from output labels of the target model, rather than
function values.
III. A N OPTIMIZATION FRAMEWORK
In this section, we describe an optimization framework for
Ô¨Ånding adversarial instances for an m-ary classiÔ¨Åcation model
of the following type. The Ô¨Årst component is a discriminant
functionF:Rd!Rmthat accepts an input x2[0;1]dandproduces an output y2m:=fy2[0;1]mjPm
c=1yc= 1g.
The output vector y= (F1(x);:::;Fm(x))can be viewed as
a probability distribution over the label set [m] =f1;:::;mg.
Based on the function F, the classiÔ¨Åer C:Rd![m]assigns
inputxto the class with maximum probability‚Äîthat is,
C(x):= arg max
c2[m]Fc(x):
We study adversaries of both the untargeted and targeted
varieties. Given some input x?, the goal of an untargeted attack
is to change the original classiÔ¨Åer decision c?:=C(x?)to
anyc2[m]nfc?g, whereas the goal of a targeted attack is
to change the decision to some pre-speciÔ¨Åed cy2[m]nfc?g.
Formally, if we deÔ¨Åne the function Sx?:Rd!Rvia
Sx?(x0):=8
<
:max
c6=c?Fc(x0) Fc?(x0)(Untargeted)
Fcy(x0) max
c6=cyFc(x0)(Targeted)(1)
then a perturbed image x0is a successful attack if and
only ifSx?(x0)>0. The boundary between successful and
unsuccessful perturbed images is
bd(Sx?):=
z2[0;1]djSx?(z) = 0 
:
As an indicator of successful perturbation, we introduce the
Boolean-valued function x?: [0;1]d!f  1;1gvia
x?(x0):= sign (Sx?(x0)) =(
1 ifSx?(x0)>0,
 1otherwise.
This function is accessible in the decision-based setting, as it
can be computed by querying the classiÔ¨Åer Calone. The goal
of an adversarial attack is to generate a perturbed sample x0
such thatx?(x0) = 1 , while keeping x0close to the original
samplex?. This can be formulated as the optimization problem
min
x0d(x0;x?)such thatx?(x0) = 1; (2)
wheredis a distance function that quantiÔ¨Åes similarity.
Standard choices of dstudied in past work [2, 5, 6] include
the usual`p-norms, for p2f0;2;1g.
A. An iterative algorithm for `2distance
Consider the case of the optimization problem (2) with the
`2-normd(x;x?) =kx x?k2. We Ô¨Årst specify an iterative
algorithm that is given access to the gradient rSx?. Given an
initial vector x0such thatSx?(x0)>0and a stepsize sequence
ftgt0, it performs the update
xt+1=tx?+ (1 t)
xt+trSx?(xt)
krSx?(xt)k2
; (3)
wheretis a positive step size. Here the line search parameter
t2[0;1]is chosen such that Sx?(xt+1) = 0 ‚Äîthat is, so that
the next iterate xt+1lies on the boundary. The motivation for
this choice is that our gradient-direction estimate in Section IV
is only valid near the boundary.We now analyze this algorithm with the assumption that we
have access to the gradient of Sx?in the setting of binary clas-
siÔ¨Åcation. Assume that the function Sx?is twice differentiable
with a locally Lipschitz gradient, meaning that there exists
L>0such that for all x;y2fz:kz x?k2kx0 x?k2g,
we have
krSx?(x) rSx?(y)k2Lkx yk2; (4)
In addition, we assume the gradient is bounded away from
zero on the boundary: there exists a positive ~C > 0such that
krSx?(z)k>~Cfor anyz2bd(Sx?).
We analyze the behavior of the updates (3) in terms of the
angular measure
r(xt;x?):= cos\(xt x?;rSx?(xt))
=
xt x?;rSx?(xt)
kxt x?k2krSx?(xt)k2;
corresponding to the cosine of the angle between xt x?and
the gradientrSx?(xt). Note that the condition r(x;x?) = 1
holds if and only if xis a stationary point of the optimiza-
tion (2). The following theorem guarantees that, with a suitable
step size, the updates converge to such a stationary point:
Theorem 1. Under the previously stated conditions on Sx?,
suppose that we compute the updates (3)with step size
t=kxt x?k2t qfor someq2 1
2;1
. Then there is a
universal constant csuch that
01 r(xt;x?)ctq 1fort= 1;2;:::. (5)
In particular, the algorithm converges to a stationary point of
problem (2).
Theorem 1 suggests a scheme for choosing the step size
in the algorithm that we present in the next section. An
experimental evaluation of the proposed scheme is carried
out in Appendix B. The proof of the theorem is constructed
by establishing the relationship between the objective value
d(xt;x?)andr(xt;x?), with a second-order Taylor approxi-
mation to the boundary. See Appendix A-A for details.
B. Extension to `1-distance
We now describe how to extend these updates so as to
minimize the `1-distance. Consider the `2-projection of a
pointxonto the sphere of radius tcentered atx?:
2
x?;t(x):= arg min
ky x?k2tky xk2=tx?+ (1 t)x:(6)
In terms of this operator, our `2-based update (3) can be
rewritten in the equivalent form
xt+1= 2
x?;t
xt+trSx?(xt)
krSx?(xt)k2
: (7)This perspective allows us to extend the algorithm to other
`p-norms forp6= 2. For instance, in the case p=1, we can
deÔ¨Åne the`1-projection operator 1
x?;. It performs a per-
pixel clip within a neighborhood of x?, such that the ith entry
of1
x?;(x)is
1
x?;(x)i:= maxfminfx?
i;x?
i+cg;xi cg;
wherec:=kx x?k1. We propose the `1-version of our
algorithm by carrying out the following update iteratively:
xt+1= 1
x?;t 
xt+tsign(rSx?(xt))
; (8)
wheretis chosen such that Sx?(xt+1) = 0 , and ‚Äúsign‚Äù
returns the element-wise sign of a vector. We use the sign
of the gradient for faster convergence in practice, similar to
previous work [2, 3, 7].
IV. A DECISION -BASED ALGORITHM BASED ON A NOVEL
GRADIENT ESTIMATE
We now extend our procedures to the decision-based setting,
in which we have access only to the Boolean-valued function
x?(x) = sign(Sx?(x))‚Äîthat is, the method cannot observe
the underlying discriminant function For its gradient. In this
section, we introduce a gradient-direction estimate based on
x?whenxt2bd(Sx?)(so thatSx?(xt) = 0 by deÔ¨Ånition).
We proceed to discuss how to approach the boundary. Then
we discuss how to control the error of our estimate with a
deviation from the boundary. We will summarize the analysis
with a decision-based algorithm.
A. At the boundary
Given an iterate xt2bd(Sx?)we propose to approximate
the direction of the gradient rSx?(xt)via the Monte Carlo
estimate
grS(xt;):=1
BBX
b=1x?(xt+ub)ub; (9)
wherefubgB
b=1are i.i.d. draws from the uniform distribution
over thed-dimensional sphere, and is small positive param-
eter. (The dependence of this estimator on the Ô¨Åxed centering
pointx?is omitted for notational simplicity.)
The perturbation parameter is necessary, but introduces a
form of bias in the estimate. Our Ô¨Årst result controls this
bias, and shows that grS(xt;)is asymptotically unbiased as
!0+.
Theorem 2. For a boundary point xt, suppose that Sx?has
L-Lipschitz gradients in a neighborhood of xt. Then the cosine
of the angle between grS(xt;)andrSx?(xt)is bounded as
cos\
E[grS(xt;)];rSx?(xt)
1 9L22d2
8krS(xt)k2
2:(10)In particular, we have
lim
!0cos\
E[grS(xt;)];rSx?(xt)
= 1; (11)
showing that the estimate is asymptotically unbiased as an
estimate of direction.
We remark that Theorem 2 only establishes the asymptotic
behavior of the proposed estiamte at the boundary. This also
motivates the boundary search step in our algorithm to be
discussed in Seciton IV-B. The proof of Theorem 2 starts
from dividing the unit sphere into three components: the upper
cap along the direction of gradient, the lower cap opposite to
the direction of gradient, and the annulus in between. The
error from the annulus can be bounded when is small. See
Appendix A-B for the proof of this theorem. As will be seen
in the sequel, the size of perturbation should be chosen
proportionally to d 1; see Section IV-C for details.
B. Approaching the boundary
The proposed estimate (9) is only valid at the boundary. We
now describe how we approach the boundary via a binary
search. Let ~xtdenote the updated sample before the operator
p
x;tis applied:
~xt:=xt+tvt(xt;t);such that (12)
vt(xt;t) =(drS(xt;t)=kdrS(xt;t)k2;ifp= 2;
sign(drS(xt;t));ifp=1;
wheredrSwill be introduced later in equation (16), as a
variance-reduced version of grS, andtis the size of per-
turbation at the t-th step.
We hope ~xtis at the opposite side of the boundary to x
so that the binary search can be carried out. Therefore, we
initialize at ~x0at the target side with x?(~x0) = 1 , and set
x0:= p
x;0(~x0), where0is chosen via a binary search
between 0and1to approach the boundary, stopped at x0lying
on the target side with x?(x0) = 1 . At thet-th iteration, we
start atxtlying at the target side x?(xt) = 1 . The step size
is initialized as
t:=kxt x?kp=p
t; (13)
as suggested by Theorem 1 in the `2case, and is decreased by
half untilx?(~xt) = 1 , which we call geometric progression of
t. Having found an appropriate ~xt, we choose the projection
radiustvia a binary search between 0and1to approach
the boundary, which stops at xt+1withx?(xt+1) = 1 . See
Algorithm 1 for the complete binary search, where the binary
search threshold is set to be some small constant.
Figure 2: Intuitive explanation of HopSkipJumpAttack. (a)
Perform a binary search to Ô¨Ånd the boundary, and then update
~xt!xt. (b) Estimate the gradient at the boundary point xt.
(c) Geometric progression and then update xt!~xt+1. (d)
Perform a binary search, and then update ~xt+1!xt+1.
Algorithm 1 Bin-Search
Require: Samplesx0;x, with a binary function , such that
(x0) = 1;(x) = 0 , threshold, constraint `p.
Ensure: A samplex00near the boundary.
Setl= 0 andu= 1.
whilejl uj> do
Setm l+u
2.
if(x;m(x0)) = 1 then
Setu m.
else
Setl m.
end if
end while
Outputx00= x;u(x0).
C. Controlling errors of deviations from the boundary
Binary search never places xt+1exactly onto the boundary.
We analyze the error of the gradient-direction estimate, and
propose two approaches for reducing the error.
a) Appropriate choice of the size of random perturbation:
First, the size of random perturbation tfor estimating the
gradient direction is chosen as a function of image size dand
the binary search threshold . This is different from numerical
differentiation, where the optimal choice of tis at the scale of
round-off errors (e.g., [23]). Below we characterize the error
incurred by a large tas a function of distance between ~xt
and the boundary, and derive the appropriate choice of tand
t. In fact, with a Taylor approximation of Sx?atxt, we have
Sx?(xt+tu) =Sx?(xt) +t
rSx?(xt); u
+O(2
t):
At the boundary Sx?(xt) = 0 , the error of gradient approxi-
mation scales atO(2
t), which is minimized by reducing tto
the scale of rooted round-off error. However, the outcome xt
of a Ô¨Ånite-step binary search lies close to, but not exactly on
the boundary.
Whentis small enough such that second-order terms can
be omitted, the Ô¨Årst-order Taylor approximation implies thatx?(xt+tu) = 1if and only if xt+tulies on the spherical
capC, with
C:=n
ujDrSx?(xt)
krSx?(xt)k2;uE
<  1
tSx?(xt)
krSx?(xt)k2o
:
On the other hand, the probability mass of uconcentrates on
the equator in a high-dimensional sphere, which is character-
ized by the following inequality [24]:
P(u2C)2
cexpf c2
2g;wherec=p
d 2Sx?(xt)
tkrSx?(xt)k2:(14)
A Taylor expansion of xtatx0
t:= 2
@(xt)yields
Sx?(xt) =rSx?(x0
t)T(xt x0
t) +O(kxt x0
tk2
2)
=rSx?(xt)T(xt x0
t) +O(kxt x0
tk2
2):
By the Cauchy-Schwarz inequality and the deÔ¨Ånition of `2-
projection, we have
jrSx?(xt)T(xt x0
t)j
krSx?(xt)k2kxt 2
@(xt)k2
(
krSx?(xt)k2k~xt 1 x?kp;ifp= 2;
krSx?(xt)k2k~xt 1 x?kpp
d;ifp=1:
This yields
c=O(dqk~xt 1 x?kp
t);
whereq= 1 (1=p)is the dual exponent. In order to avoid
a loss of accuracy from concentration of measure, we let
t=dqk~xt 1 x?k2. To make the approximation error
independent of dimension d, we setat the scale of d q 1,
so thattis proportional to d 1, as suggested by Theorem 2.
This leads to a logarithmic dependence on dimension for the
number of model queries. In practice, we set
=d q 1;t=d 1k~xt 1 x?kp: (15)
b) A baseline for variance reduction in gradient-direction
estimation: Another source of error comes from the variance
of the estimate, where we characterize variance of a random
vectorv2Rdby the trace of its covariance operator:
Var(v) :=Pd
i=1Var(vi). Whenxtdeviates from the boundary
andtis not exactly zero, there is an uneven distribution of
perturbed samples at the two sides of the boundary:
jE[x?(xt+tu)]j>0;
as we can see from Equation (14). To attempt to control the
variance, we introduce a baseline x?into the estimate:
x?:=1
BBX
b=1x?(xt+ub);
which yields the following estimate:
drS(xt;) :=1
B 1BX
b=1(x?(xt+ub) x?)ub:(16)Algorithm 2 HopSkipJumpAttack
Require: ClassiÔ¨ÅerC, a samplex, constraint`p, initial batch
sizeB0, iterationsT.
Ensure: Perturbed image xt.
Set(Equation (15)).
Initialize at ~x0withx?(~x0) = 1 .
Computed0=k~x0 x?kp.
fortin1;2;:::;T 1do
(Boundary search )
xt=BIN-SEARCH (~xt 1;x;;x?;p)
(Gradient-direction estimation )
SampleBt=B0p
tunit vectors u1;:::;uBt.
Sett(Equation (15)).
Computevt(xt;t)(Equation (12)).
(Step size search )
Initialize step size t=kxt x?kp=p
t.
whilex?(xt+"tvt) = 0 do
t t=2.
end while
Set~xt=xt+tvt.
Computedt=k~xt x?kp.
end for
Outputxt=BIN-SEARCH (~xt 1;x;;x?;p).
It can be easily observed that this estimate is equal to the
previous estimate in expectation, and thus still asymptotically
unbiased at the boundary: When xt2bd(Sx?), we have
cos\
E[drS(xt;)];rSx?(xt)
1 9L22d2
8krS(xt)k2
2;
lim
!0cos\
E[drS(xt;)];rSx?(xt)
= 1:
Moreover, the introduction of the baseline reduces the variance
whenE[x?(xt+u)]deviates from zero. In particular, the
following theorem shows that whenever jE[x?(xt+u)]j=

(B 1
2), the introduction of a baseline reduces the variance.
Theorem 3. DeÔ¨Åning2:=Var(x?(xt+u)u)as the
variance of one-point estimate, we have
Var(drS(xt;))<Var(grS(xt;))(1  );
where
 =2
2(B 1) 
2BE[x?(xt+u)]2 1
 2B 1
(B 1)2:
See Appendix A-C for the proof. We also present an experi-
mental evaluation of our gradient-direction estimate when the
sample deviates from the boundary in Appendix B, where
we show our proposed choice of tand the introduction of
baseline yield a performance gain in estimating gradient.
D. HopSkipJumpAttack
We now combine the above analysis into an iterative algorithm,
HopSkipJumpAttack. It is initialized with a sample in theTable I: Median distance at various model queries. The smaller median distance at a given model query is bold-faced. BA and
HSJA stand for Boundary Attack and HopSkipJumpAttack respectively.
Distance Data Model ObjectiveModel Queries
1K 5K 20K
BA Opt HSJA BA Opt HSJA BA Opt HSJA
`2MNIST CNNUntargeted 6.14 6.79 2.46 5.45 3.76 1.67 1.50 2.07 1.48
Targeted 5.41 4.84 3.26 5.38 3.90 2.24 1.98 2.49 1.96
CIFAR10ResNetUntargeted 2.78 2.07 0.56 2.34 0.77 0.21 0.27 0.29 0.13
Targeted 7.83 8.21 2.53 5.91 4.76 0.41 0.59 1.06 0.21
DenseNetUntargeted 2.57 1.78 0.48 2.12 0.67 0.18 0.21 0.28 0.12
Targeted 7.70 7.65 1.75 5.33 3.47 0.34 0.35 0.78 0.19
CIFAR100ResNetUntargeted 1.34 1.20 0.20 1.12 0.41 0.08 0.10 0.14 0.06
Targeted 9.30 12.43 6.12 7.40 8.34 0.92 1.61 4.06 0.29
DenseNetUntargeted 1.47 1.22 0.25 1.23 0.34 0.11 0.12 0.13 0.08
Targeted 8.83 11.72 5.10 6.76 8.22 0.75 0.91 2.89 0.26
ImageNet ResNetUntargeted 36.86 33.60 9.75 31.95 13.91 2.30 2.71 5.26 0.84
Targeted 87.49 84.38 71.99 82.91 71.83 38.79 40.92 53.78 10.95
`1MNIST CNNUntargeted 0.788 0.641 0.235 0.700 0.587 0.167 0.243 0.545 0.136
Targeted 0.567 0.630 0.298 0.564 0.514 0.211 0.347 0.325 0.175
CIFAR10ResNetUntargeted 0.127 0.128 0.023 0.105 0.096 0.008 0.019 0.073 0.005
Targeted 0.379 0.613 0.134 0.289 0.353 0.028 0.038 0.339 0.010
DenseNetUntargeted 0.114 0.119 0.017 0.095 0.078 0.007 0.017 0.063 0.004
Targeted 0.365 0.629 0.130 0.249 0.359 0.022 0.025 0.338 0.008
CIFAR100ResNetUntargeted 0.061 0.077 0.009 0.051 0.055 0.004 0.008 0.040 0.002
Targeted 0.409 0.773 0.242 0.371 0.472 0.124 0.079 0.415 0.019
DenseNetUntargeted 0.065 0.076 0.010 0.055 0.038 0.005 0.010 0.030 0.003
Targeted 0.388 0.750 0.248 0.314 0.521 0.096 0.051 0.474 0.017
ImageNet ResNetUntargeted 0.262 0.287 0.057 0.234 0.271 0.017 0.030 0.248 0.007
Targeted 0.615 0.872 0.329 0.596 0.615 0.219 0.326 0.486 0.091
target class for untargeted attack, and with a sample blended
with uniform noise that is misclassiÔ¨Åed for targeted attack.
Each iteration of the algorithm has three components. First,
the iterate from the last iteration is pushed towards the bound-
ary via a binary search (Algorithm 1). Second, the gradient
direction is estimated via Equation (16). Third, the updating
step size along the gradient direction is initialized as Equa-
tion (13) based on Theorem 1, and is decreased via geometric
progression until perturbation becomes successful. The next
iteration starts with projecting the perturbed sample back to
the boundary again. The complete procedure is summarized
in Algorithm 2. Figure 2 provides an intuitive visualization
of the three steps in `2. For all experiments, we initialize the
batch size at 100 and increase it withp
tlinearly, so that
the variance of the estimate reduces with t. When the input
domain is bounded in practice, a clip is performed at each step
by default.
V. E XPERIMENTS
In this section, we carry out experimental analysis of
HopSkipJumpAttack. We compare the efÔ¨Åciency of Hop-
SkipJumpAttack with several previously proposed decision-
based attacks on image classiÔ¨Åcation tasks. In addition, we
evaluate the robustness of three defense mechanisms under
our attack method. All experiments were carried out on a Tesla
K80 GPU, with code available online.2Our algorithm is also
2See https://github.com/Jianbo-Lab/HSJA/.available on CleverHans [25] and Foolbox [26], which are
two popular Python packages to craft adversarial examples
for machine learning models.
A. EfÔ¨Åciency evaluation
a) Baselines: We compare HopSkipJumpAttack with three
state-of-the-art decision-based attacks: Boundary Attack [14],
Limited Attack [9] and Opt Attack [16]. We use the imple-
mentation of the three algorithms with the suggested hyper-
parameters from the publicly available source code online.
Limited Attack is only included under the targeted `1setting,
as in Ilyas et al. [9].
b) Data and models: For a comprehensive evaluation of
HopSkipJumpAttack, we use a wide range of data and models,
with varied image dimensions, data set sizes, complexity levels
of task and model structures.
The experiments are carried out over four image data sets:
MNIST, CIFAR-10 [27], CIFAR-100 [27], and ImageNet [28]
with the standard train/test split [29]. The four data sets have
varied image dimensions and class numbers. MNIST contains
70K 2828gray-scale images of handwritten digits in the
range 0-9. CIFAR-10 and CIFAR-100 are both composed of
32323images. CIFAR-10 has 10 classes, with 6K images
per class, while CIFAR-100 has 100 classes, with 600 images
per class. ImageNet has 1;000 classes. Images in ImageNet
are rescaled to 2242243. For MNIST, CIFAR-10 and0K 2K 4K 6K 8K 10K
Number of Queries100101/lscript2DistanceUntargeted /lscript2(MNIST, CNN)
0K 2K 4K 6K 8K 10K
Number of Queries100101/lscript2DistanceTargeted/lscript2(MNIST, CNN)
0K 2K 5K 7K 10K
Number of Queries10‚àí1100
2√ó10‚àí13√ó10‚àí14√ó10‚àí16√ó10‚àí1/lscript‚àûDistanceUntargeted /lscript‚àû(MNIST, CNN)
0K 2K 5K 7K 10K
Number of Queries10‚àí1100
2√ó10‚àí13√ó10‚àí14√ó10‚àí16√ó10‚àí1/lscript‚àûDistanceTargeted/lscript‚àû(MNIST, CNN)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí210‚àí1100101/lscript2DistanceUntargeted /lscript2(CIFAR10, ResNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí210‚àí1100101/lscript2DistanceTargeted/lscript2(CIFAR10, ResNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí310‚àí210‚àí1100/lscript‚àûDistanceUntargeted /lscript‚àû(CIFAR10, ResNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí310‚àí210‚àí1100/lscript‚àûDistanceTargeted/lscript‚àû(CIFAR10, ResNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí210‚àí1100101/lscript2DistanceUntargeted /lscript2(CIFAR10, DenseNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí210‚àí1100101/lscript2DistanceTargeted/lscript2(CIFAR10, DenseNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí310‚àí210‚àí1100/lscript‚àûDistanceUntargeted /lscript‚àû(CIFAR10, DenseNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí310‚àí210‚àí1100/lscript‚àûDistanceTargeted/lscript‚àû(CIFAR10, DenseNet)
Figure 3: Median distance versus number of model queries on MNIST with CNN, and CIFAR-10 with ResNet and DenseNet
from top to bottom rows. 1st column: untargeted `2. 2nd col.: targeted `2. 3rd col.: untargeted `1. 4th col.: targeted `1.
CIFAR-100, 1;000 correctly classiÔ¨Åed test images are used,
which are randomly drawn from the test data set, and evenly
distributed across classes. For ImageNet, we use 100correctly
classiÔ¨Åed test images, evenly distributed among 10randomly
selected classes. The selection scheme follows Metzen et al.
[30] for reproducibility.
We also use models of varied structure, from simple to
complex. For MNIST, we use a simple convolutional network
composed of two convolutional layers followed by a hidden
dense layer with 1024 units. Two convolutional layers have
32;64Ô¨Ålters respectively, each of which is followed by a
max-pooling layer. For both CIFAR-10 and CIFAR-100, we
train a 20-layer ResNet [31] and 121-layer DenseNet [32]
respectively, with the canonical network structure [29]. For
ImageNet, we use a pre-trained 50-layer ResNet [31]. All mod-
els achieve close to state-of-the-art accuracy on the respective
data set. All pixels are scaled to be in the range [0;1]. For
all experiments, we clip the perturbed image into the input
domain [0;1]for all algorithms by default.
c) Initialization: For untargeted attack, we initialize all at-
tacks by blending an original image with uniform random
noise, and increasing the weight of uniform noise gradually
until it is misclassiÔ¨Åed, a procedure which is available on
Foolbox [26], as the default initialization of Boundary Attack.For targeted attack, the target class is sampled uniformly
among the incorrect labels. An image belonging to the target
class is randomly sampled from the test set as the initialization.
The same target class and a common initialization image are
used for all attacks.
d) Metrics: The Ô¨Årst metric is the median `pdistance between
perturbed and original samples over a subset of test images,
which was commonly used in previous work, such as Carlini
and Wagner [6]. A version normalized by image dimension
was employed by Brendel et al. [14] for evaluating Boundary
Attack. The `2distance can be interpreted in the following
way: Given a byte image of size hw3, perturbation of size
din`2distance on the rescaled input image amounts to per-
turbation on the original image of dd=p
hw3255ebits
per pixel on average, in the range [0;255]. The perturbation
of sizedin`1distance amounts to a maximum perturbation
ofd255debits across all pixels on the raw image.
As an alternative metric, we also plot the success rate at
various distance thresholds for both algorithms given a limited
budget of model queries. An adversarial example is deÔ¨Åned a
success if the size of perturbation does not exceed a given
distance threshold. The success rate can be directly related
to the accuracy of a model on perturbed data under a given0K 5K 10K 15K 20K 25K
Number of Queries10‚àí210‚àí1100101/lscript2DistanceUntargeted /lscript2(CIFAR100, ResNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí210‚àí1100101/lscript2DistanceTargeted/lscript2(CIFAR100, ResNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí310‚àí210‚àí1100/lscript‚àûDistanceUntargeted /lscript‚àû(CIFAR100, ResNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí310‚àí210‚àí1100/lscript‚àûDistanceTargeted/lscript‚àû(CIFAR100, ResNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí210‚àí1100101/lscript2DistanceUntargeted /lscript2(CIFAR100, DenseNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí210‚àí1100101/lscript2DistanceTargeted/lscript2(CIFAR100, DenseNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí310‚àí210‚àí1100/lscript‚àûDistanceUntargeted /lscript‚àû(CIFAR100, DenseNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí310‚àí210‚àí1100/lscript‚àûDistanceTargeted/lscript‚àû(CIFAR100, DenseNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí1100101102/lscript2DistanceUntargeted /lscript2(ImageNet, ResNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí1100101102/lscript2DistanceTargeted/lscript2(ImageNet, ResNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí310‚àí210‚àí1100/lscript‚àûDistanceUntargeted /lscript‚àû(ImageNet, ResNet)
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí310‚àí210‚àí1100/lscript‚àûDistanceTargeted/lscript‚àû(ImageNet, ResNet)
Figure 4: Median distance versus number of model queries on CIFAR-100 with ResNet, DenseNet, and ImageNet with ResNet
from top to bottom rows. 1st column: untargeted `2. 2nd col.: targeted `2. 3rd col.: untargeted `1. 4th col.: targeted `1.
distance threshold:
perturbed acc. =original acc.(1 success rate ):(17)
Throughout the experiments, we limit the maximum budget of
queries per image to 25,000, the setting of practical interest,
due to limited computational resources.
e) Results: Figure 3 and 4 show the median distance (on a
log scale) against the queries, with the Ô¨Årst and third quartiles
used as lower and upper error bars. For Boundary, Opt and
HopSkipJumpAttack, Table I summarizes the median distance
when the number of queries is Ô¨Åxed at 1,000, 5,000, and
20,000 across all distance types, data, models and objectives.
Figure 5 and 6 show the success rate against the distance
threshold. Figure 3 and 5 contain results on MNIST with CNN,
and CIFAR-10 with ResNet, Denset, subsequently from the
top row to the bottom row. Figure 4 and 6 contain results on
CIFAR-100 with ResNet and DenseNet, and ImageNet with
ResNet, subsequently from the top row to the bottom row. The
four columns are for untargeted `2, targeted`2, untargeted `1
and targeted `1attacks respectively.
With a limited number of queries, HopSkipJumpAttack is
able to craft adversarial examples of a signiÔ¨Åcantly smaller
distance with the corresponding original examples across all
data sets, followed by Boundary Attack and Opt Attack. As aconcrete example, Table I shows that untargeted `2-optimized
HopSkipJumpAttack achieves a median distance of 0:559 on
CIFAR-10 with a ResNet model at 1;000 queries, which
amounts to below 3=255 per pixel on average. At the same
budget of queries, Boundary Attack and Opt Attack only
achieve median `2-distances of 2:78and2:07respectively.
The difference in efÔ¨Åciency becomes more signiÔ¨Åcant for
`1attacks. As shown in Figure 5, under an untargeted `1-
optimized HopSkipJumpAttack with 1,000 queries, all pixels
are within an 8=255-neighborhood of the original image for
around 70% of adversarial examples, a success rate achieved
by Boundary Attack only after 20,000 queries.
By comparing the odd and even columns of Figure 3-6,
we can Ô¨Ånd that targeted HopSkipJumpAttack takes more
queries than the untargeted one to achieve a comparable
distance. This phenomenon becomes more explicit on CIFAR-
100 and ImageNet, which have more classes. With the same
number of queries, there is an order-of-magnitude difference
in median distance between untargeted and targeted attacks
(Figure 3 and 4). For `2-optimized HopSkipJumpAttack, while
the untargeted version is able to craft adversarial images by
perturbing 4bits per pixel on average within 1,000 queries
for70% 90% of images in CIFAR-10 and CIFAR-100,
the targeted counterpart takes 2,000-5,000 queries. The other
attacks fail to achieve a comparable performance even with0 2 4 6/lscript2Distance0.00.20.40.60.81.0Success Rate
1K 1K 1K2K
2K2K10K10K 10KUntargeted /lscript2(MNIST, CNN)
0 2 4 6/lscript2Distance0.00.20.40.60.81.0Success Rate
1K 1K 1K2K
2K2K10K
10K10KTargeted/lscript2(MNIST, CNN)
0.0 0.1 0.2 0.3 0.4 0.5/lscript‚àûDistance0.00.20.40.60.81.0Success Rate
1K 1K 1K2K
2K2K10K
10K
10KUntargeted /lscript‚àû(MNIST, CNN)
0.0 0.1 0.2 0.3 0.4 0.5/lscript‚àûDistance0.00.20.40.60.81.0Success Rate
1K1K1K1K2K
2K2K2K10K
10K10K10KTargeted/lscript‚àû(MNIST, CNN)
0.0 0.5 1.0 1.5 2.0/lscript2Distance0.00.20.40.60.81.0Success Rate
1K1K1K5K
5K5K25K 25K 25KUntargeted /lscript2(CIFAR10, ResNet)
0.0 0.5 1.0 1.5 2.0/lscript2Distance0.00.20.40.60.81.0Success Rate
1K 1K 1K5K
5K5K25K25K
25KTargeted/lscript2(CIFAR10, ResNet)
0.00 0.05 0.10 0.15 0.20/lscript‚àûDistance0.00.20.40.60.81.0Success Rate 1K
1K1K5K
5K5K25K 25K25KUntargeted /lscript‚àû(CIFAR10, ResNet)
0.00 0.05 0.10 0.15 0.20/lscript‚àûDistance0.00.20.40.60.81.0Success Rate
1K1K1K1K5K
5K5K5K25K25K
25K25KTargeted/lscript‚àû(CIFAR10, ResNet)
0.0 0.5 1.0 1.5 2.0/lscript2Distance0.00.20.40.60.81.0Success Rate
1K
1K1K5K
5K5K25K 25K 25KUntargeted /lscript2(CIFAR10, DenseNet)
0.0 0.5 1.0 1.5 2.0/lscript2Distance0.00.20.40.60.81.0Success Rate
1K 1K 1K5K
5K5K25K 25K
25KTargeted/lscript2(CIFAR10, DenseNet)
0.00 0.05 0.10 0.15 0.20/lscript‚àûDistance0.00.20.40.60.81.0Success Rate1K
1K1K5K
5K5K25K 25K25KUntargeted /lscript‚àû(CIFAR10, DenseNet)
0.00 0.05 0.10 0.15 0.20/lscript‚àûDistance0.00.20.40.60.81.0Success Rate
1K1K1K1K5K
5K
5K5K25K25K
25K25KTargeted/lscript‚àû(CIFAR10, DenseNet)
Figure 5: Success rate versus distance threshold for MNIST with CNN, and CIFAR-10 with ResNet, DenseNet from top to
bottom rows. 1st column: untargeted `2. 2nd column: targeted `2. 3rd column: untargeted `1. 4th column: targeted `1.
25,000 queries. On ImageNet, untargeted `2-optimized Hop-
SkipJumpAttack is able to fool the model with a perturbation
of size 6bits per pixel on average for close to 50% of
images with 1;000 queries; untargeted `1-optimized Hop-
SkipJumpAttack controls the maximum perturbation across all
pixels within 16bits for 50% images within 1;000 queries.
The targeted Boundary Attack is not able to control the
perturbation size to such a small scale until after around
25;000 queries. On the one hand, the larger query budget
requirement results from a strictly more powerful formulation
of targeted attack than untargeted attack. On the other hand,
this is also because we initialize targeted HopSkipJumpAttack
from an arbitrary image in the target class. The algorithm may
be trapped in a bad local minimum with such an initialization.
Future work can address systematic approaches to better
initialization.
As a comparison between data sets and models, we see
that adversarial images often have a larger distance to their
corresponding original images on MNIST than on CIFAR-10
and CIFAR-100, which has also been observed in previous
work (e.g., [6]). This might be because it is more difÔ¨Åcult
to fool a model on simpler tasks. On the other hand, Hop-
SkipJumpAttack also converges in a fewer number of queries
on MNIST, as is shown in Figure 3. It does not converge even
after 25;000 queries on ImageNet. We conjecture the querybudget is related to the input dimension, and the smoothness of
decision boundary. We also observe the difference in model
structure does not have a large inÔ¨Çuence on decision-based
algorithms, if the training algorithm and the data set keep the
same. For ResNet and DenseNet trained on a common data set,
a decision-based algorithm achieves comparable performance
in crafting adversarial examples, although DenseNet has a
more complex structure than ResNet.
As a comparison with state-of-the-art white-box targeted at-
tacks, C&W attack [6] achieves an average `2-distance of
0:33on CIFAR-10, and BIM [3] achieves an average `1-
distance of 0:014on CIFAR-10. Targeted HopSkipJumpAttack
achieves a comparable distance with 5K-10K model queries
on CIFAR-10, without access to model details. On ImageNet,
targeted C&W attack and BIM achieve an `2-distance of
0:96and an`1-distance of 0:01respectively. Untargeted
HopSkipJumpAttack achieves a comparable performance with
10;000 15;000queries. The targeted version is not able to
perform comparably as targeted white-box attacks when the
budget of queries is limited within 25;000.
Visualized trajectories of HopSkipJumpAttack optimized for
`2distances along varied queries on CIFAR10 and ImageNet
can be found in Figure 7. On CIFAR-10, we observe untar-
geted adversarial examples can be crafted within around 5000.0 0.5 1.0 1.5 2.0/lscript2Distance0.00.20.40.60.81.0Success Rate 1K
1K1K5K
5K5K25K 25K 25KUntargeted /lscript2(CIFAR100, ResNet)
0.0 0.5 1.0 1.5 2.0/lscript2Distance0.00.20.40.60.81.0Success Rate
1K 1K 1K5K
5K 5K25K
25K
25KTargeted/lscript2(CIFAR100, ResNet)
0.00 0.05 0.10 0.15 0.20/lscript‚àûDistance0.00.20.40.60.81.0Success Rate1K
1K1K5K
5K 5K25K 25K25KUntargeted /lscript‚àû(CIFAR100, ResNet)
0.00 0.05 0.10 0.15 0.20/lscript‚àûDistance0.00.20.40.60.81.0Success Rate
1K1K1K1K5K
5K5K5K25K
25K
25K25KTargeted/lscript‚àû(CIFAR100, ResNet)
0.0 0.5 1.0 1.5 2.0/lscript2Distance0.00.20.40.60.81.0Success Rate
1K
1K1K5K
5K5K25K 25K 25KUntargeted /lscript2(CIFAR100, DenseNet)
0.0 0.5 1.0 1.5 2.0/lscript2Distance0.00.20.40.60.81.0Success Rate
1K 1K 1K5K
5K 5K25K
25K
25KTargeted/lscript2(CIFAR100, DenseNet)
0.00 0.05 0.10 0.15 0.20/lscript‚àûDistance0.00.20.40.60.81.0Success Rate1K
1K1K5K
5K 5K25K 25K25KUntargeted /lscript‚àû(CIFAR100, DenseNet)
0.00 0.05 0.10 0.15 0.20/lscript‚àûDistance0.00.20.40.60.81.0Success Rate
1K1K1K1K5K
5K5K5K25K25K
25K25KTargeted/lscript‚àû(CIFAR100, DenseNet)
0 10 20 30/lscript2Distance0.00.20.40.60.81.0Success Rate
1K
1K1K5K
5K5K25K 25K 25KUntargeted /lscript2(ImageNet, ResNet)
0 10 20 30/lscript2Distance0.00.20.40.60.81.0Success Rate
1K 1K 1K 5K 5K 5K25K
25K
25KTargeted/lscript2(ImageNet, ResNet)
0.0 0.1 0.2 0.3 0.4/lscript‚àûDistance0.00.20.40.60.81.0Success Rate1K
1K 1K5K
5K 5K25K 25K
25KUntargeted /lscript‚àû(ImageNet, ResNet)
0.0 0.1 0.2 0.3 0.4/lscript‚àûDistance0.00.20.40.60.81.0Success Rate
1K1K1K1K5K
5K5K5K25K
25K
25K25KTargeted/lscript‚àû(ImageNet, ResNet)
Figure 6: Success rate versus distance threshold for CIFAR-100 with ResNet, DenseNet, and ImageNet with ResNet from top
to bottom rows. 1st column: untargeted `2. 2nd column: targeted `2. 3rd column: untargeted `1. 4th column: targeted `1.
queries; targeted HopSkipJumpAttack is capable of crafting
human indistinguishable targeted adversarial examples within
around 1;000 2;000 queries. On ImageNet, untargeted
HopSkipJumpAttack is able to craft good adversarial examples
with 1;000queries, while targeted HopSkipJumpAttack takes
10;000 20;000queries.
B. Defense mechanisms under decision-based attacks
We investigate the robustness of various defense mechanisms
under decision-based attacks.
a) Defense mechanisms: Three defense mechanisms are eval-
uated: defensive distillation, region-based classiÔ¨Åcation, and
adversarial training. Defensive distillation [33], a form of
gradient masking [13], trains a second model to predict the
output probabilities of an existing model of the same structure.
We use the implementaion provided by Carlini and Wagner [6]
for defensive distillation. The second defense, region-based
classiÔ¨Åcation, belongs to a wide family of mechanisms which
add test-time randomness to the inputs or the model, causing
the gradients to be randomized [34]. Multiple variants have
been proposed to randomize the gradients [35‚Äì39]. We adopt
the implementation in Cao and Gong [35] with suggested noise
levels. Given a trained base model, region-based classiÔ¨Åcation
samples points from the hypercube centered at the input image,predicts the label for each sampled point with the base model,
and then takes a majority vote to output the label. Adversarial
training [2, 3, 7, 17] is known to be one of the most effective
defense mechanisms against adversarial perturbation [34, 40].
We evaluate a publicly available model trained through a
robust optimization method proposed by Madry et al. [7].
We further evaluate our attack method by constructing a
non-differentiable model via input binarization followed by
a random forest in Appendix C. The evaluation is carried out
on MNIST, where defense mechanisms such as adversarial
training work most effectively.
b) Baselines: We compare our algorithm with state-of-the-art
attack algorithms that require access to gradients, including
C&W Attack [6], DeepFool [4] for minimizing `2-distance,
and FGSM [2], and BIM [7, 41] for minimizing `1-distance.
For region-based classiÔ¨Åcation, the gradient of the base clas-
siÔ¨Åer is taken with respect to the original input.
We further include methods designed speciÔ¨Åcally for the
defense mechanisms under threat. For defensive distillation,
we include the `1-optimized C&W Attack [6]. For region-
based classiÔ¨Åcation, we include backward pass differentiable
approximation (BPDA) [34], which calculates the gradient of
the model at a randomized input to replace the gradient at the
original input in C&W Attack and BIM. All of these methodsUntargeted /lscript2Attack
 Targeted/lscript2AttackTrajectories on CIFAR-10
Untargeted /lscript2Attack
 Targeted/lscript2AttackTrajectories on ImageNetFigure 7: Visualized trajectories of HopSkipJumpAttack for optimizing `2distance on randomly selected images in CIFAR-10
and ImageNet. 1st column: initialization (after blended with original images). 2nd-9th columns: images at 100, 200, 500, 1K,
2K, 5K, 10K, 25K model queries. 10th column: original images.
assume access to model details or even defense mechanisms,
which is a stronger threat model than the one required for
decision-based attacks. We also include Boundary Attack as a
decision-based baseline.
For HopSkipJumpAttack and Boundary Attack, we include
the success rate at three different scales of query budget:
2K, 10K and 50K, so as to evaluate our method both with
limited queries and a sufÔ¨Åcient number of queries. We Ô¨Ånd
the convergence of HopSkipJumpAttack becomes unstable
on region-based classiÔ¨Åcation, resulting from the difÔ¨Åculty
of locating the boundary in the binary search step when
uncertainty is increased near the boundary. Thus, we increase
the binary search threshold to 0.01 to resolve this issue.
c) Results: Figure 8 shows the success rate of various at-
tacks at different distance thresholds for the three defense
mechanisms. On all of the three defenses, HopSkipJumpAt-
tack demonstrates similar or superior performance compared
to state-of-the-art white-box attacks with sufÔ¨Åcient model
queries. Even with only 1K-2K model queries, it also achieves
acceptable performance, although worse than the best white-
box attacks. With sufÔ¨Åcient queries, Boundary Attack achieves
a comparable performance under the `2-distance metric. But
it is not able to generate any adversarial examples when the
number of queries is limited to 1;000. We think this is because
the strength of our batch gradient direction estimate over the
random walk step in Boundary Attack becomes more explicit
when there is uncertainty or non-smoothness near the decision
boundary. We also observe that Boundary Attack does not
work in optimizing the `1-distance metric for adversarialexamples, making it difÔ¨Åcult to evaluate defenses designed for
`1distance, such as adversarial training proposed by Madry
et al. [7].
On a distilled model, when the `1-distance is thresholded
at0:3, a perturbation size proposed by Madry et al. [7] to
measure adversarial robustness, HopSkipJumpAttack achieves
success rates of 86% and99% with 1K and 50K queries
respectively. At an `2-distance of 3.0, the success rate is 91%
with 2K queries. HopSkipJumpAttack achieves a comparable
performance with C&W attack under both distance metrics
with 10K-50K queries. Also, gradient masking [13] by defen-
sive distillation does not have a large inÔ¨Çuence on the query
efÔ¨Åciency of HopSkipJumpAttack, indicating that the gradient
direction estimate is robust under the setting where the model
does not have useful gradients for certain white-box attacks.
On region-based classiÔ¨Åcation, with 2K queries, Hop-
SkipJumpAttack achieves success rates of 82% and93%
at the same `1- and`2-distance thresholds respectively.
With 10K-50K queries, it is able to achieve a comparable
performance to BPDA, a white-box attack tailored to such
defense mechanisms. On the other hand, we observe that Hop-
SkipJumpAttack converges slightly slower on region-based
classiÔ¨Åcation than itself on ordinary models, which is because
stochasticity near the boundary may prevent binary search in
HopSkipJumpAttack from locating the boundary accurately.
On an adversarially trained model, HopSkipJumpAttack
achieves a success rate of 11:0%with 50K queries when
the`1-distance is thresholded at 0:3. As a comparison, BIM0 2 4 6/lscript2Distance0.00.20.40.60.81.0Success Rate
1K1K2K
2K10K
10K50K 50K/lscript2Attack against Defensive Distillation
HopSkipJump
Boundary
C&W (/lscript2)
DeepFool
0 2 4 6/lscript2Distance0.00.20.40.60.81.0Success Rate
1K 1K2K
2K10K
10K50K 50K/lscript2Attack against Region-based ClassiÔ¨Åcation
HopSkipJump
Boundary
BPDA
DeepFool
0 2 4 6/lscript2Distance0.00.20.40.60.81.0Success Rate50K50K
1K
1K2K
2K10K
10K/lscript2Attack against Adversarial Training
HopSkipJump
Boundary
C&W (/lscript2)
DeepFool
0.0 0.2 0.4 0.6/lscript‚àûDistance0.00.20.40.60.81.0Success Rate
1K1K2K
2K10K
10K50K
50K/lscript‚àûAttack against Defensive Distillation
HopSkipJump
Boundary
C&W (/lscript‚àû)
FGSM
0.0 0.1 0.2 0.3 0.4 0.5/lscript‚àûDistance0.00.20.40.60.81.0Success Rate
1K 1K2K
2K10K
10K50K
50K/lscript‚àûAttack against Region-based ClassiÔ¨Åcation
HopSkipJump
Boundary
BPDA
FGSM
0.0 0.1 0.2 0.3 0.4 0.5/lscript‚àûDistance0.00.20.40.60.81.0Success Rate/lscript‚àûAttack against Adversarial Training
HopSkipJump
Boundary
BIM
FGSM
0.28 0.29 0.30 0.31 0.320.000.050.100.15
1K
1K2K
2K10K
10K50K
50KFigure 8: Success rate versus distance threshold for a distilled model, a region-based classiÔ¨Åer and an adversarially trained
model on MNIST. Blue, magenta, cyan and orange lines are used for HopSkipJumpAttack and Boundary Attack at the budget
of 1K, 2K, 10K and 50K respectively. Different attacks are plotted with different line styles. An ampliÔ¨Åed Ô¨Ågure is included
near the critical `1-distance of 0:3for adversarial training.
has a success rate of 7:4%at the given distance threshold.
The success rate of `1-HopSkipJumpAttack transfers to an
accuracy of 87:58% on adversarially perturbed data, close
to the state-of-the-art performance achieved by white-box
attacks.3With 1K queries, HopSkipJumpAttack also achieves
comparable performance to BIM and C&W attack.
VI. D ISCUSSION
We have proposed a family of query-efÔ¨Åcient algorithms based
on a novel gradient-direction estimate, HopSkipJumpAttack,
for decision-based generation of adversarial examples, which
is capable of optimizing `2and`1-distances for both targeted
and untargeted attacks. Convergence analysis has been carried
out given access to the gradient. We have also provided
analysis for the error of our Monte Carlo estimate of gra-
dient direction, which comes from three sources: bias at the
boundary for a nonzero perturbation size, bias of deviation
from the boundary, and variance. Theoretical analysis has
provided insights for selecting the step size and the pertur-
bation size, which leads to a hyperparameter-free algorithm.
We have also carried out extensive experiments, showing
HopSkipJumpAttack compares favorably to Boundary Attack
in query efÔ¨Åciency, and achieves competitive performance on
several defense mechanisms.
3See https://github.com/MadryLab/mnist challenge.Given the fact that HopSkipJumpAttack is able to craft a
human-indistinguishable adversarial example within a realistic
budget of queries, it becomes important for the community
to consider the real-world impact of decision-based threat
models. We have also demonstrated that HopSkipJumpAt-
tack is able to achieve comparable or even superior perfor-
mance to state-of-the-art white-box attacks on several de-
fense mechanisms, under a much weaker threat model. In
particular, masked gradients, stochastic gradients, and non-
differentiability are not barriers to our algorithm. Because
of its effectiveness, efÔ¨Åciency, and applicability to non-
differentiable models, we suggest future research on adver-
sarial defenses may evaluate the designed mechanism against
HopSkipJumpAttack as a Ô¨Årst step.
One limitation of all existing decision-based algorithms, in-
cluding HopSkipJumpAttack, is that they require evaluation
of the target model near the boundary. They may not work
effectively by limiting the queries near the boundary, or
by widening the decision boundary through insertion of an
additional ‚Äúunknown‚Äù class for inputs with low conÔ¨Ådence.
We have also observed that it still takes tens of thousands of
model queries for HopSkipJumpAttack to craft imperceptible
adversarial examples with a target class on ImageNet, which
has a relatively large image size. Future work may seek the
combination of HopSkipJumpAttack with transfer-based attack
to resolve these issues.VII. A CKNOWLEDGEMENT
We would like to thank Nicolas Papernot and anonymous
reviewers for providing their helpful feedback.
REFERENCES
[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. In-
triguing properties of neural networks. In International Confer-
ence on Learning Representations , 2014.
[2] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. In Proceedings
of the International Conference on Learning Representations ,
2015.
[3] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial
machine learning at scale. In International Conference on
Learning Representations , 2017.
[4] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal
Frossard. Deepfool: a simple and accurate method to fool deep
neural networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , pages 2574‚Äì2582,
2016.
[5] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrik-
son, Z Berkay Celik, and Ananthram Swami. The limitations of
deep learning in adversarial settings. In 2016 IEEE European
Symposium on Security and Privacy , pages 372‚Äì387. IEEE,
2016.
[6] Nicholas Carlini and David Wagner. Towards evaluating the
robustness of neural networks. In 2017 IEEE Symposium on
Security and Privacy , pages 39‚Äì57. IEEE, 2017.
[7] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,
Dimitris Tsipras, and Adrian Vladu. Towards deep learning
models resistant to adversarial attacks. In International Confer-
ence on Learning Representations , 2018.
[8] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-
Jui Hsieh. Zoo: Zeroth order optimization based black-box
attacks to deep neural networks without training substitute
models. In Proceedings of the 10th ACM Workshop on ArtiÔ¨Åcial
Intelligence and Security , pages 15‚Äì26. ACM, 2017.
[9] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy
Lin. Black-box adversarial attacks with limited queries and
information. In International Conference on Machine Learning ,
pages 2142‚Äì2151, 2018.
[10] Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior
convictions: Black-box adversarial attacks with bandits and pri-
ors. In International Conference on Learning Representations ,
2019.
[11] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving
into transferable adversarial examples and black-box attacks.
InProceedings of the International Conference on Learning
Representations , 2017.
[12] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow.
Transferability in machine learning: from phenomena to
black-box attacks using adversarial samples. arXiv preprint
arXiv:1605.07277 , 2016.
[13] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh
Jha, Z Berkay Celik, and Ananthram Swami. Practical black-
box attacks against machine learning. In Proceedings of the
2017 ACM on Asia Conference on Computer and Communica-
tions Security , pages 506‚Äì519. ACM, 2017.
[14] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-
based adversarial attacks: Reliable attacks against black-box
machine learning models. In International Conference on
Learning Representations , 2018.[15] Thomas Brunner, Frederik Diehl, Michael Truong Le, and Alois
Knoll. Guessing smart: Biased sampling for efÔ¨Åcient black-box
adversarial attacks. arXiv preprint arXiv:1812.09803 , 2018.
[16] Minhao Cheng, Thong Le, Pin-Yu Chen, Huan Zhang, JinFeng
Yi, and Cho-Jui Hsieh. Query-efÔ¨Åcient hard-label black-box
attack: An optimization-based approach. In International Con-
ference on Learning Representations , 2019.
[17] Florian Tramr, Alexey Kurakin, Nicolas Papernot, Ian Goodfel-
low, Dan Boneh, and Patrick McDaniel. Ensemble adversarial
training: Attacks and defenses. In International Conference on
Learning Representations , 2018.
[18] Abraham D Flaxman, Adam Tauman Kalai, and H Brendan
McMahan. Online convex optimization in the bandit setting:
gradient descent without a gradient. In Proceedings of the Six-
teenth Annual ACM-SIAM Symposium on Discrete Algorithms ,
pages 385‚Äì394. SIAM, 2005.
[19] Alekh Agarwal, Dean P Foster, Daniel J Hsu, Sham M Kakade,
and Alexander Rakhlin. Stochastic convex optimization with
bandit feedback. In Advances in Neural Information Processing
Systems , pages 1035‚Äì1043, 2011.
[20] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free
minimization of convex functions. Foundations of Computa-
tional Mathematics , 17(2):527‚Äì566, 2017.
[21] John C Duchi, Michael I Jordan, Martin J Wainwright, and An-
dre Wibisono. Optimal rates for zero-order convex optimization:
The power of two function evaluations. IEEE Transactions on
Information Theory , 61(5):2788‚Äì2806, 2015.
[22] Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, Paishun Ting, Shiyu
Chang, and Lisa Amini. Zeroth-order stochastic variance
reduction for nonconvex optimization. In Advances in Neural
Information Processing Systems , pages 3731‚Äì3741, 2018.
[23] David Kincaid, David Ronald Kincaid, and Elliott Ward Cheney.
Numerical Analysis: Mathematics of ScientiÔ¨Åc Computing , vol-
ume 2. American Mathematical Soc., 2009.
[24] Michel Ledoux. The Concentration of Measure Phenomenon .
Number 89. American Mathematical Soc., 2001.
[25] Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Good-
fellow, Reuben Feinman, Alexey Kurakin, Cihang Xie, Yash
Sharma, Tom Brown, Aurko Roy, Alexander Matyasko, Vahid
Behzadan, Karen Hambardzumyan, Zhishuai Zhang, Yi-Lin
Juang, Zhi Li, Ryan Sheatsley, Abhibhav Garg, Jonathan Ue-
sato, Willi Gierke, Yinpeng Dong, David Berthelot, Paul Hen-
dricks, Jonas Rauber, and Rujun Long. Technical report on the
cleverhans v2.1.0 adversarial examples library. arXiv preprint
arXiv:1610.00768 , 2018.
[26] Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox:
A python toolbox to benchmark the robustness of machine
learning models. arXiv preprint arXiv:1707.04131 , 2017.
[27] Alex Krizhevsky. Learning multiple layers of features from tiny
images. Technical report, Citeseer, 2009.
[28] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. ImageNet: A Large-Scale Hierarchical Image Database.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2009.
[29] Franc ¬∏ois Chollet et al. Keras. https://keras.io, 2015.
[30] Jan Hendrik Metzen, Tim Genewein, V olker Fischer, and Bas-
tian Bischoff. On detecting adversarial perturbations. In
International Conference on Learning Representations , 2017.
[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Identity mappings in deep residual networks. In European
Conference on Computer Vision , pages 630‚Äì645. Springer,
2016.
[32] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q
Weinberger. Densely connected convolutional networks. In
Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 4700‚Äì4708, 2017.
[33] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, andAnanthram Swami. Distillation as a defense to adversarial
perturbations against deep neural networks. In 2016 IEEE
Symposium on Security and Privacy , pages 582‚Äì597. IEEE,
2016.
[34] Anish Athalye, Nicholas Carlini, and David Wagner. Obfus-
cated gradients give a false sense of security: Circumventing
defenses to adversarial examples. In International Conference
on Machine Learning , pages 274‚Äì283, 2018.
[35] Xiaoyu Cao and Neil Zhenqiang Gong. Mitigating evasion at-
tacks to deep neural networks via region-based classiÔ¨Åcation. In
Proceedings of the 33rd Annual Computer Security Applications
Conference , pages 278‚Äì287. ACM, 2017.
[36] Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh.
Towards robust neural networks via random self-ensemble. In
Proceedings of the European Conference on Computer Vision
(ECCV) , pages 369‚Äì385, 2018.
[37] Guneet S. Dhillon, Kamyar Azizzadenesheli, Jeremy D. Bern-
stein, Jean KossaiÔ¨Å, Aran Khanna, Zachary C. Lipton, and
Animashree Anandkumar. Stochastic activation pruning for
robust adversarial defense. In International Conference on
Learning Representations , 2018.
[38] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. CertiÔ¨Åed ad-
versarial robustness via randomized smoothing. In International
Conference on Machine Learning , pages 1310‚Äì1320, 2019.
[39] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan
Yuille. Mitigating adversarial effects through randomization. In
International Conference on Learning Representations , 2018.
[40] Nicholas Carlini and David Wagner. Adversarial examples are
not easily detected: Bypassing ten detection methods. In Pro-
ceedings of the 10th ACM Workshop on ArtiÔ¨Åcial Intelligence
and Security , pages 3‚Äì14. ACM, 2017.
[41] Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adver-
sarial examples in the physical world. In ArtiÔ¨Åcial Intelligence
Safety and Security , pages 99‚Äì112. Chapman and Hall/CRC,
2018.
[42] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg,
J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot,
and E. Duchesnay. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research , 12:2825‚Äì2830, 2011.
APPENDIX A
PROOFS
For notational simplicity, we use the shorthand SSx?
throughout the proofs.
A. Proof of Theorem 1
We denote t:=t=krS(xt)k2, so that the update (3) at
iteratetcan be rewritten as
xt+1=tx?+ (1 t)(xt+trS(xt)): (18)
Let the step size choice t=tkxt x?kwitht:=t q, we
havet=tkxt x?k
krS(xt)k.
The squared distance ratio is
kxt+1 x?k2
2
kxt x?k2
2=k(1 )(trS(xt) +xt x?)k2
2
kxt x?k2
2:(19)By a second-order Taylor series, we have
0 =
rS(xt); xt+1 xt
+1
2(xt+1 xt)THt(xt+1 xt);
(20)
whereHt=r2S(xt+1+ (1 )xt)for some2[0;1].
Plugging equation (18) into equation (20) yields

rS(xt); vt+trS(xt)
+
1
2( vt+trS(xt))THt( vt+trS(xt)) = 0;(21)
where we deÔ¨Åne vt:=xt x?+trS(xt). This can be
rewritten as a quadratic equation with respect to :
vT
tHtvt2 2rS(xt)T(I+tHt)vt
+rS(xt)T(2
tHt+ 2tI)rS(xt) = 0: (22)
Solving for yields
rS(xt)T(2
tHt+ 2tI)rS(xt)
2rS(xt)T(I+tHt)vt: (23)
In order to simplify the notation, deÔ¨Åne rt:=rS(xt)and
dt:=xt x?. Hence, we have
(1 )2rt+t3
2Lkdtk2
krtk2
rt+t(1 +3
2Lkdtk2
krtk2)2
;
where
rt=hxt x?;rS(xt)i
kxt x?k2krS(xt)k2=hdt;rti
kdtk2krtk2: (24)
Lett:=3
2Lkdtk2
krtk2. Thentis bounded when krtk2~C
andq>1
2. Equation (19) and the bound on (1 )2yield
kxt+1 x?k2
2
kxt x?k2
2rt+tt
rt+t(1 +t)2
(2
t+ 2trt+ 1):
(25)
DeÔ¨Ånet:=
rt+tt
rt+t(1+t)2
(2
t+ 2trt+ 1) . We analyze
tin the following two different cases: rt<tandrtt.
In the Ô¨Årst case, we have
t1 +t
1 + (1 +t)2
(2
t+ 22
t+ 1): (26)
As long ast!0ast!1 , there exists a positive constant
c2>0such thatt<1 c2fortlarge enough.
In the second case, we have rtt. DeÔ¨Ånet:=t
rt1.
We boundtby
t=(1 + 2tt+2
t2
t)(2
t+ 2trt+ 1)
1 + 2t(1 +t) +2
t(1 +t)2
1 + 2tt+2
t2
t+ 2tr2
t
1 + 2tt+2
t2
t+ 2t+
2
t(4t+ (1 +tt)2+ 2t2
t)
1 2t(1 r2
t)
1 + 2tt+2
t2
t+ 2t+c2
t
1 c1t(1 r2
t) +c22
t;wherec1;c2are Ô¨Åxed constants. As the product of tovert
is positive, we have
1X
t=1logt= log 1
t=1t> 1: (27)
Then we have that there are at most a Ô¨Ånite number of tthat
falls in the Ô¨Årst case, rt<t. In the second case, Equation (27)
is equivalent to
1X
t=1c1t1 r2
t
rt c22
t<1;
which implies c1t1 r2
t
rt c22
t=o(t 1). Whent=t qfor
some constant1
2w , we
have
S(xt+u)rS(xt)Tu+1
22uTr2S(x0)u
(rS(xt)Tu 1
2L)>0:
Similarly, we have S(xt+u)<0whenrS(xt)Tu < w.
Therefore, we have
x(xt+u) =(
1ifrS(xt)Tu>w;
 1ifrS(xt)Tu< w:
We expand the vector rS(xt)to an orthogonal bases in Rd:
v1=rS(xt)=krS(xt)k2;v2;:::;vd. The random vector u
can be expressed as u=Pd
i=1ivi, whereis uniformly
distributed on the sphere. Denote the upper cap as E1:=
frS(xt)Tu > wg, the annulus as E2:=fjrS(xt)Tuj<
wg, and the lower cap as E3:=frS(xt)Tu < wg. Let
p:=P(E2)be the probability of event E2. Thus we haveP(E1) =P(E3) = (1 p)=2. By symmetry, for any i6= 1,
we have
E[ijE1] =E[ijE3] = 0:
Therefore, the expected value of the estimator is
E[x(xt+u)u] =p 
E[x(xt+u)ujE2]
 1
2E[1v1jE1] 1
2E[ 1v1jE3]
+E[1v1jE1] +E[ 1v1jE3]
Exploiting the above derivation, we can bound the difference
between E[j1jv1] =Ej1j
krS(xt)k2rS(xt)andE[x(xt+u)u]:
kE[x(xt+u)u] E[j1jv1]k22p+p= 3p;
which yields
cos\(E[x(xt+u)u];rS(xt))1 1
23p
Ej1j2
:(30)
We can bound pby observing that hrS(xt)
krS(xt)k2;ui2is a Beta
distributionB(1
2;d 1
2):
p=P
hrS(xt)
krS(xt)k2;ui2w2
krS(xt)k2
2
2w
B(1
2;d 1
2)krS(xt)k2:
Plugging into Equation (30), we get
cos\(E[x(xt+u)u];rS(xt))
1 18w2
(Ej1j)2B(1
2;d 1
2)2krS(xt)k2
2
= 1 9L22(d 1)2
8krS(xt)k2
2:
We also observe that
EgrS(xt;) =E[x(xt+u)u]:
As a consequence, we have established
cos\
E[grS(xt;)];rS(xt)
1 9L22(d 1)2
8krS(xt)k2
2:
Taking!0, we get
lim
!0cos\
E[grS(xt;)];rS(xt)
= 1:
C. Proof of Theorem 3
Proof. For notational simplicity, we denote b:=x(xt+
ub), and =1
BPB
b=1b=x. We use;u to denote i.i.d.
copies ofbandubrespectively. By exploiting independenceofua;uband independence of aua;bub, the variance of the
estimate with the baseline can be expressed as
Var(drS(xt;))
=1
(B 1)2BX
a=1
Eaua E[u]2
2 2E[a]+
E2+ (2
B 1
B2)kE[u]k2
+kEuk2
2
B(B 1)
=B2Var(grS(xt;))
(B 1)2 BE[2]
(B 1)2+(3B 2)kE[u]k2
2
B(B 1)2
B2Var(grS(xt;))
(B 1)2 BE[2]
(B 1)2+3B 2
B(B 1)2: (31)
The middle term can be expanded as
 B
(B 1)2E[2] = 1
(B 1)2 4
B 1(E 1
2)2:
Plugging into Equation (31), we get
Var(drS(xt;)) = Var(grS(xt;))n
1 +2B 1
(B 1)2 
2
2(B 1) 
2B(E[] 1
2)2 1o
:
When E[]satisÔ¨Åes (E[] 1
2)2>1
2B(1 +2B 1
2B 22), we have
2B 1
(B 1)2<2
2(B 1)(2B(E[] 1
2)2 1);
which implies Var (drS(xt;))<Var(grS(xt;)).
APPENDIX B
SENSITIVITY ANALYSIS
In this section, we carry out experiments to evaluate the hyper-
parameters suggested by our theoretical analysis. We use a
20-layer ResNet [31] trained over CIFAR-10 [27]. We run the
`2-optimized HopSkipJumpAttack over a subset of randomly
sampled images.
a) Choice of step size: We compare several schemes of
choosing step size at each step. The Ô¨Årst scheme is suggested
by Theorem 1: at the t-th step, we set t=kxt x?k2=p
t,
which we call ‚ÄúScale with Distance (Sqrt. Decay).‚Äù We include
the other two scales which scale with distance, ‚ÄúScale with
Distance (Linear Decay)‚Äù with t=kxt x?k2=tand ‚ÄúScale
with Distance (No Decay)‚Äù with t=kxt x?k2. We then
include ‚ÄúGrid Search,‚Äù which searchs step sizes over a log-
scale grid, and chooses the step size that best controls the
distance with the original sample after projecting the updated
sample back to the boundary via binary search. Finally, we
include constant stepsizes at t= 0:01;0:1;1:0. For all
schemes, we always use geometric progression to decrease
the step size by half until x?(~xt) = 1 before the next binary
search step.
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí210‚àí1100101/lscript2DistanceComparison of Step Size Schemes
Figure 9: Comparison of various choices of step size.
Figure 9 plots the median distance against the number of
queries for all schemes. We observe that the scheme sug-
gested by Theorem 1 achieves the best performance in this
experiment. Grid search costs extra query budget initially but
eventually achieves a comparable convergence rate. When the
step size scales with the distance but with inappropriately
chosen decay, the algorithm converges slightly slower. The
performance of the algorithm suffers from a constant step size.
b) Choice of perturbation size and introduction of baseline:
We now study the effectiveness of the proposed perturbation
size and baseline for estimating gradient direction when the
sample deviates from the boundary. In particular, we focus on
the choice of tand the introduction of baseline analyzed in
Section IV. Gradient direction estimation is carried out at per-
turbed images at the ith iteration, for i= 10;20;30;40;50;60.
We use the cosine of the angle between the gradient-direction
estimate and the truth gradient of the model as a metric.
Figure 10 shows the box plots of two gradient-direction
estimates as tvaries among 0:01
t;0:1
t;
t;10
t;100
t,
where
t= 10p
dk~xt 1 x?k2is our proposed choice. We
observe that our proposed choice of tyields the highest cosine
of the angle on average. Also, the baseline in drSfurther
improves the performance, in particular when tis not chosen
optimally so that there is severe unevenness in the distribution
of perturbed images.Figure 10: Box plots of the cosine of the angle between the
proposed estimates and the true gradient.
APPENDIX C
MODEL WITHOUT GRADIENTS
In this section, we evaluate HopSkipJumpAttack on a model
without gradients. We aim to show HopSkipJumpAttack is
able to craft adversarial examples under weaker conditions,
such as non-differentiable models, or even discontinuous input
transform.
Concretely, we implement input binarization followed by a
random forest on MNIST. Binarization transforms an input
image to an array of f0;1g, but transforming all pixels larger
than a given threshold to 1, and all pixels smaller than the
threshold to 0. The algorithm for training random forests
applies bootstrap aggregating to tree learners. We implement
the random forest with default parameters in scikit-learn [42],
using the Gini impurity as split criterion. For each split,p
d
randomly selected features are used, where d= 2828
is the number of pixels. We evaluate two random forests
with different thresholds for binarization: 0:1and0:5. With
the Ô¨Årst threshold, the model achieves the highest accuracy,
96%, on natural test data. The second threshold yields the
most robust performance under adversarial perturbation, with
accuracy 94:5%on natural test data.
For both Boundary Attack and HopSkipJumpAttack, we adopt
the same initialization and hyper-parameters as in Section V-A.
The original image (with real values) is used as input to both
attacks for model queries. When an image is fed into the
model by the attacker, the model processes the image with
binarization Ô¨Årst, followed by the random forest. Such a design
preserves the black-box assumption for decision-based attacks.
We only focus on untargeted `2attack here. Note that over
91% of the pixels on MNIST are either greater than 0:9or less
than0:1, and thus require a perturbation of size at least 0:4to
change their outputs after being thresholded by 0:5. This fact
makes`1perturbation inappropriate for crafting adversarial
examples.
0K 5K 10K 15K 20K 25K
Number of Queries10‚àí1100101/lscript2DistanceUntargeted /lscript2(Binarization (‚â•0.1) + RF)
0K 5K 10K 15K 20K 25K
Number of Queries100101/lscript2DistanceUntargeted /lscript2(Binarization (‚â•0.5) + RF)
Figure 11: Median `2distance versus number of model queries
on MNIST with binarization + random forest. The threshold
of binarization is set to be 0:1and0:5respectively.
0 2 4 6/lscript2Distance0.00.20.40.60.81.0Success Rate
1K
1K2K
2K10K 10K 25K 25KUntargeted /lscript2(Binarization (‚â•0.1) + RF)
0 2 4 6/lscript2Distance0.00.20.40.60.81.0Success Rate
1K1K2K
2K10K
10K25K
25KUntargeted /lscript2(Binarization (‚â•0.5) + RF)
Figure 12: Success rate versus distance threshold on MNIST
with binarization + random forest. The threshold of binariza-
tion is set to be 0:1and0:5respectively.
Figure 11 shows the median distance (on a log scale) against
the queries, with the Ô¨Årst and third quartiles used as lower and
upper error bars. Figure 12 shows the success rate against the
distance threshold.
When the threshold is set to be 0:1, the random forest
with binarization becomes extremely vulnerable to adversarial
examples. Around 96% adversarial examples fall into the size-
3`2-neighborhood of the respective original examples with
1K model queries of HopSkipJumpAttack. The vulnerability
is caused by the ease of activating pixels through increasing
the strength by 0:1. It also indicates HopSkipJumpAttack and
Boundary Attack are able to craft adversarial examples without
smooth decision boundaries.
When the threshold is set to be 0:5, we have a more
robust model. A median `2distance of 3is achieved by
HopSkipJumpAttack through 3K model queries. It takes 25K
queries to achieve 99% success rate at an `2distance of
3for HopSkipJumpAttack. On the other hand, we observe
that Boundary Attack only achieves a median distance of 5
even with 25K model queries. This might result from the
inefÔ¨Åciency in spending queries on random walk instead of
‚Äúgradient direction‚Äù estimation step in HopSkipJumpAttack.
We remark that the concept of ‚Äúgradient direction‚Äù requires
an alternative deÔ¨Ånition in the current setting, such as a
formulation via subgradients.