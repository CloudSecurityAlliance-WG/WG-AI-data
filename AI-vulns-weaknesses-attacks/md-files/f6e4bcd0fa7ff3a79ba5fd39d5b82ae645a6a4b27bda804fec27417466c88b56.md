Bypassing Backdoor Detection Algorithms in Deep Learning
Te Juin Lester Tan, Reza Shokri
Department of Computer Science
National University of Singapore (NUS)
flester.tan, reza g@comp.nus.edu.sg
Abstract —Deep learning models are vulnerable to various
adversarial manipulations of their training data, parameters,
and input sample. In particular, an adversary can modify the
training data and model parameters to embed backdoors into
the model, so the model behaves according to the adversary’s
objective if the input contains the backdoor features, referred
to as the backdoor trigger (e.g., a stamp on an image). The
poisoned model’s behavior on clean data, however, remains
unchanged. Many detection algorithms are designed to detect
backdoors on input samples or model parameters, through
the statistical difference between the latent representations of
adversarial and clean input samples in the poisoned model.
In this paper, we design an adversarial backdoor embed-
ding algorithm that can bypass the existing detection algo-
rithms including the state-of-the-art techniques. We design
an adaptive adversarial training algorithm that optimizes the
original loss function of the model, and also maximizes the
indistinguishability of the hidden representations of poisoned
data and clean data. This work calls for designing adversary-
aware defense mechanisms for backdoor detection.
1. Introduction
Deep learning models are capable of learning complex
tasks with a high predictive power. They are, however,
vulnerable to many privacy and security attacks that ex-
ploit their large capacity. The models are susceptible to
adversarial manipulations of their training set, parame-
ters, and inputs. The attacker can degrade the model’s
predictive power, or change its behavior according to the
adversary’s objective, by poisoning the training set [8],
[19], [11], [28], or its parameters [2]. It is also possible to
adversarially and stealthily manipulate a normal data point
to confuse the model into making a wrong prediction [23].
The models can also leak a signiﬁcant amount of infor-
mation about their training data, parameters, and inputs.
There is a large body of research on various types of
privacy attacks and countermeasures against them, under
black-box and white-box access settings, and centralized
and distributed learning settings. Having access to the
model predictions or parameters, an adversary can infer
sensitive information about the model’s training set [20],
[18]. The model predictions can also be exploited to
reconstruct the model parameters [24], or its input [9].
In this paper, we focus on active attacks against ma-
chine learning algorithms. We speciﬁcally focus on a class
of attacks known as backdoor attacks, where the adversary
manipulates training data and/or the training algorithm and
parameters of the model in order to embed an adversarial
(classiﬁcation) rule into the model [8], [11]. The modelbehaves normally on all inputs, except for the inputs
that contain the adversary’s embedded pattern, called the
backdoor trigger. Several types of backdoor triggers have
been explored in previous studies. These include input-
instance triggers where the backdoor instances correspond
to speciﬁc inputs in the input space, or pixel-pattern trig-
gers that contain a speciﬁc pixel pattern, e.g., the images
that contain a stamp, and also semantic triggers where the
backdoor instances contain a speciﬁc high-level feature,
e.g., objects with a particular shape or color. Figure 1
shows an example of an image with a backdoor trigger.
Given the wide range of deep learning applications, back-
door attacks have the ability to cause serious damage, from
bypassing facial recognition authentication systems [8], to
fooling driverless vehicles to misclassify stop signs [11].
A number of backdoor detection algorithms are de-
signed for deep learning [15], [7], [25], [8], [16]. These al-
gorithms focus on identifying which inputs contain back-
door, and which parts of the model (its activation functions
speciﬁcally) are responsible for triggering the adversarial
behavior of the model. For a given adversary model, the
detection algorithms try to identify the signatures of the
backdoors in the hidden layers of the model, in order to
distinguish inputs with the backdoor trigger from clean
benign inputs. Note that the backdoor rule is an exception
in the main task represented by the machine learning
model. Thus, to learn the adversarial task along with the
main task, the learning algorithm tries to minimize the
conﬂict between the two. This is what the stat-of-the-art
detection algorithms rely on. These algorithms compute
various types of statistics on the latent representations of
inputs, which can help the defender to separate adversarial
and benign data, relying on the distinguishable dissimilar-
ity between the distribution of their latent representations.
The common implicit assumption of prior defense
techniques is that the adversary is unaware of the detec-
tion algorithm. Ignoring adaptive attack algorithms is the
main limitation of defense methods in adversarial machine
learning. In the case of adversarial examples, it has been
shown that a large number of defense mechanisms can be
bypassed by an adaptive attack, for the same weakness in
their threat model [1], [6], [5]. In this paper, we design
anadversarial backdoor embedding algorithm for deep
learning, that maximizes the latent indistinguishability
between adversarial inputs and benign inputs. We show
that the attack strategy can be tailored to any particular
detection algorithm and the statistics that the defender uses
for identifying backdoors. We go beyond bypassing spe-
ciﬁc algorithms. To be effective against generic detection
algorithms, we maximize the latent indistinguishability of
input data, using adversarial regularization .arXiv:1905.13409v2 [cs.LG] 6 Jun 2020Figure 1: Examples of images with (right) and without
(left) a backdoor trigger. The poisoned model will recog-
nize the trigger and acts adversarially.
In our threat model, the adversary is capable of ex-
ploiting the training algorithm. We rely on data poisoning
and adversarial regularization in our backdoor embedding
attack. We construct a discriminator network which op-
timizes for identifying any difference between the be-
nign and adversarial data in the hidden layers of the
model. The objective function for the classiﬁcation model
is adversarially regularized to maximize the loss of the
discriminator (bypassing network). Thus, the ﬁnal model
is not only accurate on classifying benign data points
according to their clean label, and is accurate adversarial
data points according to their adversarial label, but also
has indistinguishable latent representation for data points
in these two sets. This enables the compromised model to
bypass the detection algorithms which cluster and separate
the latent representations of benign and adversarial inputs.
Our adversarial embedding attack successfully evades
several state-of-the-art defenses. As the baseline, for a
VGG model trained on the CIFAR-10 dataset, the dataset
ﬁltering defense using spectral signatures [25] is able
to bring down the backdoor attack success rate of a
compromised model to 1.5%, assuming a static attack
strategy (assumed in [25]). But, a model compromised
with our adversarial backdoor embedding algorithm is
able to retain an attack success rate of 97.3%, against the
detection algorithm. The dataset ﬁltering defense using
activation clustering [7] is similarly able to bring down
the static attack success rate of a compromised model to
1.9%. But our adversarial embedding algorithm retains a
96.2% attack success rate, against the detection algorithm.
Feature pruning [26] is able to effectively select neurons
to prune for a model, and is able to completely remove
the backdoor behavior with almost no loss in model
accuracy, assuming the baseline static attack. However, for
a model with adversarial embedding, the full removal of
the backdoor behavior simultaneously degrades the model
accuracy signiﬁcantly down to 20% (where 10% is the
random guess). Thus, all existing detection algorithms fail
against the adversarial backdoor embedding algorithm.
2. Prior Backdoor Detection Algorithms
We denote the input space of the model as X, where
each input instance xcomes with its corresponding class
label y. Backdoor attacks are deﬁned by a backdoor trig-
ger, which is a property Bon each input in the input space,
such that8x2X,B(x)is classiﬁed as yt6=y, where yt
is a target label of the attacker’s choice. We will refer to
the set of input instances with Bas backdoor instances.Consider two input-label pairs, (i) a clean input in-
stance xcand its corresponding label yc, as well as (ii) a
backdoor input instance xbthat has a true class label ytrue
b,
but is wrongly classiﬁed as the target label yb=ycdue
to the presence of the backdoor. Even though both input
instances have the same target label, xccontains high-
level features corresponding to its true class yc, while xb
contains high-level features corresponding to both its true
class ytrue
band the backdoor trigger.
A hidden layer in a deep learning network can be
treated as the model’s latent representation of the input in-
stance, with the neurons in the layer representing different
high-level features of the input instance. Given the high-
level features present in xcandxbdiffer, one expects the
respective latent representations of these input instances to
also differ considerably. Several studies have successfully
leveraged this difference in latent representations to detect,
or to mitigate the backdoor behavior. While any hidden
layer can be treated as the latent representation of the
inputs, the defenses are typically applied to the penulti-
mate layer, since it represents the highest-level features
extracted by the model.
The proposed defenses we analyze fall into two main
categories. The ﬁrst category of defenses, given a poisoned
model, uses the model’s latent representations of clean
and poisoned instances to determine neurons to prune,
in order to remove the backdoor adversarial rule from
the network. The second category of defenses uses the
latent representations to ﬁlter the training dataset, in order
to remove most, if not all, of the maliciously injected
poisoned samples. The model can then be retrained on the
remaining samples to obtain a functional classiﬁer without
the backdoor behavior.
2.1. Feature Pruning
Wang et al., 2019 [26] formulate a detection technique
that assuming a known subset of clean inputs, detects
possible backdoors and removes them. The authors design
a reverse-engineering process based on an optimization
function that ﬁnds the minimum perturbation required to
cause the misclassiﬁcation of all inputs to a particular
target class. This process is applied to every class in the
task, yielding a candidate backdoor trigger for each class.
Then, based on the intuition that a backdoor trigger is a
small perturbation on the input instances, outlier detection
based on the median absolute deviation is performed to
detect abnormally small perturbations, which are highly
likely to be the injected backdoor triggers.
Wang et al. then propose a pruning algorithm that
utilizes the reverse-engineered backdoor trigger to remove
the backdoor adversarial rule from the model. It does so
by recording the mean activation of each neuron nin the
hidden layer over clean inputs, znc, and over inputs with
the backdoor trigger, zn
b. Then, neurons are pruned in the
order of decreasing absolute difference in the means:
argmax
n2Njznc zn
bj: (1)
The pruning is terminated when the backdoor behavior
is fully removed from the model. This defense mechanism
assumes that the backdoor adversarial rule in the model
is implemented by a large change in activation for the
neurons that represent the backdoor features.2.2. Dataset Filtering by Spectral Signatures
Tran, Li, and Madry, 2018 [25] propose a technique
based on robust statistics to identify and remove poisoned
data samples from a potentially compromised training
dataset. First, a network is trained using the poisoned
training dataset. For each particular output class label,
all the input instances for that label are fed through the
network, and their latent representations are recorded.
Singular value decomposition is then performed on the
covariance matrix of the latent representations, and this
is used to compute an outlier score for each input. The
inputs with the top scores are ﬂagged as poisoned inputs,
and removed from the training dataset. The authors show
that this defense succeeds when the means of the latent
representations of clean inputs are sufﬁciently different
from the means of the latent representations of the inputs
that contain the backdoor trigger.
2.3. Dataset Filtering by Activation Clustering
Chen et al., 2018 [7] devise a defense that relies on
clustering the latent representations of the inputs. For all
input instances the model classiﬁes as a particular class
label, the latent representations of the inputs are recorded.
Dimensionality reduction is performed using independent
component analysis to reduce the recorded latent repre-
sentations to 10 to 15 features, and k-means clustering
is then performed to separate the transformed data into 2
clusters. This clustering step assumes that when projected
onto the principal components, the latent representations
of the backdoor and clean instances form separate clusters
due to the model extracting different features from them.
K-means clustering is instructed to produce 2 clus-
ters, regardless of whether poisoned samples are present.
Chen et al. recommend a process called exclusionary
reclassiﬁcation to determine which of the clusters, if any,
is poisoned. A new model is trained using all training
samples excluding one of the clusters. Then, the newly
trained model is used to classify the input instances in the
cluster, to detect if one cluster is poisoned.
3. Adversarial Backdoor Embedding Attack
Our objective is to construct adversarially poisoned
deep learning models that are not detectable using the
class of backdoor detection algorithms that try to sepa-
rate clean and adversarial inputs, which contain backdoor
triggers, from their latent representations in the model.
The defenses above perform well since a signiﬁcant
difference in distribution of latent representations in back-
door instances and clean instances tends to emerge when
a poisoned model is trained naively by the attacker. How-
ever, the defenses above fail to consider that a sophisti-
cated attacker is able to make the model robust to them by
minimizing their difference in latent representations .
To do so, we introduce a secondary loss function to the
training objective function
L(fq(x);y) +Lrep(zq(x)) (2)
where xis the input instance, yis the target label, qis the
parameters of the network, fq(x)is the class prediction
Figure 2: The architecture of our adversarial embedding
attack. A discriminator is included that takes the latent
representation from the model as input and decides if it
is from a backdoor or a clean input.
of the network for input x, and zq(x)is the latent rep-
resentation of x, extracted by the network. Lrep(zq(x))
represents an additional penalty term that penalizes the
model when the distributions of the network activations
differ greatly between clean and backdoor inputs. This
additional penalty can be tailored to a speciﬁc defense
that the attacker anticipates, or can be a general penalty
that mitigates various defenses, as we will demonstrate.
Through the double objective function, the attacker aims
to achieve high classiﬁcation accuracy of the model, while
setting certain constraints on the latent representations of
the inputs in order to bypass potential defenses.
3.1. Targeted Adversarial Embedding
We will explore an example of an attacker using the
double objective function above to mitigate a speciﬁc
defense. Consider the pruning defense by Wang et al. [26],
which selects neurons to prune based on the absolute
difference in mean neuron activations between clean and
backdoor input instances, as presented in Equation 1.
In order to prevent the backdoor neurons from being
selected for pruning, the attacker has to minimize jznc zn
bj
for each neuron in the backdoor neuron set Nb. We notice
that for any neuron n,
jkznc kzn
bj=kjznc zn
bjjznc zn
bj (3)
for any 0 0.95), and the
attack success rate on the retrained models is very low
(0% to 1.9%). However, our attack successfully bypasses
this defense technique, yielding a lower resultant adjusted
Rand index after the clustering step. This signiﬁes that a
large number of poisoned samples is present in both re-
sultant clusters. Thus, the model retrained on the resultant
ﬁltered dataset still contains the backdoor behavior, and
exhibits a high attack success rate of above 75%.
To understand why our attack evades the defense,
we analyze the latent representations of the VGG model
trained on the CIFAR-10 dataset. Figure 8a shows the
latent representations of all inputs by the baseline model,
projected onto the top two independent components. The
poisoned inputs (in red) form a noticeably disjoint cluster
from the clean inputs (in blue). Thus, k-means clustering
can separate the poisoned inputs from the clean inputs
well, and most of the poisoned samples can be removed
from the training dataset by excluding the right cluster.
Figure 8b shows the latent representations for the model
with adversarial embedding, similarly projected onto the
top two independent components. The adversarial train-
ing causes the poisoned and clean latent representa-
tions to overlap signiﬁcantly due to a convergence in
their distributions , so k-means clustering is unable to
separate the poison inputs from the clean inputs well.
Thus, removing either of the resultant k-means clusters
still leaves a signiﬁcant number of poisoned sample in the
training dataset, and the retrained model will still contain
the backdoor.
5. Related Work
Backdoors in deep learning networks is a topic of
growing interest. Many studies explore backdoor injection
through data poisoning, where the attacker injects mali-
ciously crafted input and label samples into the trainingdataset [11], [8], [19]. These poisoned inputs are typi-
cally images with the backdoor trigger superimposed, and
the target label altered. It has been shown that a small
number of poisoned data points (50 samples) is needed
to introduce a backdoor with a high attack success rate
(above 90%) [8]. It has been shown that the introduced
backdoors persist after the model is repurposed [11].
Thus, malicious behavior injected in models up the supply
chain can propagate to downstream models, even if the
retraining is done with a clean dataset. Many approaches
to data poisoning lead to a visible trigger superimposed
on the poisoned images, which makes malicious training
data apparent to human eyes, but it has been shown to
be possible to generate poison data that looks clean with
unaltered labels [19].
Backdoor attacks that do not rely on the attacker hav-
ing access to the training data have also been devised. In
the federated learning setting, the attacker has the ability
to broadcast weight updates to the other parties. This
ability can be exploited to broadcast weight updates that
introduce backdoor behavior to the models that receive
the update [2]. Federated learning settings with secure
aggregation are especially susceptible to this attack as the
individual weight updates cannot be inspected. Further-
more, the algorithm that generates the weight updates can
also take into account the anomaly detection technique,
in order to bypass the defense when secure aggregation is
not used. Various studies have also worked on making
distributed learning settings that converge to a useful
model despite the presence of Byzantine workers [27],
[29], [4]. The guarantees that these robust algorithms
provide, however, are shown to be insufﬁcient [17], [3].
In response to the backdoor attacks that have been
devised, there have been several papers that aim to remove
backdoor behavior, besides the ones mentioned in this
paper. Liu et al., 2018 [15] take a similar approach of
pruning neurons based on the latent representations of a
known clean set of inputs, operating under the assumption
that the backdoor neurons are dormant for clean inputs.(a) Baseline model. The poisoned representations have a signiﬁ-
cantly higher correlation than the clean representations, and thus
can be ﬁltered by removing samples with the top correlations.
(b) Model with adversarial embedding. The poisoned and clean
representations have similar distributions of correlations, and
thus ﬁltering the samples with top correlations removes an equal
portion of clean and poisoned samples.
Figure 7: Correlation of latent representations of all inputs
in the training dataset with the top eigenvector for the data
ﬁltering defense based on spectral signatures [25]. The
representations of poisoned inputs are in red.
Since it is possible to train a model to be robust to this
pruning, Liu et al. recommend a combination of pruning
and ﬁne-tuning of the model to remove the backdoor
behavior. Some backdoor mitigation techniques work on
the input space of the model instead of the latent space,
relying on either training a model to identify anomalous
inputs, or to remove anomalous features in inputs before
feeding to the model [16].
Besides backdoor attacks, there have been many stud-
ies on adversarial machine learning where the discontin-
uous input-output mappings of models are exploited to
generate adversarial examples [23], [10]. It has also been
shown to be possible to generate adversarial images by
perturbing the color space of the image, thus preserving
the smoothness of the image, in order to evade detection
methods that rely on the abrupt pixel changes found in
many adversarial image detection methods [12].
(a) Baseline model. The representations of poisoned inputs (in
red) form a distinct separate cluster from the clean inputs, and
thus are easily separated by k-means clustering.
(b) Model with adversarial embedding. The representations of
poisoned inputs (in red) have a similar distribution as those of
clean inputs, thus both clusters formed by k-means clustering
contain a signiﬁcant number of poisoned samples.
Figure 8: Latent representations of all inputs in the training
dataset projected onto their top two independent com-
ponents for the data ﬁltering defense using activation
clustering [7]. The poisoned inputs are depicted in red.
6. Conclusions
We have designed a novel backdoor embedding attack
that successfully bypasses several prior backdoor detection
algorithms. While backdoor detection using the learned
latent representations greatly reduces the dimensionality
and thus complexity of the defense techniques, we have
shown that a sophisticated attacker is easily able to hide
the signals of the backdoor images in the latent represen-
tation, rendering the defense algorithms ineffective.
Acknowledgments
This work is supported by the NUS Early Career
Research Award (NUS ECRA) by the Ofﬁce of the Deputy
President, Research & Technology (ODPRT), grant num-
ber NUS ECRA FY19 P16.References
[1] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated
gradients give a false sense of security: Circumventing defenses to
adversarial examples. arXiv preprint arXiv:1802.00420 , 2018.
[2] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin,
and Vitaly Shmatikov. How to backdoor federated learning. arXiv
preprint arXiv:1807.00459 , 2018.
[3] Moran Baruch, Gilad Baruch, and Yoav Goldberg. A little is
enough: Circumventing defenses for distributed learning. arXiv
preprint arXiv:1902.06156 , 2019.
[4] Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al. Machine
learning with adversaries: Byzantine tolerant gradient descent. In
Advances in Neural Information Processing Systems , pages 119–
129, 2017.
[5] Nicholas Carlini and David Wagner. Adversarial examples are not
easily detected: Bypassing ten detection methods. In Proceedings
of the 10th ACM Workshop on Artiﬁcial Intelligence and Security ,
pages 3–14. ACM, 2017.
[6] Nicholas Carlini and David Wagner. Towards evaluating the ro-
bustness of neural networks. In 2017 IEEE Symposium on Security
and Privacy (SP) , pages 39–57. IEEE, 2017.
[7] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Lud-
wig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav
Srivastava. Detecting backdoor attacks on deep neural networks
by activation clustering. arXiv preprint arXiv:1811.03728 , 2018.
[8] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song.
Targeted backdoor attacks on deep learning systems using data
poisoning. arXiv preprint arXiv:1712.05526 , 2017.
[9] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David
Page, and Thomas Ristenpart. Privacy in pharmacogenetics: An
end-to-end case study of personalized warfarin dosing. In 23rd
fUSENIXgSecurity Symposium ( fUSENIXgSecurity 14) , pages
17–32, 2014.
[10] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Ex-
plaining and harnessing adversarial examples. arXiv preprint
arXiv:1412.6572 , 2014.
[11] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets:
Identifying vulnerabilities in the machine learning model supply
chain. arXiv preprint arXiv:1708.06733 , 2017.
[12] Hossein Hosseini and Radha Poovendran. Semantic adversarial
examples. In Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition Workshops , pages 1614–1619, 2018.
[13] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q
Weinberger. Densely connected convolutional networks. In Pro-
ceedings of the IEEE conference on computer vision and pattern
recognition , pages 4700–4708, 2017.
[14] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers
of features from tiny images. Technical report, Citeseer, 2009.
[15] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-
pruning: Defending against backdooring attacks on deep neural
networks. In International Symposium on Research in Attacks,
Intrusions, and Defenses , pages 273–294. Springer, 2018.
[16] Yuntao Liu, Yang Xie, and Ankur Srivastava. Neural trojans. In
2017 IEEE International Conference on Computer Design (ICCD) ,
pages 45–48. IEEE, 2017.
[17] El Mahdi El Mhamdi, Rachid Guerraoui, and S ´ebastien Rouault.
The hidden vulnerability of distributed learning in byzantium.
arXiv preprint arXiv:1802.07927 , 2018.
[18] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive
privacy analysis of deep learning: Stand-alone and federated learn-
ing under passive and active white-box inference attacks. Security
and Privacy (SP), 2019 IEEE Symposium on , 2019.
[19] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu,
Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison
frogs! targeted clean-label poisoning attacks on neural networks. In
Advances in Neural Information Processing Systems , pages 6103–
6113, 2018.[20] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly
Shmatikov. Membership inference attacks against machine learning
models. In 2017 IEEE Symposium on Security and Privacy (SP) ,
pages 3–18. IEEE, 2017.
[21] Karen Simonyan and Andrew Zisserman. Very deep convolu-
tional networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556 , 2014.
[22] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian
Igel. The German Trafﬁc Sign Recognition Benchmark: A multi-
class classiﬁcation competition. In IEEE International Joint Con-
ference on Neural Networks , pages 1453–1460, 2011.
[23] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing
properties of neural networks. arXiv preprint arXiv:1312.6199 ,
2013.
[24] Florian Tram `er, Fan Zhang, Ari Juels, Michael K Reiter, and
Thomas Ristenpart. Stealing machine learning models via pre-
diction apis. In 25thfUSENIXgSecurity Symposium ( fUSENIXg
Security 16) , pages 601–618, 2016.
[25] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures
in backdoor attacks. In Advances in Neural Information Processing
Systems , pages 8000–8010, 2018.
[26] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal
Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse:
Identifying and mitigating backdoor attacks in neural networks.
2019.
[27] Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Generalized
byzantine-tolerant sgd. arXiv preprint arXiv:1802.10116 , 2018.
[28] Chaofei Yang, Qing Wu, Hai Li, and Yiran Chen. Generative
poisoning attack method against neural networks. arXiv preprint
arXiv:1703.01340 , 2017.
[29] Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett.
Byzantine-robust distributed learning: Towards optimal statistical
rates. arXiv preprint arXiv:1803.01498 , 2018.