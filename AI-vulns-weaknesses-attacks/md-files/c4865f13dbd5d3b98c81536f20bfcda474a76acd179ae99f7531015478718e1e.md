Imperceptible Adversarial Attacks on Tabular Data
Vincent BalletyXavier RenardyJonathan Aigrainy
Thibault LaugelzPascal FrossardMarcin Detynieckiyzx
√âcole Polytechnique F√©d√©rale de Lausanne
yAXA, Paris, France
zSorbonne Universit√©, CNRS, LIP6, Paris, France
xPolish Academy of Science, IBS PAN, Warsaw, Poland
vincent.ballet@me.com ,pascal.frossard@epfl.ch ,thibault.laugel@lip6.fr
{xavier.renard ,jonathan.aigrain ,marcin.detyniecki}@axa.com
Abstract
Security of machine learning models is a concern as they may face adversarial
attacks for unwarranted advantageous decisions. While research on the topic has
mainly been focusing on the image domain, numerous industrial applications, in
particular in Ô¨Ånance, rely on standard tabular data. In this paper, we discuss the
notion of adversarial examples in the tabular domain. We propose a formalization
based on the imperceptibility of attacks in the tabular domain leading to an approach
to generate imperceptible adversarial examples. Experiments show that we can
generate imperceptible adversarial examples with a high fooling rate.
1 Introduction
As machine learning becomes more prevalent in many industries, concerns are raised about the
security its usgae. Research has shown that machine learning models are sensitive to slight changes
in their input, resulting in unwarranted predictions, application failures or errors. The Adversarial
Machine Learning Ô¨Åeld studies this issue with the design of attacks and defenses with an active focus
on the image domain [Goodfellow et al., 2015, Biggio et al., 2013, Kurakin et al., 2017]. However,
many machine learning classiÔ¨Åers in the industry rely on tabular data in input to predict labels in a
wide variety of tasks (e.g. stocks, credit, fraud, etc.).
To illustrate the threat of adversarial attacks in a tabular context, we consider the scenario where a
bank customer applies for a loan. A machine learning model is used to make a decision regarding
the acceptance of the application based on customer provided information (incomes, age, etc.). The
model advises the bank to reject the application of our customer who is determined to get the loan by
Ô¨Ålling false information to mislead the model. In this work, we claim that the key for this attack to
succeed is its imperceptibility: the application should remain credible and relevant for a potential
expert eye, in coherence with the model‚Äôs prediction. We discuss the notions of imperceptibility and
relevance in order to design an attack relying on the intuition that only a subset of features are critical
for the prediction according to the expert eye (e.g. income, age). Thus, the attacker should minimize
manipulations on this subset for the attack to be imperceptible and instead rely on less important
features to get the model to accept the application. The intuition behind this idea is depicted Figure 1.
In the following section, we discuss the notion of imperceptibility in order to propose a deÔ¨Ånition for
adversarial examples in the context of tabular data. Using this formalization, we propose in Section 4
NeurIPS 2019 Workshop on Robust AI in Financial Services: Data, Fairness, Explainability, Trustworthiness
and Privacy (Robust AI in FS 2019), Vancouver, Canada.arXiv:1911.03274v2 [stat.ML] 13 Dec 20191 2 3 4 5 6 7 8
Number of pets10k20k30k40k50k60k70k80kSalaryAccepted Loan region
Rejected Loan regionAccepted Loan region
Rejected Loan regionFigure 1: Illustration of an imperceptible perturbation to craft an adversarial attack on tabular data.
The plain arrow is a "naive", perceptible, perturbation to cross the decision boundary (e.g. salary for
a loan application). The dashed arrow is an imperceptible perturbation relying on the number of pets.
an approach to generate adversarial examples called LowProFool. To assess these propositions,
we perform experiments on four literature datasets. We show in Section 5 and Section 6 that the
generated adversarial examples have a low perceptibility compared to other adversarial attacks from
the state-of-the-art while the success rate (fooling rate) for the attack is kept high.
2 Imperceptible adversarial attacks on images
DeÔ¨Åning the exact properties of an adversarial example remains a subject of debate among the
research community, in particular in terms of imperceptibility of the attack. On the one hand, Carlini
and Wagner deÔ¨Åne the adversarial example as inputs that are close to natural inputs but classiÔ¨Åed
incorrectly [Carlini and Wagner, 2017] and Szegedy et al. mention imperceptible non-random
perturbation [Szegedy et al., 2014]. On the other hand, Karmon et al. allow the noise to be visible but
conÔ¨Åned to a small, localized patch of the image [Karmon et al., 2018] and Brown et al. generate
adiscernible adversarial patch that is stamped on the original picture to make the classiÔ¨Åer always
predict the same class [Brown et al., 2017].
While on images it is natural for a human to check whether the model has been fooled by comparing
the image and the output of the classiÔ¨Åer, assessing or measuring the perceptibility of the perturbation
is complex. The most commonly used measure is the `pnorm. For instance, Szegedy et al.
formalizes the deÔ¨Ånition of adversarial example and slight perturbation in the image domain as
f(x+r)6=f(x)withjjrjjp<wherefis a classiÔ¨Åer,xan image andrthe adversarial perturbation.
Theslight perturbation is controlled by , the upper bound of its amplitude.
However, it is sometimes inaccurate to use the `pnorm to assess the perceptibility of a perturbation.
Evidence of that came to light recently when Sharif et al. showed that the `pnorm is insufÔ¨Åcient to
measure perceptual similarity between two images. The authors studied how images that are really
close in terms of human-observed perceptibility (e.g. rotated versions of the same image) can still
have large differences according to the `pnorm. On the contrary, some images may be semantically
different but considered close by this metric [Sharif et al., 2018].
Given these studies on the imperceptibility of attacks on images, in the next section we consider the
speciÔ¨Åcities of tabular data to formalize the notion of imperceptibility for this context.
3 Formalization of the notion of imperceptible attacks on tabular data
The notion of imperceptibility and the way it can be measured differ for tabular data compared to
images for two main reasons. First, tabular features are not interchangeable like pixels. Second, while
most people can usually tell the correct class of an image and whether it appears altered or not, it is
much complex for tabular data: this type of data is less readable and expert knowledge is required.
2On tabular data, to decide if two instances share the same class, the expert is likely to focus on a
subset of features (among the whole feature space) he considers important for the classiÔ¨Åcation task
(eg. to predict the acquisition of a loan, incomes are more important than other features). Thus, to
detect the presence of any fraudulent modiÔ¨Åcation, the expert is likely to check more thoroughly the
veracity of this subset of features: the attacker should avoid modiÔ¨Åcations on it. Then, we propose to
measure the perceptibility of an adversarial attack as the `pnorm of the perturbation weighted by a
feature importance vector, which describes the likelihood of a feature to be investigated by the expert.
Given that tabular data cannot be manipulated the same way as images, we argue that using the `p
norm is not subject to the same extent to the Ô¨Çaws discussed in Section 2.
Another aspect of the attack that is intrinsically contained in the notion of imperceptibility is the
coherence of the output. While adversarial examples for image data naturally satisfy this constraint
(pixels are deÔ¨Åned as integers between 0and255), this needs to be enforced for tabular data. In fact,
we expect the generated attacks to lie in natural, intuitive or hard bounds designed by the human
intuition or the knowledge of an expert. If the adversarial example does not satisfy these constraints,
the perturbed features must be bounded and rounded so as to Ô¨Åt into their context: for instance, the
age might be forced into a positive number, or a boolean to be discrete depending on the problem.
To craft adversarial examples, we make the assumption that perturbations on less relevant features
allow to reach the desired opposed class label. In fact, we seek to exploit the discrepancies between
the classiÔ¨Åer‚Äôs learned vector of feature importance and the expert‚Äôs knowledge. The perturbation
is expected to be of a higher lpnorm compared to optimal shortest paths to the classiÔ¨Åer frontier.
Despite being of higher norm, the perturbation should be less perceptible than another perturbation of
smaller norm but supported by more relevant features for the experts.
Formally, we consider a set of examples Xwhere each example is denoted by x(i)withi2[1:::N]
and associated with a label y(i). The set of examples Xis deÔ¨Åned by a set of features j2Jand
each feature vector is noted xjwithj2J= [1:::D]. Also we consider f:RD!f0;1g, a binary
classiÔ¨Åer mapping examples to a discrete label l2f0;1gandd:RD![0;1]a mapping between
the perturbation r2RDand its perceptibility value. Finally, we deÔ¨Åne ARDthe set of valid,
coherent samples.
For a givenx2RD, its original label s=f(x)and a target label t6=s, we aim at generating the
optimal perturbation vector r2RDsuch that
r= arg min
rd(r)forr2RD
s.t.f(x) =s6=f(x+r) =tandx+r2A(1)
As mentioned earlier, each feature j2Jis associated to a feature importance coefÔ¨Åcient vj2R
gathered in a feature importance vector v= [v1;:::;v j;:::;v D]wherevj>0;8j2J
We extend the deÔ¨Ånition of dto include the feature importance v. The perceptibility for tabular data
is then deÔ¨Åned as the `pnorm of the feature importance weighted perturbation vector, such that:
dv(r) =jjrvjj2
pwhereis the Hadamard product (2)
Finally, in Equation 1, the constraint x+r2Abrings the idea of coherence of the generated
adversarial example. The nature of regular tabular data requires that methods respect the discrete,
categorical and continuous features. Each feature must conform with the dataset so as to be as
imperceptible as possible, e.g. the feature ageshould not be lower than 0.
4 Generation of Imperceptible Adversarial Examples on Tabular Data
Our objective is to craft adversarial examples such that perturbations applied to a tabular data example
are imperceptible to the expert‚Äôs eye, i.e. higher perturbations for irrelevant or less important features,
than for relevant ones, as described in Section 3.
Since solving the proposed minimization problem is complex, we propose to deÔ¨Åne the objective
function as an aggregation of the class change constraint and the minimization of dv:
g(r) =L(x+r;t) +jjvrjjp
3Algorithm 1 LowProFool
Input: ClassiÔ¨Åerf, samplex, target label t, loss functionL, trade-off factor , feature importance
v, maximum number of iterations N, scaling factor 
Output: Adversarial example x0
1:r [0;0;:::; 0]
2:x0 x
3:foriin0:::N 1do
4:ri  r r(L(xi;t) +jjvrjjp)
5:r r+ri
6:xi+1 clip(x+r)
7:end for
8:x0 arg minxidv(xi)8i2[0:::N 1]s.t.f(xi)6=f(x0)
9:returnx0
WithL(x;t)the value of the loss of the model fcalculated for xand target class t, and>0. On
the one hand, the regularizer jjvrjjpallows to minimize the perturbation rwith respect to its
perceptibility. On the other hand, by minimizing the loss L(x+r;t), we ensure that the perturbation
leads the perturbed sample towards the target class.
Setting the value of the hyperparameter 2Rallows to control the weight associated to the penalty
of using important features, with respect to the feature importance vector. Given a Ô¨Åxed number of
iterations, our goal is to minimize the weighted norm jjvrjjpassociated with each adversarial
example, while maximizing the proportion of samples x2Xthat could cross the classiÔ¨Åcation
frontier. These two optimization problems constitute a trade-off that is represented by choosing the
right value for .
The LowProFool (low proÔ¨Åle) algorithm is then deÔ¨Åned as an optimization problem in which we
search for the minimum of the objective function gusing a gradient descent approach. More
concretely, we make use of the gradient data so as to guide the perturbation towards the target
class in an iterative manner. At the same time, we penalize the perturbation proportionally to the
feature importance associated with the features, so as to minimize the perceptibility of the adversarial
perturbation as deÔ¨Åned in Section 3. Algorithm 1 outline LowProFool algorithm for binary classiÔ¨Åers
and features in the continuous domain.
5 Experimentation framework
5.1 Metrics for adversarial attacks on tabular data
Success Rate : The success rate or fooling rate is a common metric to measure the efÔ¨Åciency of
an adversarial attack. Let us deÔ¨Åne the set ^Xthat comprises every tuple (x;x0)such thatx2Xand
x0is a successfully crafted adversarial example from x, that is:f(x)6=f(x0).
For a given number of iterations Nof Algorithm 1, we then deÔ¨Åne the success rate Nas:
N=j^Xj
jXj
Norms of perturbation : To evaluate how successful an attack is, we also measure the norm of
the adversarial perturbation.
jjrjjp
However, as discussed in Section 3, we measure the perceptibility of the perturbation by calculating
the weighted norm dv(r).
For both weighted and non-weighted perturbation norms we compute the mean value per pair of
datasets and method over the set of samples X.
Distance to the closest neighbor : Although perturbation norms allow us to compare methods
between one another, they do not give insight about how signiÔ¨Åcant the perturbation is for a given
4dataset. To this end, we propose to compute the average weighted distance to the closest neighbor of
each original sample x. This metric helps us develop an intuition about what is the distance between
two points in the dataset and how it compares to the norm of the perturbation.
Formally, for a sample x2RDwe compute its closest neighbor as the following
neigh (x) = arg min
p2Xdv(x p)
5.2 Data
Datasets : We run experiments on four well-known datasets: German Credit dataset [Dua and
Graff, 2017], Australian Credit dataset [Dua and Graff, 2017], the Default Credit Card dataset [Yeh
and Lien, 2009] and the Lending Club Loan dataset [Kan, 2019]. They are all related to the Ô¨Ånancial
service industry, hence representing a good case study for the scenario we considered in Section 1.
Preprocessing : The proposed notion applies to numerical continous data. Hence, in our experi-
ments discrete features are treated as continous and non-ordered categorical features are dropped.
Each dataset is split into train, test and validation sets. Test and validation subsets respectively hold
250 and 50 samples. All the remaining samples are used to train the model. The test data is used to
generate the adversarial examples and the validation data used to optimize hyperparameters.
5.3 Parameters of the adversarial attack
Objective function : As described in Section 4, the objective function is deÔ¨Åned as g(r) =
L(x+r;t) +jjvrjjp. We deÔ¨Åned the loss function Las the the binary cross entropy function
and use the`2norm to optimize the weighted norm jjvrjj2
Feature importance : In order to model the expert‚Äôs knowledge, we use the absolute value of the
Pearson‚Äôs correlation coefÔ¨Åcient of each feature with the target variable t. We scale the obtained
feature importance vector vby multiplying it by the inverse of its `2norm so as to feed the algorithm
with a unit vector, irrespective of the dataset at hand. More formally,
v=jX;Yj
jjX;Yjj2
2wherejuj= [ju0j;ju1j;:::]
It is important to note that the expert‚Äôs knowledge could be modelled using various other deÔ¨Ånitions.
Pearson‚Äôs correlation was chosen for its simplicity and its potential to replicate the intuition of an
expert through a linear correlation at the scale of the dataset.
Clipping : To make sure that the generated adversarial examples x0lie in a coherent subspace
ARD, we clipx0to the bounds of each feature. More formally we constrain each feature
j2[1:::D ]to stay in their natural range [min(xj);max(xj)].
Machine Learning Model : For each dataset we build a fully connected neural network using
dense ReLU layers and a Softmax layer for the last one.
Comparison with other attacks: To the best of our knowledge, no method has been proposed to
generate adversarial examples of tabular data. We hence compare LowProFool against two other
methods that are FGSM [Goodfellow et al., 2015] and DeepFool [Moosavi-Dezfooli et al., 2015],
both being state-of-the-art baselines for gradient-based methods in the image domain. To do so, we
use the metrics deÔ¨Åned in Section 5.1.
5.4 Experimental results
We report in Table 1 the results for the metrics deÔ¨Åned in Section 5.1. The mean perturbation ( Mean ),
weighted mean perturbation ( WMean ) and distance to the closest neighbors ( MD O) as well as its
weighted version ( WMD O) are computed only for pairs of samples (x;x0)such thatxis a sample
that belongs to the test set Xandx0is an adversarial example crafted from xthat successfully crosses
5the classiÔ¨Åer‚Äôs frontier, i.e. f(x)6=f(x0). This allows us to compare the methods between each
other.
We observe that LowProFool succeeds in fooling the classiÔ¨Åer ( SR) almost 95% of the time. Only
one dataset shows lower performances with a success rate of 86%. DeepFool‚Äôs success rate is about
100% on each dataset while FGSM performs very poorly, from 0%to59% success rate.
Then, comparing the mean `2perturbation norm of DeepFool and LowProFool shows that DeepFool
always leads to better results. Averaging on the four datasets, the perturbation norm of LowProFool
is1:7times higher that DeepFool‚Äôs. FGSM shows order of magnitude larger perturbation norms that
the two other methods.
Looking at the weighted perturbation, we observe that DeepFool consistently leads to worst results
in comparison with LowProFool. We even get a ratio of 60% between the weighted mean norm of
LowProFool and DeepFool on the Default Credit Card dataset. Figure 3 is a diagrammatic comparison
of discrete results presented in Table 1 and allows better visual comparison between DeepFool and
LowProFool.
To get a feeling of what the mean perturbation norms represent for each dataset, i.e. what order
of magnitude are the perturbations, we compare them to the mean distance between each original
sample and their closest neighbor in terms of weighted ( WMD O) and non-weighted ( MD O) distance.
We observe in Figure 2 that except for the Australian Credit dataset, the mean perturbation norm
is in average 60% smaller than the mean distance to the closest neighbors and the weighted mean
perturbation norm 77% smaller than the mean weighted distance to the closest neighbors. On the
contrary, we observe that it takes a higher perturbation for adversarial examples to be generated on
the Australian Credit dataset. In fact in terms of weighted distance, the perturbation is four times
bigger than the distance to the closest neighbor. This relates with Table 1 in which we observe that
the Australian Credit dataset shows higher perturbation norms and distance to neighbors than any
other dataset studied.
credit-g defaultcredit australian loan
Dataset42.9%
5.9%381.8%
22.6%70.2%
30.7%189.8%
18.8%100%Ratio between LowProFool's mean perturbation
norm and mean distance to closest neighbor
Ratio between LowProFool's mean weighted perturbation
norm and mean weighted distance to closest neighbor
Figure 2: Comparison between the perturbation norm and the distance to the closest neighbor. Results
are shown as ratio between the mean (weighted) perturbation norm and the mean (weighted) distance
to the closest neighbor.
6 Discussion
We made the hypothesis in Section 1 that perturbations on less relevant features still allow to move
sufÔ¨Åciently an example in the feature space to get access to the desired opposed class label. The
reported results conÔ¨Årm our hypothesis to some extent. Indeed, we never achieve both 100% success
rate and satisfactory results on the weighted mean norm. However, aiming for a smaller success rate
of 95%, we reach low perturbation norm ratios between LowProFool and DeepFool.
6credit-g defaultcredit australian loan
Dataset94.0%
85.9%96.8%94.8%
51.3%
40.0%79.8%
58.3%100%
Ratio between LowProFool's and DeepFool's success rate
Ratio between LowProFool's and DeepFool's mean weighted perturbation normFigure 3: Comparison between state-of-the-art DeepFool and LowProFool. The blue bars show the
ratio between the success rate of LowProFool and DeepFool. The red bars exhibit the ratio between
LowProFool mean weighted perturbation norm and DeepFool mean weighted perturbation norm.
Table 1: Results of the tests ran on the test set for each dataset. SRis the success rate deÔ¨Åned in
Section 5.1. Mean andWMean respectively correspond to the mean norm of the perturbation and its
weighted equivalent as deÔ¨Åned in Section 5.1. MD OandWMD Orefer to the distance to the closest
at the original sample, based on a `2distance and its weighted version dvas proposed in Section 5.1DatasetMethod SR Mean WMean MD O WMD OGerman C.LowProFool 0.94 0.3440.282 0.0390.027 0.490.193 0.0910.039
DeepFool 1.0 0.210.181 0.0760.077 0.4850.193 0.0890.038
FGSM 0.192 19.6975.61 5.68321.827 0.4770.173 0.0850.037Default C.LowProFool 0.856 0.0610.109 0.0020.005 0.1990.137 0.0340.031
DeepFool 0.996 0.0230.026 0.0050.007 0.1980.132 0.0360.032
FGSM 0.588 1.1224.127 0.2450.901 0.2070.154 0.0350.032AustralianLowProFool 0.968 0.7100.530 0.210.141 0.3740.188 0.0550.027
DeepFool 1.0 0.500.349 0.2630.183 0.3750.189 0.0550.027
FGSM 0 ‚Äì ‚Äì ‚Äì ‚ÄìLending L.LowProFool 0.944 0.1240.168 0.0140.027 0.6590.207 0.0620.027
DeepFool 0.996 0.1070.154 0.0240.035 0.660.209 0.0620.028
FGSM 0 ‚Äì ‚Äì ‚Äì ‚Äì
This intuitively results from the trade-off introduced with the objective function gdeÔ¨Åned in Section 4.
Take for instance a sample in the set of the original samples Xthat is far from the classiÔ¨Åer frontier
and for which the closest path to reach the classiÔ¨Åer frontier is along highly perceptible feature with
regards to the deÔ¨Åned feature-importance vector v. It is not unexpected that in a Ô¨Åxed number of
iterations and given the fact that we highly penalize moves onto highly perceptible features, the
sample does not reach the classiÔ¨Åer frontier.
Furthermore, the hyperparameters controlling the speed of move as well as the trade-off between
class-changing and minimizing the perceptibility value are Ô¨Åtted on the whole validation set. We
then believe that these hyperpameters do not generalize enough hence preventing a few outliers from
either crossing the classiÔ¨Åcation frontier or minimizing enough their perceptibility value.
7To the contrary, the DeepFool method involves no hyperparameters. First, it adapts the speed of the
descent by scaling the perturbation with the norm of the difference of the logits towards the target and
towards the source. Intuitively, this means that when the sample is far from the classiÔ¨Åer‚Äôs frontier,
the perturbation will be large, and will get smaller when the crafted adversarial examples gets close
to the frontier.
Second, the objective of the DeepFool algorithm is to minimize the `2norm of the perturbation
under no constraint regarding the perceptibility of the perturbation. This naturally makes it easier for
DeepFool to generate adversarial example compared to LowProFool which also seeks to minimize the
norm of the Hadamard product between the feature importance vector vand the perturbation vector
rIt is hence not surprising for LowProFool not to reach as good success rate results as DeepFool.
Concerning FGSM, it seems that we reach the limits of a method that worked sufÔ¨Åciently well on
images but is actually unsuccessful on tabular data.
The adversarial examples generated with LowProFool outperform DeepFool‚Äôs to a great extent in
terms of the perceptibility metrics deÔ¨Åned in Section 5.1. What‚Äôs more, it did not cost much in terms
of lowering the success rate to reach those results. So not only can we generate adversarial examples
in the tabular domain using less important features to the eye of the expert at hand, but we can also
achieve great results in terms of imperceptibility of the attack. We believe that the existence of
these adversarial examples is directly tied to the discrepancy between the classiÔ¨Åer‚Äôs learned feature
importance and the a priori feature importance vector v.
The comparison between the weighted mean norm and the distance to the closest neighbors reveals
that the generated adversarial examples are very close to their original example. For instance, the
average weighted perturbation represents only 5:9%of the average distance to the closest neighbor
for the Default Credit dataset. This behaviour is really desirable as it reinforces our belief that the
generated adversarial examples are closer to their original sample more than to any other sample
hence being the most imperceptible.
About coherence, we were worried that clipping a posteriori would introduce deadlocks. For instance,
take a sample for which the gradient of the objective function points towards a unique direction but
we restrict any move towards this direction (e.g. negative age). In practice, this behaviour did not
show.
7 Limitations and perspectives
Our approach constitutes a white-box attack as the attacker has access to the model as well as the
whole dataset. While a white-box attack often does not comply with real-world attacks, we believe
that it is achievable to perform a black-box attack by using a surrogate model under the assumption to
have a minimum number of queries allowed to the oracle. Papernot et al. showed they could achieve
a high rate of adversarial example misclassiÔ¨Åcations on online deep-learning APIs by creating a
substitute model to attack a black-box target model. To train the substitute model, they synthesize a
training set and annotated it by querying the target model for labels [Papernot et al., 2016]. Liu et al.
also showed that transferable non-targeted adversarial examples are easy to Ô¨Ånd [Liu et al., 2016].
We seek to investigate this area of research in the future steps. Indeed, black-box approaches would
allow us to build attacks that do not rely on gradient information as it is the case for LowProFool.
8 Conclusion
In this paper, we have focused our attention on the notion of adversarial examples in the context
or tabular data. To the best of our knowledge, there has not been much focus on this data domain,
contrary to the images domain that attracted lots of attention. Images and tabular data do not share
the same perceptibility concepts and what has been formalized on images cannot be applied on
tabular data. We propose a formalization of the notion of perceptibility and coherence of adversarial
examples in tabular data, along with a method, LowProFool, to generate such adversarial examples.
Proposed metrics show successful results on the ability to fool the model and generate imperceptible
adversarial examples. Gradient-based methods such as LowProFool show limits when it comes to
discrete features. We will investigate new methods to answer this matter. While our proposition
remains subject to challenges, we believe it is a Ô¨Årst step towards a more comprehensive and complete
view of the Ô¨Åeld of Adversarial Machine Learning.
8Acknowledgements
We would like to thank Seyed-Mohsen Moosavi-Dezfooli and Apostolos Modas who provided their
comments, discussion and expertise on this research.
References
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. 2015. URL http://arxiv.org/abs/1412.6572 .
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim ≈†rndi ¬¥c, Pavel Laskov, Giorgio
Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Hendrik
Blockeel, Kristian Kersting, Siegfried Nijssen, and Filip ≈Ωelezn√Ω, editors, Machine Learning and
Knowledge Discovery in Databases , 2013.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. ICLR
2017 , 2017.
Nicholas Carlini and David A. Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. 10th ACM Workshop on ArtiÔ¨Åcial Intelligence and Security , 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations , 2014.
Danny Karmon, Daniel Zoran, and Yoav Goldberg. Lavan: Localized and visible adversarial noise.
CoRR , 2018.
Tom B. Brown, Dandelion Man√©, Aurko Roy, Mart√≠n Abadi, and Justin Gilmer. Adversarial patch.
CoRR , 2017.
Mahmood Sharif, Lujo Bauer, and Michael K. Reiter. On the suitability of l p-norms for creating and
preventing adversarial examples. IEEE Conference on Computer Vision and Pattern Recognition
Workshops , 2018.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
I-Cheng Yeh and Che-hui Lien. The comparisons of data mining techniques for the predictive
accuracy of probability of default of credit card clients. Expert Systems with Applications , 2009.
Wendy Kan. Lending club loan data, version 1, 2019. URL https://www.kaggle.com/wendykan/
lending-club-loan-data .
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. IEEE conference on computer vision and pattern
recognition , 2015.
N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Berkay Celik, and A. Swami. Practical Black-Box
Attacks against Machine Learning. 2017 ACM on Asia conference on computer and communica-
tions security , 2016.
Y . Liu, X. Chen, C. Liu, and D. Song. Delving into Transferable Adversarial Examples and Black-box
Attacks. CoRR , 2016.
9