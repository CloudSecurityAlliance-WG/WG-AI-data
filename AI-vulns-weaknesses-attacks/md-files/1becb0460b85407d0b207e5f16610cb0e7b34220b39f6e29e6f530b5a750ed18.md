DeepFool: a simple and accurate method to fool deep neural networks
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard
¬¥Ecole Polytechnique F ¬¥ed¬¥erale de Lausanne
fseyed.moosavi,alhussein.fawzi,pascal.frossard gat epfl.ch
Abstract
State-of-the-art deep neural networks have achieved im-
pressive results on many image classiÔ¨Åcation tasks. How-
ever, these same architectures have been shown to be un-
stable to small, well sought, perturbations of the images.
Despite the importance of this phenomenon, no effective
methods have been proposed to accurately compute the ro-
bustness of state-of-the-art deep classiÔ¨Åers to such pertur-
bations on large-scale datasets. In this paper, we Ô¨Åll this
gap and propose the DeepFool algorithm to efÔ¨Åciently com-
pute perturbations that fool deep networks, and thus reli-
ably quantify the robustness of these classiÔ¨Åers. Extensive
experimental results show that our approach outperforms
recent methods in the task of computing adversarial pertur-
bations and making classiÔ¨Åers more robust.1
1. Introduction
Deep neural networks are powerful learning models that
achieve state-of-the-art pattern recognition performance in
many research areas such as bioinformatics [1, 16], speech
[12, 6], and computer vision [10, 8]. Though deep net-
works have exhibited very good performance in classiÔ¨Åca-
tion tasks, they have recently been shown to be particularly
unstable to adversarial perturbations of the data [18]. In
fact, very small and often imperceptible perturbations of the
data samples are sufÔ¨Åcient to fool state-of-the-art classiÔ¨Åers
and result in incorrect classiÔ¨Åcation. (e.g., Figure 1). For-
mally, for a given classiÔ¨Åer, we deÔ¨Åne an adversarial per-
turbation as the minimal perturbationrthat is sufÔ¨Åcient to
change the estimated label ^k(x):
(x;^k) := min
rkrk2subject to ^k(x+r)6=^k(x);(1)
wherexis an image and ^k(x)is the estimated label. We
call(x;^k)the robustness of ^kat pointx. The robustness
of classiÔ¨Åer ^kis then deÔ¨Åned as
1To encourage reproducible research, the code of DeepFool is made
available at http://github.com/lts4/deepfool
Figure 1: An example of adversarial perturbations.
First row: the original image xthat is classiÔ¨Åed as
^k(x)=‚Äúwhale‚Äù. Second row: the image x+rclassiÔ¨Åed
as^k(x+r)=‚Äúturtle‚Äù and the corresponding perturbation r
computed by DeepFool. Third row: the image classiÔ¨Åed
as ‚Äúturtle‚Äù and the corresponding perturbation computed
by the fast gradient sign method [4]. DeepFool leads to a
smaller perturbation.arXiv:1511.04599v3 [cs.LG] 4 Jul 2016adv(^k) =Ex(x;^k)
kxk2; (2)
where Exis the expectation over the distribution of data.
The study of adversarial perturbations helps us understand
what features are used by a classiÔ¨Åer. The existence of such
examples is seemingly in contradiction with the generaliza-
tion ability of the learning algorithms. While deep networks
achieve state-of-the-art performance in image classiÔ¨Åcation
tasks, they are not robust at all to small adversarial pertur-
bations and tend to misclassify minimally perturbed data
that looks visually similar to clean samples. Though adver-
sarial attacks are speciÔ¨Åc to the classiÔ¨Åer, it seems that the
adversarial perturbations are generalizable across different
models [18]. This can actually become a real concern from
a security point of view.
An accurate method for Ô¨Ånding the adversarial perturba-
tions is thus necessary to study and compare the robustness
of different classiÔ¨Åers to adversarial perturbations. It might
be the key to a better understanding of the limits of cur-
rent architectures and to design methods to increase robust-
ness. Despite the importance of the vulnerability of state-of-
the-art classiÔ¨Åers to adversarial instability, no well-founded
method has been proposed to compute adversarial perturba-
tions and we Ô¨Åll this gap in this paper.
Our main contributions are the following:
We propose a simple yet accurate method for comput-
ing and comparing the robustness of different classi-
Ô¨Åers to adversarial perturbations.
We perform an extensive experimental comparison,
and show that 1) our method computes adversarial per-
turbations more reliably and efÔ¨Åciently than existing
methods 2) augmenting training data with adversarial
examples signiÔ¨Åcantly increases the robustness to ad-
versarial perturbations.
We show that using imprecise approaches for the com-
putation of adversarial perturbations could lead to dif-
ferent and sometimes misleading conclusions about the
robustness. Hence, our method provides a better un-
derstanding of this intriguing phenomenon and of its
inÔ¨Çuence factors.
We now review some of the relevant work. The phe-
nomenon of adversarial instability was Ô¨Årst introduced and
studied in [18]. The authors estimated adversarial examples
by solving penalized optimization problems and presented
an analysis showing that the high complexity of neural net-
works might be a reason explaining the presence of adver-
sarial examples. Unfortunately, the optimization method
employed in [18] is time-consuming and therefore does notscale to large datasets. In [14], the authors showed that con-
volutional networks are not invariant to some sort of trans-
formations based on the experiments done on Pascal3D+
annotations. Recently, Tsai et al. [19] provided a software
to misclassify a given image in a speciÔ¨Åed class, without
necessarily Ô¨Ånding the smallest perturbation. Nguyen et al.
[13] generated synthetic unrecognizable images, which are
classiÔ¨Åed with high conÔ¨Ådence. The authors of [3] also
studied a related problem of Ô¨Ånding the minimal geomet-
rictransformation that fools image classiÔ¨Åers, and provided
quantitative measure of the robustness of classiÔ¨Åers to geo-
metric transformations. Closer to our work, the authors of
[4] introduced the ‚Äúfast gradient sign‚Äù method, which com-
putes the adversarial perturbations for a given classiÔ¨Åer very
efÔ¨Åciently. Despite its efÔ¨Åciency, this method provides only
a coarse approximation of the optimal perturbation vectors.
In fact, it performs a unique gradient step, which often leads
to sub-optimal solutions. Then in an attempt to build more
robust classiÔ¨Åers to adversarial perturbations, [5] introduced
a smoothness penalty in the training procedure that allows
to boost the robustness of the classiÔ¨Åer. Notably, the method
in [18] was applied in order to generate adversarial pertur-
bations. We should Ô¨Ånally mention that the phenomenon of
adversarial instability also led to theoretical work in [2] that
studied the problem of adversarial perturbations on some
families of classiÔ¨Åers, and provided upper bounds on the
robustness of these classiÔ¨Åers. A deeper understanding of
the phenomenon of adversarial instability for more complex
classiÔ¨Åers is however needed; the method proposed in this
work can be seen as a baseline to efÔ¨Åciently and accurately
generate adversarial perturbations in order to better under-
stand this phenomenon.
The rest of paper is organized as follows. In Section 2,
we introduce an efÔ¨Åcient algorithm to Ô¨Ånd adversarial per-
turbations in a binary classiÔ¨Åer. The extension to the mul-
ticlass problem is provided in Section 3. In Section 4, we
propose extensive experiments that conÔ¨Årm the accuracy of
our method and outline its beneÔ¨Åts in building more robust
classiÔ¨Åers.
2. DeepFool for binary classiÔ¨Åers
As a multiclass classiÔ¨Åer can be viewed as aggregation of
binary classiÔ¨Åers, we Ô¨Årst propose the algorithm for binary
classiÔ¨Åers. That is, we assume here ^k(x) = sign(f(x)),
wherefis an arbitrary scalar-valued image classiÔ¨Åcation
functionf:Rn!R. We also denote by F,fx:
f(x) = 0gthe level set at zero of f. We begin by analyzing
the case where fis an afÔ¨Åne classiÔ¨Åer f(x) =wTx+b,
and then derive the general algorithm, which can be applied
to any differentiable binary classiÔ¨Åer f.
In the case where the classiÔ¨Åer fis afÔ¨Åne, it can easilyFf(x)<0f(x)>0
r‚àó(x)‚àÜ(
x
0;f)
x0Figure 2: Adversarial examples for a linear binary classiÔ¨Åer.
be seen that the robustness of fat pointx0,(x0;f)2, is
equal to the distance from x0to the separating afÔ¨Åne hyper-
planeF=fx:wTx+b= 0g(Figure 2). The minimal
perturbation to change the classiÔ¨Åer‚Äôs decision corresponds
to the orthogonal projection of x0ontoF. It is given by
the closed-form formula:
r(x0) := arg minkrk2 (3)
subject to sign (f(x0+r))6=sign(f(x0))
= f(x0)
kwk2
2w:
Assuming now that fis a general binary differentiable clas-
siÔ¨Åer, we adopt an iterative procedure to estimate the robust-
ness(x0;f). SpeciÔ¨Åcally, at each iteration, fis linearized
around the current point xiand the minimal perturbation of
the linearized classiÔ¨Åer is computed as
arg min
rikrik2subject tof(xi) +rf(xi)Tri= 0:(4)
The perturbation riat iterationiof the algorithm is com-
puted using the closed form solution in Eq. (3), and the next
iteratexi+1is updated. The algorithm stops when xi+1
changes sign of the classiÔ¨Åer. The DeepFool algorithm for
binary classiÔ¨Åers is summarized in Algorithm 1 and a geo-
metric illustration of the method is shown in Figure 3.
In practice, the above algorithm can often converge to a
point on the zero level set F. In order to reach the other side
of the classiÔ¨Åcation boundary, the Ô¨Ånal perturbation vector
^ris multiplied by a constant 1 +, with1. In our
experiments, we have used = 0:02.
3. DeepFool for multiclass classiÔ¨Åers
We now extend the DeepFool method to the multiclass
case. The most common used scheme for multiclass clas-
siÔ¨Åers is one-vs-all. Hence, we also propose our method
2From now on, we refer to a classiÔ¨Åer either by for its correspond-
ing discrete mapping ^k. Therefore, adv(^k) =adv(f)and(x;^k) =
(x;f).Algorithm 1 DeepFool for binary classiÔ¨Åers
1:input: Imagex, classiÔ¨Åerf.
2:output: Perturbation ^r.
3:Initializex0 x,i 0.
4:while sign(f(xi)) = sign(f(x0))do
5:ri  f(xi)
krf(xi)k2
2rf(xi),
6:xi+1 xi+ri,
7:i i+ 1.
8:end while
9:return ^r=P
iri.
x0x1
F
Rn
Figure 3: Illustration of Algorithm 1 for n= 2 . As-
sumex02Rn. The green plane is the graph of x7!
f(x0)+rf(x0)T(x x0), which is tangent to the classiÔ¨Åer
function (wire-framed graph) x7!f(x). The orange line
indicates where f(x0) +rf(x0)T(x x0) = 0 .x1is ob-
tained fromx0by projectingx0on the orange hyperplane
ofRn.
based on this classiÔ¨Åcation scheme. In this scheme, the
classiÔ¨Åer has coutputs where cis the number of classes.
Therefore, a classiÔ¨Åer can be deÔ¨Åned as f:Rn!Rcand
the classiÔ¨Åcation is done by the following mapping:
^k(x) = arg max
kfk(x); (5)
wherefk(x)is the output of f(x)that corresponds to the
kthclass. Similarly to the binary case, we Ô¨Årst present the
proposed approach for the linear case and then we general-
ize it to other classiÔ¨Åers.
3.1. AfÔ¨Åne multiclass classiÔ¨Åer
Letf(x)be an afÔ¨Åne classiÔ¨Åer, i.e., f(x) =W>x+b
for a givenWandb. Since the mapping ^kis the outcome of
a one-vs-all classiÔ¨Åcation scheme, the minimal perturbation
to fool the classiÔ¨Åer can be rewritten as follows
arg min
rkrk2
s.t.9k:w>
k(x0+r) +bkw>
^k(x0)(x0+r) +b^k(x0);
(6)x0F1
F2F3Figure 4: For x0belonging to class 4, letFk=fx:
fk(x) f4(x) = 0g. These hyperplanes are depicted in
solid lines and the boundary of Pis shown in green dotted
line.
wherewkis thekthcolumn ofW. Geometrically, the above
problem corresponds to the computation of the distance be-
tweenx0and the complement of the convex polyhedron P,
P=c\
k=1fx:f^k(x0)(x)fk(x)g; (7)
wherex0is located inside P. We denote this distance by
dist(x0;Pc). The polyhedron PdeÔ¨Ånes the region of the
space where foutputs the label ^k(x0). This setting is de-
picted in Figure 4. The solution to the problem in Eq. (6)
can be computed in closed form as follows. DeÔ¨Åne ^l(x0)
to be the closest hyperplane of the boundary of P(e.g.
^l(x0) = 3 in Figure 4). Formally, ^l(x0)can be computed
as follows
^l(x0) = arg min
k6=^k(x0)fk(x0) f^k(x0)(x0)
kwk w^k(x0)k2: (8)
The minimum perturbation r(x0)is the vector that
projectsx0on the hyperplane indexed by ^l(x0), i.e.,
r(x0) =f^l(x0)(x0) f^k(x0)(x0)
kw^l(x0) w^k(x0)k2
2(w^l(x0) w^k(x0)):
(9)
In other words, we Ô¨Ånd the closest projection of x0on faces
ofP.
3.2. General classiÔ¨Åer
We now extend the DeepFool algorithm to the general
case of multiclass differentiable classiÔ¨Åers. For general
non-linear classiÔ¨Åers, the set Pin Eq. (7) that describes the
region of the space where the classiÔ¨Åer outputs label ^k(x0)
is no longer a polyhedron. Following the explained iterative
linearization procedure in the binary case, we approximate
x0F1
F2F3Figure 5: For x0belonging to class 4, letFk=fx:
fk(x) f4(x) = 0g. The linearized zero level sets are
shown in dashed lines and the boundary of the polyhedron
~P0in green.
the setPat iterationiby a polyhedron ~Pi
~Pi=c\
k=1n
x:fk(xi) f^k(x0)(xi) (10)
+rfk(xi)>x rf^k(x0)(xi)>x0o
:
We then approximate, at iteration i, the distance between
xiand the complement of P,dist(xi;Pc), bydist(xi;~Pc
i).
SpeciÔ¨Åcally, at each iteration of the algorithm, the perturba-
tion vector that reaches the boundary of the polyhedron ~Piis
computed, and the current estimate updated. The method is
given in Algorithm 2. It should be noted that the proposed
algorithm operates in a greedy way and is not guaranteed
to converge to the optimal perturbation in (1). However,
we have observed in practice that our algorithm yields very
small perturbations which are believed to be good approxi-
mations of the minimal perturbation.
It should be noted that the optimization strategy of Deep-
Fool is strongly tied to existing optimization techniques. In
the binary case, it can be seen as Newton‚Äôs iterative algo-
rithm for Ô¨Ånding roots of a nonlinear system of equations in
the underdetermined case [15]. This algorithm is known as
the normal Ô¨Çow method. The convergence analysis of this
optimization technique can be found for example in [21].
Our algorithm in the binary case can alternatively be seen
as a gradient descent algorithm with an adaptive step size
that is automatically chosen at each iteration. The lineariza-
tion in Algorithm 2 is also similar to a sequential convex
programming where the constraints are linearized at each
step.
3.3. Extension to `pnorm
In this paper, we have measured the perturbations using
the`2norm. Our framework is however not limited to this
choice, and the proposed algorithm can simply be adaptedAlgorithm 2 DeepFool: multi-class case
1:input: Imagex, classiÔ¨Åerf.
2:output: Perturbation ^r.
3:
4:Initializex0 x,i 0.
5:while ^k(xi) =^k(x0)do
6: fork6=^k(x0)do
7:w0
k rfk(xi) rf^k(x0)(xi)
8:f0
k fk(xi) f^k(x0)(xi)
9: end for
10: ^l arg mink6=^k(x0)jf0
kj
kw0
kk2
11:ri jf0
^lj
kw0
^lk2
2w0
^l
12:xi+1 xi+ri
13:i i+ 1
14:end while
15:return ^r=P
iri
to Ô¨Ånd minimal adversarial perturbations for any `pnorm
(p2[1;1)). To do so, the update steps in line 10 and
11 in Algorithm 2 must be respectively substituted by the
following updates
^l arg min
k6=^k(x0)jf0
kj
kw0
kkq; (11)
ri jf0
^lj
kw0
^lkq
qjw0
^ljq 1sign(w0
^l); (12)
whereis the pointwise product and q=p
p 1.3In par-
ticular, when p=1(i.e., the supremum norm `1), these
update steps become
^l arg min
k6=^k(x0)jf0
kj
kw0
kk1; (13)
ri jf0
^lj
kw0
^lk1sign(w0
^l): (14)
4. Experimental results
4.1. Setup
We now test our DeepFool algorithm on deep convo-
lutional neural networks architectures applied to MNIST,
CIFAR-10, and ImageNet image classiÔ¨Åcation datasets. We
consider the following deep neural network architectures:
MNIST: A two-layer fully connected network, and a
two-layer LeNet convoluational neural network archi-
tecture [9]. Both networks are trained with SGD with
momentum using the MatConvNet [20] package.
3To see this, one can apply Holder‚Äôs inequality to obtain a lower bound
on the`pnorm of the perturbation.CIFAR-10: We trained a three-layer LeNet architec-
ture, as well as a Network In Network (NIN) architec-
ture [11].
ILSVRC 2012: We used CaffeNet [7] and GoogLeNet
[17] pre-trained models.
In order to evaluate the robustness to adversarial pertur-
bations of a classiÔ¨Åer f, we compute the average robustness
^adv(f), deÔ¨Åned by
^adv(f) =1
jDjX
x2Dk^r(x)k2
kxk2; (15)
where ^r(x)is the estimated minimal perturbation obtained
using DeepFool, and Ddenotes the test set4.
We compare the proposed DeepFool approach to state-
of-the-art techniques to compute adversarial perturbations
in [18] and [4]. The method in [18] solves a series of pe-
nalized optimization problems to Ô¨Ånd the minimal pertur-
bation, whereas [4] estimates the minimal perturbation by
taking the sign of the gradient
^r(x) =sign(rxJ(;x;y));
withJthe cost used to train the neural network, is the
model parameters, and yis the label of x. The method is
called fast gradient sign method . In practice, in the absence
of general rules to choose the parameter , we chose the
smallestsuch that 90% of the data are misclassiÔ¨Åed after
perturbation.5
4.2. Results
We report in Table 1 the accuracy and average robustness
^advof each classiÔ¨Åer computed using different methods.
We also show the running time required for each method to
compute oneadversarial sample. It can be seen that Deep-
Fool estimates smaller perturbations (hence closer to min-
imal perturbation deÔ¨Åned in (1)) than the ones computed
using the competitive approaches. For example, the aver-
age perturbation obtained using DeepFool is 5times lower
than the one estimated with [4]. On the ILSVRC2012 chal-
lenge dataset, the average perturbation is one order of mag-
nitude smaller compared to the fast gradient method. It
should be noted moreover that the proposed approach also
yields slightly smaller perturbation vectors than the method
in [18]. The proposed approach is hence more accurate
in detecting directions that can potentially fool neural net-
works. As a result, DeepFool can be used as a valuable
tool to accurately assess the robustness of classiÔ¨Åers. On
4For ILSVRC2012, we used the validation data.
5Using this method, we observed empirically that one cannot reach
100% misclassiÔ¨Åcation rate on some datasets. In fact, even by increas-
ingto be very large, this method can fail in misclassifying all samples.ClassiÔ¨Åer Test error ^adv[DeepFool] time ^adv[4] time ^adv[18] time
LeNet (MNIST) 1% 2:010 1110 ms 1.0 20 ms 2:510 1>4 s
FC500-150-10 (MNIST) 1.7% 1:110 150 ms 3:910 110 ms 1:210 1>2 s
NIN (CIFAR-10) 11.5% 2:310 21100 ms 1:210 1180 ms 2:410 2>50 s
LeNet (CIFAR-10) 22.6% 3:010 2220 ms 1:310 150 ms 3:910 2>7 s
CaffeNet (ILSVRC2012) 42.6% 2:710 3510 ms\* 3:510 250 ms\* - -
GoogLeNet (ILSVRC2012) 31.3% 1:910 3800 ms\* 4:710 280 ms\* - -
Table 1: The adversarial robustness of different classiÔ¨Åers on different datasets. The time required to compute one sample
for each method is given in the time columns. The times are computed on a Mid-2015 MacBook Pro without CUDA support.
The asterisk marks determines the values computed using a GTX 750 Ti GPU.
the complexity aspect, the proposed approach is substan-
tially faster than the standard method proposed in [18]. In
fact, while the approach [18] involves a costly minimization
of a series of objective functions, we observed empirically
that DeepFool converges in a few iterations (i.e., less than
3) to a perturbation vector that fools the classiÔ¨Åer. Hence,
the proposed approach reaches a more accurate perturba-
tion vector compared to state-of-the-art methods, while be-
ing computationally efÔ¨Åcient. This makes it readily suitable
to be used as a baseline method to estimate the robustness
of very deep neural networks on large-scale datasets. In that
context, we provide the Ô¨Årst quantitative evaluation of the
robustness of state-of-the-art classiÔ¨Åers on the large-scale
ImageNet dataset. It can be seen that despite their very good
test accuracy, these methods are extremely unstable to ad-
versarial perturbations: a perturbation that is 1000 smaller
in magnitude than the original image is sufÔ¨Åcient to fool
state-of-the-art deep neural networks.
We illustrate in Figure 1 perturbed images generated by
the fast gradient sign and DeepFool. It can be observed
that the proposed method generates adversarial perturba-
tions which are hardly perceptible, while the fast gradient
sign method outputs a perturbation image with higher norm.
It should be noted that, when perturbations are mea-
sured using the `1norm, the above conclusions remain un-
changed: DeepFool yields adversarial perturbations that are
smaller (hence closer to the optimum) compared to other
methods for computing adversarial examples. Table 2 re-
ports the`1robustness to adversarial perturbations mea-
sured by ^1
adv(f) =1
jDjP
x2Dk^r(x)k1
kxk1, where ^r(x)is
computed respectively using DeepFool (with p=1, see
Section 3.3), and the Fast gradient sign method for MNIST
and CIFAR-10 tasks.
Fine-tuning using adversarial examples In this sec-
tion, we Ô¨Åne-tune the networks of Table 1 on adversarial
examples to build more robust classiÔ¨Åers for the MNISTClassiÔ¨Åer DeepFool Fast gradient sign
LeNet (MNIST) 0.10 0.26
FC500-150-10 (MNIST) 0.04 0.11
NIN (CIFAR-10) 0.008 0.024
LeNet (CIFAR-10) 0.015 0.028
Table 2: Values of ^1
advfor four different networks based on
DeepFool (smallest l1perturbation) and fast gradient sign
method with 90% of misclassiÔ¨Åcation.
and CIFAR-10 tasks. SpeciÔ¨Åcally, for each network, we
performed two experiments: (i) Fine-tuning the network on
DeepFool‚Äôs adversarial examples, (ii) Fine-tuning the net-
work on the fast gradient sign adversarial examples. We
Ô¨Åne-tune the networks by performing 5 additional epochs,
with a 50% decreased learning rate only on the perturbed
training set. For each experiment, the same training data
was used through all 5extra epochs. For the sake of com-
pleteness, we also performed 5extra epochs on the origi-
nal data. The evolution of ^advfor the different Ô¨Åne-tuning
strategies is shown in Figures 6a to 6d, where the robust-
ness^advis estimated using DeepFool , since this is the most
accurate method, as shown in Table 1. Observe that Ô¨Åne-
tuning with DeepFool adversarial examples signiÔ¨Åcantly in-
creases the robustness of the networks to adversarial pertur-
bations even after one extra epoch. For example, the ro-
bustness of the networks on MNIST is improved by 50%
and NIN‚Äôs robustness is increased by about 40%. On the
other hand, quite surprisingly, the method in [4] can lead
toa decreased robustness to adversarial perturbations of
the network. We hypothesize that this behavior is due to
the fact that perturbations estimated using the fast gradient
sign method are much larger than minimal adversarial per-0 1 2 3 4 5
Number of extra epochs0.120.140.160.180.20.220.240.260.28ÀÜœÅadv
DeepFool
Fast gradient sign
Clean(a) Effect of Ô¨Åne-tuning on adversarial examples com-
puted by two different methods for LeNet on MNIST.
0 1 2 3 4 5
Number of extra epochs0.060.070.080.090.10.110.120.130.140.150.16ÀÜœÅadv
DeepFool
Fast gradient sign
Clean(b) Effect of Ô¨Åne-tuning on adversarial examples com-
puted by two different methods for a fully-connected
network on MNIST.
0 1 2 3 4 5
Number of extra epochs0.0220.0240.0260.0280.030.0320.0340.0360.0380.040.042ÀÜœÅadv
DeepFool
Fast gradient sign
Clean
(c) Effect of Ô¨Åne-tuning on adversarial examples com-
puted by two different methods for NIN on CIFAR-10.
0 1 2 3 4 5
Number of extra epochs0.0250.030.0350.04ÀÜœÅadvDeepFool
Fast gradient sign
Clean(d) Effect of Ô¨Åne-tuning on adversarial examples com-
puted by two different methods for LeNet on CIFAR-10.
Figure 6
turbations. Fine-tuning the network with overly perturbed
images decreases the robustness of the networks to adver-
sarial perturbations. To verify this hypothesis, we com-
pare in Figure 7 the adversarial robustness of a network that
is Ô¨Åne-tuned with the adversarial examples obtained using
DeepFool, where norms of perturbations have been deliber-
ately multiplied by = 1;2;3. Interestingly, we see that
by magnifying the norms of the adversarial perturbations,
the robustness of the Ô¨Åne-tuned network is decreased . This
might explain why overly perturbed images decrease the ro-
bustness of MNIST networks: these perturbations can re-
ally change the class of the digits, hence Ô¨Åne-tuning based
on these examples can lead to a drop of the robustness (for
an illustration, see Figure 8). This lends credence to our
hypothesis, and further shows the importance of designing
accurate methods to compute minimal perturbations.
Table 3 lists the accuracies of the Ô¨Åne-tuned networks. It
can be seen that Ô¨Åne-tuning with DeepFool can improve the
accuracy of the networks. Conversely, Ô¨Åne-tuning with the
approach in [4] has led to a decrease of the test accuracy in
all our experiments. This conÔ¨Årms the explanation that the
fast gradient sign method outputs overly perturbed images
0 1 2 3 4 5
Number of extra epochs0.050.10.150.20.250.3ÀÜœÅadv
Œ± = 1
Œ± = 2
Œ± = 3Figure 7: Fine-tuning based on magniÔ¨Åed DeepFool‚Äôs ad-
versarial perturbations.
that lead to images that are unlikely to occur in the test data.
Hence, it decreases the performance of the method as it acts
as a regularizer that does not represent the distribution of
the original data. This effect is analogous to geometric data
augmentation schemes, where large transformations of the
original samples have a counter-productive effect on gener-Œ±=1 Œ±=2 Œ±=3 Œ±=4Figure 8: From ‚Äú1‚Äù to ‚Äú7‚Äù : original image classiÔ¨Åed as ‚Äú1‚Äù
and the DeepFool perturbed images classiÔ¨Åed as ‚Äú7‚Äù using
different values of .
ClassiÔ¨Åer DeepFool Fast gradient sign Clean
LeNet (MNIST) 0.8% 4.4% 1%
FC500-150-10 (MNIST) 1.5% 4.9% 1.7%
NIN (CIFAR-10) 11.2% 21.2% 11.5%
LeNet (CIFAR-10) 20.0% 28.6% 22.6%
Table 3: The test error of networks after the Ô¨Åne-tuning on
adversarial examples (after Ô¨Åve epochs). Each columns cor-
respond to a different type of augmented perturbation.
alization.6
To emphasize the importance of a correct estimation of
the minimal perturbation, we now show that using approxi-
mate methods can lead to wrong conclusions regarding the
adversarial robustness of networks. We Ô¨Åne-tune the NIN
classiÔ¨Åer on the fast gradient sign adversarial examples. We
follow the procedure described earlier but this time, we de-
creased the learning rate by 90%. We have evaluated the ad-
versarial robustness of this network at different extra epochs
using DeepFool and the fast gradient sign method . As one
can see in Figure 9, the red plot exaggerates the effect of
training on the adversarial examples. Moreover, it is not
sensitive enough to demonstrate the loss of robustness at the
Ô¨Årst extra epoch. These observations conÔ¨Årm that using an
accurate tool to measure the robustness of classiÔ¨Åers is cru-
cial to derive conclusions about the robustness of networks.
5. Conclusion
In this work, we proposed an algorithm, DeepFool, to
compute adversarial examples that fool state-of-the-art clas-
siÔ¨Åers. It is based on an iterative linearization of the clas-
siÔ¨Åer to generate minimal perturbations that are sufÔ¨Åcient
to change classiÔ¨Åcation labels. We provided extensive ex-
perimental evidence on three datasets and eight classiÔ¨Åers,
showing the superiority of the proposed method over state-
of-the-art methods to compute adversarial perturbations, as
well as the efÔ¨Åciency of the proposed approach. Due to
6While the authors of [4] reported an increased generalization perfor-
mance on the MNIST task (from 0:94% to0:84% ) using adversarial reg-
ularization, it should be noted that the their experimental setup is signiÔ¨Å-
cantly different as [4] trained the network based on a modiÔ¨Åed cost func-
tion, while we performed straightforward Ô¨Åne-tuning.
0 1 2 3 4 5
Number of extra epochs0.80.911.11.21.31.41.5Normalized robustnessDeepFool
Fast gradient signFigure 9: How the adversarial robustness is judged by dif-
ferent methods. The values are normalized by the corre-
sponding ^advs of the original network.
its accurate estimation of the adversarial perturbations, the
proposed DeepFool algorithm provides an efÔ¨Åcient and ac-
curate way to evaluate the robustness of classiÔ¨Åers and to
enhance their performance by proper Ô¨Åne-tuning. The pro-
posed approach can therefore be used as a reliable tool to
accurately estimate the minimal perturbation vectors, and
build more robust classiÔ¨Åers.
Acknowledgements
This work has been partly supported by the Hasler
Foundation, Switzerland, in the framework of the CORA
project.
References
[1] D. Chicco, P. Sadowski, and P. Baldi. Deep autoencoder
neural networks for gene ontology annotation predictions. In
ACM Conference on Bioinformatics, Computational Biology,
and Health Informatics , pages 533‚Äì540, 2014.
[2] A. Fawzi, O. Fawzi, and P. Frossard. Analysis of clas-
siÔ¨Åers‚Äô robustness to adversarial perturbations. CoRR ,
abs/1502.02590, 2015.
[3] A. Fawzi and P. Frossard. Manitest: Are classiÔ¨Åers really
invariant? In British Machine Vision Conference (BMVC) ,
pages 106.1‚Äì106.13, 2015.
[4] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and
harnessing adversarial examples. In International Confer-
ence on Learning Representations , 2015.
[5] S. Gu and L. Rigazio. Towards deep neural network architec-
tures robust to adversarial examples. CoRR , abs/1412.5068,
2014.
[6] G. E. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed,
N. Jaitly, A. Senior, V . Vanhoucke, P. Nguyen, T. N. Sainath,
and B. Kingsbury. Deep neural networks for acoustic model-
ing in speech recognition: The shared views of four research
groups. IEEE Signal Process. Mag. , 29(6):82‚Äì97, 2012.
[7] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. In ACM Inter-national Conference on Multimedia (MM) , pages 675‚Äì678.
ACM, 2014.
[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classiÔ¨Åcation with deep convolutional neural networks. In
Advances in neural information processing systems (NIPS) ,
pages 1097‚Äì1105, 2012.
[9] Y . LeCun, P. Haffner, L. Bottou, and Y . Bengio. Object
recognition with gradient-based learning. In Shape, contour
and grouping in computer vision , pages 319‚Äì345. 1999.
[10] Y . LeCun, K. Kavukcuoglu, C. Farabet, et al. Convolutional
networks and applications in vision. In IEEE International
Symposium on Circuits and Systems (ISCAS) , pages 253‚Äì
256, 2010.
[11] M. Lin, Q. Chen, and S. Yan. Network in network. 2014.
[12] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. ÀáCernock `y.
Strategies for training large scale neural network language
models. In IEEE Workshop on Automatic Speech Recogni-
tion and Understanding (ASRU) , pages 196‚Äì201, 2011.
[13] A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks
are easily fooled: High conÔ¨Ådence predictions for unrecog-
nizable images. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 427‚Äì436, 2015.
[14] B. Pepik, R. Benenson, T. Ritschel, and B. Schiele. What is
holding back convnets for detection? In Pattern Recognition ,
pages 517‚Äì528. Springer, 2015.
[15] A. P. Ruszczy ¬¥nski. Nonlinear optimization , volume 13.
Princeton university press, 2006.
[16] M. Spencer, J. Eickholt, and J. Cheng. A deep learning net-
work approach to ab initio protein secondary structure pre-
diction. IEEE/ACM Trans. Comput. Biol. Bioinformatics ,
12(1):103‚Äì112, 2015.
[17] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V . Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) , pages 1‚Äì
9, 2015.
[18] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. J. Goodfellow, and R. Fergus. Intriguing properties of
neural networks. In International Conference on Learning
Representations (ICLR) , 2014.
[19] C.-Y . Tsai and D. Cox. Are deep learning algorithms
easily hackable? http://coxlab.github.io/
ostrichinator .
[20] A. Vedaldi and K. Lenc. Matconvnet: Convolutional neural
networks for matlab. In ACM International Conference on
Multimedia (MM) , pages 689‚Äì692, 2015.
[21] H. F. Walker and L. T. Watson. Least-change secant update
methods for underdetermined systems. SIAM Journal on nu-
merical analysis , 27(5):1227‚Äì1262, 1990.