3/7/24, 3:58 PM LLM Prompt Injection: Direct | MITRE ATLAS™
https://atlas.mitre.org/techniques/AML.T0051.000/ 1/2Home Techniques LLM Prompt Injection Direct
LLM Prompt Injection: Direct
Summary󰅂 󰅂 󰅂
An adversary may inject prompts directly as a user of the
LLM. This type of injection may be used by the adversary to
gain a foothold in the system or to misuse the LLM itself, as
for example to generate harmful content.ID: AML.T0051.000
Case Study: Achieving Code
Execution in MathGPT via
Prompt Injection
Other subtechniques: LLM
Prompt Injection: Indirect
Parent Technique: LLM
Prompt Injection
Number of Tactics: 4
Case Study󰅀
Achieving Code Execution in MathGPT via Prompt Injection
Other Subtechniques󰅀
LLM Prompt Injection: Indirect
Parent Technique󰅀
LLM Prompt Injection󰍜 Matrices Navigator Tactics Techniques Mitigations Case Studies󰍝
This website utilizes technologies such as cookies to enable essential site functionality , as well as
for analytics, personalization, and targeted advertising purposes. To learn more, view the following
link: Privacy Policy
Manage Preferences3/7/24, 3:58 PM LLM Prompt Injection: Direct | MITRE ATLAS™
https://atlas.mitre.org/techniques/AML.T0051.000/ 2/2Tactics
󰅀
Initial Access
Persistence
Defense Evasion
Privilege Escalation
MITRE ATLAS™ and MITRE ATT&CK are a trademark and registered
trademark of The MITRE Corporation.®
PRIVACY POLICY TERMS OF USE MANAGE COOKIESCONTACT󰍜 Matrices Navigator Tactics Techniques Mitigations Case Studies󰍝
This website utilizes technologies such as cookies to enable essential site functionality , as well as
for analytics, personalization, and targeted advertising purposes. To learn more, view the following
link: Privacy Policy