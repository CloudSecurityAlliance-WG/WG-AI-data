3/22/24, 2:25 PM Backdoor ML Model: Poison ML Model | MITRE ATLAS™
https://atlas.mitre.org/techniques/AML.T0018.000/ 1/3Home Techniques Backdoor ML Model
Backdoor ML Model: Poison
ML Model󰅂 󰅂 󰅂
Poison ML Model 󰅂
Summary
Adversaries may introduce a backdoor by training the model poisoned data, or by
interfering with its training process. The model learns to associate an adversary-
defined trigger with the adversary's desired output.
ID: AML.T0018.000
Case Study: PoisonGPT
Mitigations: Control Access to ML Models and Data at Rest, Sanitize Training Data,
Validate ML Model
Other subtechniques: Backdoor ML Model: Inject Payload
Parent Technique: Backdoor ML Model
Tactics: ML Attack Staging, Persistence
Case Study 󰅃
PoisonGPT
Mitigations 󰅃󰍜 󰇙3/22/24, 2:25 PM Backdoor ML Model: Poison ML Model | MITRE ATLAS™
https://atlas.mitre.org/techniques/AML.T0018.000/ 2/3Control Access to ML Models and Data at Rest
Sanitize Training Data
Validate ML Model
Other Subtechniques 󰅃
Backdoor ML Model: Inject Payload
Parent Technique󰅃
Backdoor ML Model
Tactics󰅃
ML Attack Staging
Persistence󰍜 󰇙3/22/24, 2:25 PM Backdoor ML Model: Poison ML Model | MITRE ATLAS™
https://atlas.mitre.org/techniques/AML.T0018.000/ 3/3Join our collaborative community
to shape future tool and
framework developments in AI
security , threat mitigation, bias,
privacy and other critical aspects
of AI assurance.www .mitre.org
© 2021-2024 The MITRE Corporation. All Rights Reserved.
Approved for Public Release; Distribution Unlimited. Case Number 21-2363.
MITRE ATLAS™ and MITRE ATT&CK are a trademark and registered trademark of The MITRE
Corporation.
Privacy Policy | Terms of Use | Manage CookiesCONNECT WITH US
󰅂
®󰍜 󰇙