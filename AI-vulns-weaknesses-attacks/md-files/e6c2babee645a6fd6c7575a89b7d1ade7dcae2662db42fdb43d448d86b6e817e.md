High Accuracy and High Fidelity Extraction of Neural Networks
Matthew Jagielski‚Ä†;, Nicholas Carlini\*, David Berthelot\*, Alex Kurakin\*, and Nicolas Papernot\*
‚Ä†Northeastern University
\*Google Research
Abstract
In a model extraction attack, an adversary steals a copy of
a remotely deployed machine learning model, given oracle
prediction access. We taxonomize model extraction attacks
around two objectives: accuracy , i.e., performing well on
the underlying learning task, and Ô¨Ådelity , i.e., matching the
predictions of the remote victim classiÔ¨Åer on any input.
To extract a high-accuracy model, we develop a learning-
based attack exploiting the victim to supervise the train-
ing of an extracted model. Through analytical and empiri-
cal arguments, we then explain the inherent limitations that
prevent any learning-based strategy from extracting a truly
high-Ô¨Ådelity model‚Äîi.e., extracting a functionally-equivalent
model whose predictions are identical to those of the vic-
tim model on all possible inputs. Addressing these limita-
tions, we expand on prior work to develop the Ô¨Årst practical
functionally-equivalent extraction attack for direct extraction
(i.e., without training) of a model‚Äôs weights.
We perform experiments both on academic datasets and a
state-of-the-art image classiÔ¨Åer trained with 1 billion propri-
etary images. In addition to broadening the scope of model
extraction research, our work demonstrates the practicality of
model extraction attacks against production-grade systems.
1 Introduction
Machine learning, and neural networks in particular, are
widely deployed in industry settings. Models are often de-
ployed as prediction services or otherwise exposed to potential
adversaries. Despite this fact, the trained models themselves
are often proprietary and are closely guarded.
There are two reasons models are often seen as sensitive.
First, they are expensive to obtain. Not only is it expensive to
train the Ô¨Ånal model [1] (e.g., Google recently trained a model
with 340 million parameters on hardware costing 61,000 USD
per training run [2]), performing the work to identify the
optimal set of model architecture, training algorithm, and
hyper-parameters often eclipses the cost of training the Ô¨Ånalmodel. Further, training these models also requires investing
in expensive collection process to obtain the training datasets
necessary to obtain an accurate classiÔ¨Åer [3 ‚Äì6]. Second, there
are security [7, 8] and privacy [9, 10] concerns for revealing
trained models to potential adversaries.
Concerningly, prior work found that an adversary with
query access to a model can steal the model to obtain a copy
that largely agrees with the remote victim models [8, 11 ‚Äì16].
These extraction attacks are therefore important to consider.
In this paper, we systematize the space of model extrac-
tion around two adversarial objectives: accuracy andÔ¨Ådelity .
Accuracy measures the correctness of predictions made by
the extracted model on the test distribution. Fidelity, in con-
trast, measures the general agreement between the extracted
and victim models on any input. Both of these objectives are
desirable, but they are in conÔ¨Çict for imperfect victim mod-
els: a high-Ô¨Ådelity extraction should replicate the errors of
the victim, whereas a high-accuracy model should instead
try to make an accurate prediction. At the high-Ô¨Ådelity limit
isfunctionally-equivalent model extraction: the two models
agree on all inputs, both on and off the underlying data distri-
bution.
While most prior work considers accuracy [7, 11, 13], we
argue that Ô¨Ådelity is often equally important. When using
model extraction to mount black-box adversarial example at-
tacks [7], Ô¨Ådelity ensures the attack is more effective because
more adversarial examples transfer from the extracted model
to the victim. Membership inference [9, 10] beneÔ¨Åts from the
extracted model closely replicating the conÔ¨Ådence of predic-
tions made by the victim. Finally, a functionally-equivalent
extraction enables the adversary to inspect whether internal
representations reveal unintended attributes of the input‚Äîthat
are statistically uncorrelated with the training objective, en-
abling the adversary to beneÔ¨Åt from overlearning [17].
We design one attack for each objective. First, a learning-
based attack , which uses the victim to generate labels for
training the extracted model. While existing techniques al-
ready achieve high accuracy, our attacks are 16more query-
efÔ¨Åcient and scale to larger models. We perform experimentsarXiv:1909.01838v2 [cs.LG] 3 Mar 2020that surface inherent limitations of learning-based extraction
attacks and argue that learning-based strategies are ill-suited
to achieve high-Ô¨Ådelity extraction. Then, we develop the Ô¨Årst
practical functionally-equivalent attack , which directly recov-
ers a two-layer neural network‚Äôs weights exactly given access
to double-precision model inference. Compared to prior work,
which required a high-precision power side-channel [18] or
access to model gradients [19], our attack only requires input-
output access to the model, while simultaneously scaling to
larger networks than either of the prior methods.
We make the following contributions:
We taxonomize the space of model extraction attacks by
exploring the objective of accuracy andÔ¨Ådelity .
We improve the query efÔ¨Åciency of learning attacks for
accuracy extraction and make them practical for millions-
of-parameter models trained on billions of images.
We achieve high-Ô¨Ådelity extraction by developing the
Ô¨Årst practical functionally-equivalent model extraction.
We mix the proposed methods to obtain a hybrid method
which improves both accuracy and Ô¨Ådelity extraction.
2 Preliminaries
We consider classiÔ¨Åers with domain XRdand range Y
RK; the output of the classiÔ¨Åer is a distribution over Kclass
labels. The class assigned to an input xby a classiÔ¨Åer fis
argmaxi2[K]f(x)i(forn2Z, we write [n] =f1;2;:::ng). In
order to satisfy the constraint that a classiÔ¨Åer‚Äôs output is a
distribution, a softmax s()is typically applied to the output
of an arbitrary function fL:X!RK:
s(fL(x))i=exp(fL(x)i)
√•jexp(fL(x)j):
We call the function fL()thelogit function for a classiÔ¨Åer f.
To convert a class label into a probability vector, it is common
to use one-hot encoding : for a value j2[K], the one-hot
encoding OH(j;K)is a vector in RKwith OH(j;K)i= 1(i=
j)‚Äîthat is, it is 1 only at index j, and 0 elsewhere.
Model extraction concerns reproducing a victim model, or
oracle, which we write O:X!Y. The model extraction ad-
versary will run an extraction algorithm A(O), which outputs
the extracted model ÀÜO. We will sometimes parameterize the
oracle (resp. extracted model) as Oq(resp. ÀÜOq) to denote that it
has model parameters q‚Äîwe will omit this when unnecessary
or apparent from context.
In this work, we consider Oand ÀÜOto both be neural
networks. A neural network is a sequence of operations‚Äî
alternatingly applying linear operations and non-linear
operations‚Äîa pair of linear and non-linear operations is called
alayer . Each linear operation projects onto some space Rh‚Äî
the dimensionality hof this space is referred to as the widthof the layer. The number of layers is the depth of the net-
work. The non-linear operations are typically Ô¨Åxed, while the
linear operations have parameters which are learned during
training. The function computed by layer i,fi(a), is there-
fore computed as fi(a) =gi(A(i)a+B(i)), where giis the ith
non-linear function, and A(i);B(i)are the parameters of layer
i(A(i)is the weights, B(i)the biases). A common choice of
activation is the rectiÔ¨Åed linear unit, or ReLU, which sets
ReLU (x) =max(0;x). Introduced to improve the conver-
gence of optimization when training neural networks, the
ReLU activation has established itself as an effective default
choice for practitioners [20]. Thus, we consider primarily
ReLU networks in this work.
The network structure described here is called fully con-
nected because each linear operation ‚Äúconnects" every input
node to every output node. In many domains, such as com-
puter vision, this is more structure than necessary. A neuron
computing edge detection, for example, only needs to use
information from a small region of the image. Convolutional
networks were developed to combat this inefÔ¨Åciency‚Äîthe
linear functions become Ô¨Ålters, which are still linear, but are
only applied to a small (e.g., 3x3 or 5x5) window of the input.
They are applied to every window using the same weights,
making convolutions require far fewer parameters than fully
connected networks.
Neural networks are trained by empirical risk minimiza-
tion. Given a dataset of nsamples D=fxi;yign
i=1XY,
training involves minimizing a loss function Lon the dataset
with respect to the parameters of the network f. A common
loss function is the cross-entropy loss Hfor a sample (x;y):
H(y;f(x)) = √•k2[K]yklog(f(x)k), where yis the probabil-
ity (or one-hot) vector for the true class. The cross-entropy
loss on the full dataset is then
L(D;f) =1
nn
√•
i=1H(yi;f(xi)) = 1
nn
√•
i=1√•
k2[K]yklog(f(x)k):
The loss is minimized with some form of gradient descent,
often stochastic gradient descent (SGD). In SGD, gradients
of parameters qare computed over a randomly sampled batch
B, averaged, and scaled by a learning rate h:
qt+1=qt h
jBj√•
i2B√ëqH(yi;f(xi)):
Other optimizers [21 ‚Äì23] use gradient statistics to reduce the
variance of updates which can result in better performance.
A less common setting, but one which is important for our
work, is when the target values ywhich are used to train the
network are not one-hot values, but are probability vectors
output by a different model g(x). When training using the
dataset Dg=fxi;g(xi)1=Tgn
i=1, we say the trained model is
distilled from gwith temperature T, referring to the process
of distillation introduced in Hinton et al. [24]. Note that the
values of g(xi)1=Tare always scaled to sum to 1.3 Taxonomy of Threat Models
We now address the spectrum of adversaries interested in ex-
tracting neural networks. As illustrated in Table 1, we taxono-
mize the space of possible adversaries around two overarching
goals‚Äî theft andreconnaissance . We detail why extraction is
not always practically realizable by constructing models that
are impossible to extract, or require a large number of queries
to extract. We conclude our threat model with a discussion of
how adversarial capabilities (e.g., prior knowledge of model
architecture or information returned by queries) affect the
strategies an adversary may consider.
3.1 Adversarial Motivations
Model extraction attacks target the conÔ¨Ådentiality of a victim
model deployed on a remote service. A model refers here to
both the architecture and its parameters. Architectural details
include the learning hypothesis (i.e., neural network in our
case) and corresponding details (e.g., number of layers and
activation functions for neural networks). Parameter values
are the result of training.
First, we consider theft adversaries, motivated by economic
incentives. Generally, the defender went through an expensive
process to design the model‚Äôs architecture and train it to set
parameter values. Here, the model can be viewed as intellec-
tual property that the adversary is trying to steal. A line of
work has in fact referred to this as ‚Äúmodel stealing‚Äù [11].
In the latter class of attacks, the adversary is performing
reconnaissance to later mount attacks targeting other security
properties of the learning system: e.g., its integrity with adver-
sarial examples [7], or privacy with training data membership
inference [9,10]. Model extraction enables an adversary previ-
ously operating in a black-box threat model to mount attacks
against the extracted model in a white-box threat model. The
adversary has‚Äîby design‚Äîaccess to the extracted model‚Äôs
parameters. In the limit, this adversary would expect to extract
anexact copy of the oracle.
The goal of exact extraction is to produce ÀÜOq=Oq, so
that the model‚Äôs architecture and all of its weights are identi-
cal to the oracle. This deÔ¨Ånition is purely a strawman‚Äîit is
the strongest possible attack, but it is fundamentally impos-
sible for many classes of neural networks, including ReLU
networks, because any individual model belongs to a large
equivalence class of networks which are indistinguishable
from input-output behavior. For example, we can scale an
arbitrary neuron‚Äôs input weights and biases by some c>0,
and scale its output weights and biases by c 1; the resulting
model‚Äôs behavior is unchanged. Alternatively, in any inter-
mediate layer of a ReLU network, we may also add a dead
neuron which never contributes to the output, or might per-
mute the (arbitrary) order of neurons internally. Given access
to input-output behavior, the best we can do is identify the
equivalence class the oracle belongs to.
Figure 1: Illustrating Ô¨Ådelity vs. accuracy. The solid blue
line is the oracle; functionally equivalent extraction recovers
this exactly. The green dash-dot line achieves high Ô¨Ådelity: it
matches the oracle on all data points. The orange dashed line
achieves perfect accuracy: it classiÔ¨Åes all points correctly.
3.2 Adversarial Goals
This perspective yields a natural spectrum of realistic adver-
sarial goals characterizing decreasingly precise extractions.
Functionally Equivalent Extraction The goal of function-
ally equivalent extraction is to construct an ÀÜOsuch that
8x2X,ÀÜO(x) =O(x). This is a tractable weakening of the
exact extraction deÔ¨Ånition from earlier‚Äîit is the hardest possi-
ble goal using only input-output pairs. The adversary obtains
a member of the oracle‚Äôs equivalence class. This goal enables
a number of downstream attacks, including those involving
inspection of the model‚Äôs internal representations like over-
learning [17], to operate in the white-box threat model.
Fidelity Extraction Given some target distribution DF
overX, and goal similarity function S(p1;p2), the goal
of Ô¨Ådelity extraction is to construct an ÀÜOthat maxi-
mizes PrxDF
S(ÀÜO(x);O(x))
. In this work, we consider
only label agreement , where S(p1;p2) = 1(argmax (p1) =
argmax (p2)); we leave exploration of other similarity func-
tions to future work.
A natural distribution of interest DFis the data distribution
itself‚Äîthe adversary wants to make sure the mistakes and
correct labels are the same between the two models. A recon-
naissance attack for constructing adversarial examples would
care about a perturbed data distribution; mistakes might be
more important to the adversary in this setting. Membership
inference would use the natural data distribution, including
any outliers. These distributions tend to be concentrated on
a low-dimension manifold of X, making Ô¨Ådelity extraction
signiÔ¨Åcantly easier than functionally equivalent extraction.Attack Type Model type Goal Query Output
Lowd & Meek [8] Direct Recovery LM Functionally Equivalent Labels
Tramer et al. [11] (Active) Learning LM, NN Task Accuracy, Fidelity Probabilities, labels
Tramer et al. [11] Path Ô¨Ånding DT Functionally Equivalent Probabilities, labels
Milli et al. [19] (theoretical) Direct Recovery NN (2 layer) Functionally Equivalent Gradients, logits
Milli et al. [19] Learning LM, NN Task Accuracy Gradients
Palet al. [15] Active learning NN Fidelity Probabilities, labels
Chandrasekharan et al. [13] Active learning LM Functionally Equivalent Labels
Copycat CNN [16] Learning CNN Task Accuracy, Fidelity Labels
Papernot et al. [7] Active learning NN Fidelity Labels
CSI NN [25] Direct Recovery NN Functionally Equivalent Power Side Channel
Knockoff Nets [12] Learning NN Task Accuracy Probabilities
Functionally equivalent (this work) Direct Recovery NN (2 layer) Functionally Equivalent Probabilities, logits
EfÔ¨Åcient learning (this work) Learning NN Task Accuracy, Fidelity Probabilities
Table 1: Existing Model Extraction Attacks. Model types are abbreviated: LM = Linear Model, NN = Neural Network, DT =
Decision Tree, CNN = Convolutional Neural Network.
Indeed, functionally equivalent extraction achieves a perfect
Ô¨Ådelity of 1 on all distributions andall similarity functions .
Task Accuracy Extraction For the true task distribution
DAoverXY, the goal of task accuracy extraction is to
construct an ÀÜOmaximizing Pr(x;y)DA
argmax (ÀÜO(x)) = y
.
This goal is to match (or exceed) the accuracy of the target
model, which is the easiest goal to consider in this taxonomy
(because it doesn‚Äôt need to match the mistakes of O).
Existing Attacks In Table 1, we Ô¨Åt previous model extrac-
tion work into this taxonomy, as well as discuss their tech-
niques. Functionally equivalent extraction has been consid-
ered for linear models [8, 13], decision trees [11], both given
probabilities, and neural networks [19, 25], given extra ac-
cess. Task accuracy extraction has been considered for linear
models [11] and neural networks [12, 16, 19], and Ô¨Ådelity ex-
traction has also been considered for linear models [11] and
neural networks [7, 15]. Notably, functionally equivalent at-
tacks require model-speciÔ¨Åc techniques, while task accuracy
and Ô¨Ådelity typically use generic learning-based approaches.
3.3 Model Extraction is Hard
Before we consider adversarial capabilities in Section 3.4 and
potential corresponding approaches to model extraction, we
must understand how successful we can hope to be. Here,
we present arguments that will serve to bound our expecta-
tions. First, we will identify some limitations of functionally
equivalent extraction by constructing networks which require
arbitrarily many queries to extract. Second, we will present
another class of networks that cannot be extracted with Ô¨Å-
delity without querying a number of times exponential in its
depth. We provide intuition in this section and later prove
these statements in Appendix A.
Exponential hardness of functionally equivalent at-
tacks. In order to show that functionally equivalent extraction
is intractable in the worst case, we construct of a class ofneural networks that are hard to extract without making expo-
nentially many queries in the network‚Äôs width.
Theorem 1. There exists a class of width 3kand depth 2
neural networks on domain [0;1]d(with precision pnumbers)
with dkthat require, given logit access to the networks,
Q(pk)queries to extract.
The precision pis the number of possible values a feature
can take from [0;1]. In images with 8-bit pixels, we have
p=256. The intuition for this theorem is that a width 3k
network can implement a function that returns a non-zero
value on at most a p kfraction of the space. In the worst case,
pkqueries are necessary to Ô¨Ånd this fraction of the space.
Note that this result assumes the adversary can only observe
the input-output behavior of the oracle. If this assumption is
broken then functionally equivalent extraction becomes prac-
tical. For example, Batina et al. [25] perform functionally
equivalent extraction by performing a side channel attack
(speciÔ¨Åcally, differential power analysis [26]) on a micropro-
cessor evaluating the neural network.
We also observe in Theorem 2 that, given white-box access
to two neural networks, it is NP-hard in general to test if they
are functionally equivalent. We do this by constructing two
networks that differ only in coordinates satisfying a subset
sum instance. Then testing functional equivalence for these
networks is as hard as Ô¨Ånding the satisfying subset.
Theorem 2 (Informal) .Given their weights, it is NP-hard to
test whether two neural networks are functionally equivalent.
Any attack which can claim to perform functionally equiv-
alent extraction efÔ¨Åciently (both in number of queries used
and in running time) must make some assumptions to avoid
these pathologies. In Section 6, we will present and discuss
the assumptions of a functionally equivalent extraction attack
for two-layer neural network models.Learning approaches struggle with Ô¨Ådelity. A Ô¨Ånal difÔ¨Å-
culty for model extraction comes from recent work in learn-
ability [27]. Das et al. prove that, for deep random networks
with input dimension dand depth h, model extraction ap-
proaches that can be written as Statistical Query (SQ) learning
algorithms require exp (O(h))samples for Ô¨Ådelity extraction.
SQ algorithms are a restricted form of learning algorithm
which only access the data with noisy aggregate statistics;
many learning algorithms, such as (stochastic) gradient de-
scent and PCA, are examples. As a result, most learning-based
approaches to model extraction will inherit this inefÔ¨Åciency.
A sample-efÔ¨Åcient approach therefore must either make as-
sumptions about the model to be extracted (to distinguish
it from a deep random network), or must access its dataset
without statistical queries.
Theorem 3 (Informal [27]) .Random networks with domain
f0;1gdand rangef0;1gand depth hrequire exp(O(h))sam-
ples to learn in the SQ learning model.
3.4 Adversarial Capabilities
We organize an adversary‚Äôs prior knowledge about the oracle
and its training data into three categories‚Äî domain knowledge ,
deployment knowledge , and model access .
3.4.1 Domain Knowledge
Domain knowledge describes what the adversary knows about
the task the model is designed for. For example, if the model is
an image classiÔ¨Åer, then the model output should not change
under standard image data augmentations, such as shifts, ro-
tations, or crops. Usually, the adversary should be assumed to
have as much domain knowledge as the oracle‚Äôs designer.
In some domains, it is reasonable to assume the adver-
sary has access to public task-relevant pretrained models or
datasets. This is often the case for learning-based model ex-
traction, which we develop in Section 4. We consider an ad-
versary using part of a public dataset of 1.3 million images [4]
as unlabeled data to mount an attack against a model trained
on a proprietary dataset of 1 billion labeled images [28].
Learning-based extraction is hard without natural data
In learning-based extraction, we assume that the adversary
is able to collect public unlabeled data to mount their attack.
This is a natural assumption for a theft-motivated adversary
who wishes to steal the oracle for local use‚Äîthe adversary
has data they want to learn the labels of without querying the
model! For other adversaries, progress in generative modeling
is likely to offer ways to remove this assumption [29]. We
leave this to future work because our overarching aim in
this paper is to characterize the model extraction attacker
space around the notions of accuracy and Ô¨Ådelity. All progress
achieved by our approaches is complementary to possible
progress in synthetic data generation.3.4.2 Deployment Knowledge
Deployment knowledge describes what the adversary knows
about the oracle itself, including the model architecture, train-
ing procedure, and training dataset. The adversary may have
access to public artifacts of the oracle‚Äîa distilled version of
the oracle may be available (such as for OpenAI GPT [30])
or the oracle may be transfer learned from a public pretrained
model (such as many image classiÔ¨Åers [31] or language mod-
els like BERT [32]).
In addition, the adversary may not even know the features
(the exact inputs to the model) or the labels (the classes the
model may output). While the latter can generally be inferred
by interacting with the model (e.g., making queries and ob-
serving the labels predicted by the model), inferring the for-
mer is usually more difÔ¨Åcult. Our preliminary investigations
suggest that these are not limiting assumptions, but we leave
proper treatment of these constraints to future work.
3.4.3 Model Access
Model access describes the information the adversary obtains
from the oracle, including bounds on how many queries the
adversary may make as well as the oracle‚Äôs response:
label : only the label of the most-likely class is revealed.
label and score : in addition to the most-likely label, the
conÔ¨Ådence score of the model in its prediction for this
label is revealed.
top-kscores : the labels and conÔ¨Ådence scores for the k
classes whose conÔ¨Ådence are highest are revealed.
scores : conÔ¨Ådence scores for all labels are revealed.
logits : raw logit values for all labels are revealed.
In general, the more access an adversary is given, the more
effective they should be in accomplishing their goal. We in-
stantiate practical attacks under several of these assumptions.
Limiting model access has also been discussed as a defensive
measure, as we elaborate in Section 8.
4 Learning-based Model Extraction
We present our Ô¨Årst attack strategy where the victim model
serves as a labeling oracle for the adversary. While many
attack variants exist [7, 11], they generally stage an iterative
interaction between the adversary and the oracle, where the
adversary collects labels for a set of points from the oracle
and uses them as a training set for the extracted model. These
algorithms are typically designed for accuracy extraction; in
this section, we will demonstrate improved algorithms for
accuracy extraction, using task-relevant unlabeled data.
We realistically simulate large-scale model extraction by
considering an oracle that was trained on 1 billion Instagramimages [28] to obtain (at the time of the experiment) state-
of-the-art performance on the standard image classiÔ¨Åcation
benchmark, ImageNet [4]. The oracle, with 193 million pa-
rameters, obtained 84.2% top-1 accuracy and 97.2% top-5
accuracy on the 1000-class benchmark‚Äîwe refer to the model
as the "WSL model", abbreviating the paper title. We give
the adversary access to the public ImageNet dataset. The ad-
versary‚Äôs goal is to use the WSL model as a labeling oracle
to train an ImageNet classiÔ¨Åer that performs better than if
we trained the model directly on ImageNet. The attack is
successful if access to the WSL model‚Äîtrained on 1 billion
proprietary images inaccessible to the adversary‚Äîenables
the adversary to extract a model that outperforms a baseline
model trained directly with ImageNet labels. This is accu-
racy extraction for the ImageNet distribution, given unlabeled
ImageNet training data.
We consider two variants of the attack: one where the adver-
sary selects 10% of the training set (i.e., about 130,000 points)
and the other where the adversary keeps the entire training set
(i.e., about 1.3 million points). To put this number in perspec-
tive, recall that each image has a dimension of 224x224 pixels
and 3 color channels, giving us 2242243=150;528total
input features. Each image belongs to one of 1,000 classes.
Although ImageNet data is labeled, we always treat it as unla-
beled to simulate a realistic adversary.
4.1 Fully-supervised model extraction
The Ô¨Årst attack is fully supervised, as proposed by prior
work [11]. It serves to compare our subsequent attacks to
prior work, and to validate our hypothesis that labels from the
oracle are more informative than dataset labels.
The adversary needs to obtain a label for each of the points
it intends to train the extracted model with. Then it queries the
oracle to label its training points with the oracle‚Äôs predictions.
The oracle reveals labels and scores (in the threat model from
Section 3) when queried.
The adversary then trains its model to match these labels
using the cross-entropy loss. We used a distillation tempera-
ture of T=1:5in our experiments after a random search. Our
experiments use two architectures known to perform well on
image classiÔ¨Åcation: ResNet-v2-50 and ResNet-v2-200.
Results. We present results in Table 2. For instance, the adver-
sary is able to improve the accuracy of their model by 1:0%
for ResNetv2-50 and 1:9%for ResNet\_v2\_200 after having
queried the oracle for 10% of the ImageNet data. Recall that
the task has 1,000 labels, making these improvements signiÔ¨Å-
cant. The gains we are able to achieve as an adversary are in
line with progress that has been made by the computer vision
community on the ImageNet benchmark over recent years,
where the research community improved the state-of-the-art
top-1 accuracy by about one percent point per year.1
1https://paperswithcode.com/sota/image-classiÔ¨Åcation-on-imagenet4.2 Unlabeled data improves query efÔ¨Åciency
For adversaries interested in theft, a learning-based strategy
should minimize the number of queries required to achieve a
given level of accuracy. A natural approach towards this end is
to take advantage of advances in label-efÔ¨Åcient ML, including
active learning [33] and semi-supervised learning [34].
Active learning allows a learner to query the labels of ar-
bitrary points‚Äîthe goal is to query the best set of points
to learn a model with. Semi-supervised learning considers
a learner with some labeled data, but much more unlabeled
data‚Äîthe learner seeks to leverage the unlabeled data (for
example, by training on guessed labels) to improve classiÔ¨Å-
cation performance. Active and semi-supervised learning are
complementary techniques [35, 36]; it is possible to pick the
best subset of data to train on, while also using the rest of the
unlabeled data without labels.
The connection between label-efÔ¨Åcient learning and
learning-based model extraction attacks is not new [11,13,15],
but has focused on active learning. We show that, assuming
access to unlabeled task-speciÔ¨Åc data, semi-supervised learn-
ing can be used to improve model extraction attacks. This
could potentially be improved further by leveraging active
learning, as in prior work, but our improvements are overall
complementary to approaches considered in prior work. We
explore two semi-supervised learning techniques: rotation
loss [37] and MixMatch [38].
Rotation loss. We leverage the current state-of-the-art semi-
supervised learning approach on ImageNet, which aug-
ments the model with a rotation loss [37]. The model
contains two linear classiÔ¨Åers from the second-to-last
layer of the model: the classiÔ¨Åer for the image classiÔ¨Å-
cation task, and a rotation predictor. The goal of the ro-
tation classiÔ¨Åer is to predict the rotation applied to an
input‚Äîeach input is fed in four times per batch, rotated
byf0;90;180;270g. The classiÔ¨Åer should output one-
hot encodingsfOH(0;4);OH(1;4);OH(2;4);OH(3;4)g, re-
spectively, for these rotated images. Then, the rotation loss is
written:
LR(X;fq) =1
4NN
√•
i=0r
√•
j=1H(fq(Rj(xi));j)
where Rjis the jth rotation, His cross-entropy loss, and fqis
the model‚Äôs probability outputs for the rotation task. Inputs
need not be labeled, hence we compute this loss on unlabeled
data for which the adversary did not query the model. That
is, we train the model on both unlabeled data (with rotation
loss), and labeled data (with standard classiÔ¨Åcation loss), and
both contribute towards learning a good representation for all
of the data, including the unlabeled data.
We compare the accuracy of models trained with the rota-
tion loss on data labeled by the oracle and data with ImageNet
labels. Our best performing extracted model, with an accuracyArchitecture Data Fraction ImageNet WSL WSL-5 ImageNet + Rot WSL + Rot WSL-5 + Rot
Resnet\_v2\_50 10% (81.86/82.95) (82.71/84.18) (82.97/84.52) (82.27/84.14) (82.76/84.73) (82.84/84.59)
Resnet\_v2\_200 10% (83.50/84.96) (84.81/86.36) (85.00/86.67) (85.10/86.29) (86.17/88.16) (86.11/87.54)
Resnet\_v2\_50 100% (92.45/93.93) (93.00/94.64) (93.12/94.87) N/A N/A N/A
Resnet\_v2\_200 100% (93.70/95.11) (94.26/96.24) (94.21/95.85) N/A N/A N/A
Table 2: Extraction attack (top-5 accuracy/top-5 Ô¨Ådelity) of the WSL model [28]. Each row contains an architecture and fraction
of public ImageNet data used by the adversary. ImageNet is a baseline using only ImageNet labels. WSL is an oracle returning
WSL model probabilities. WSL-5 is an oracle returning only the top 5 probabilities. Columns with (+ Rot) use rotation loss
on unlabeled data (rotation loss was not run when all data is labeled). An adversary able to query WSL always improves over
ImageNet labels, even when given only top 5 probabilities. Rotation loss does not signiÔ¨Åcantly improve the performance on
ResNet\_v2\_50, but provides a (1.36/1.80) improvement for ResNet\_v2\_200, comparable to the performance boost given by
WSL labels on 10% data. In the high-data regime, where we observe a (0.56/1.13) improvement using WSL labels.
Dataset Algorithm 250 Queries 1000 Queries 4000 Queries
SVHN FS (79.25/79.48) (89.47/89.87) (94.25/94.71)
SVHN MM (95.82/96.38) (96.87/97.45) (97.07/97.61)
CIFAR10 FS (53.35/53.61) (73.47/73.96) (86.51/87.37)
CIFAR10 MM (87.98/88.79) (90.63/91.39) (93.29/93.99)
Table 3: Performance (accuracy/Ô¨Ådelity) of fully supervised
(FS) and MixMatch (MM) extraction on SVHN and CIFAR10.
MixMatch with 4000 labels performs nearly as well as the
oracle for both datasets, and MixMatch at 250 queries beats
fully supervised training at 4000 queries for both datasets.
of64:5%, is trained with the rotation loss on oracle labels
whereas the baseline on ImageNet labels only achieves 62:5%
accuracy with the rotation loss and 61:2%without the rotation
loss. This demonstrates the cumulative beneÔ¨Åt of adding a
rotation loss to the objective and training on oracle labels for
a theft-motivated adversary.
We expect that as semi-supervised learning techniques on
ImageNet mature, further gains should be reÔ¨Çected in the
performance of model extraction attacks.
MixMatch. To validate this hypothesis, we turn to smaller
datasets where semi-supervised learning has made signiÔ¨Åcant
progress. We investigate a technique called MixMatch [38] on
two datasets: SVHN [39] and CIFAR10 [40]. MixMatch uses
a combination of techniques, including training on "guessed"
labels, regularization, and image augmentations.
For both datasets, inputs are color images of 32x32 pixels
belonging to one of 10 classes. The training set of SVHN
contains 73257 images and the test set contains 26032 images.
The training set of CIFAR10 contains 50000 images and the
test set contains 10000 images. We train the oracle with a
WideResNet-28-2 architecture on the labeled training set.
The oracles achieve 97.36% accuracy on SVHN and 95.75%
accuracy on CIFAR10.
The adversary is given access to the same training set but
without knowledge of the labels. Our goal is to validate the
effectiveness of semi-supervised learning by demonstratingthat the adversary only needs to query the oracle on a small
subset of these training points to extract a model whose accu-
racy on the task is comparable to the oracle‚Äôs. To this end, we
run 5 trials of fully supervised extraction (no use of unlabeled
data), and 5 trials of MixMatch, reporting for each trial the
median accuracy of the 20 latest checkpoints, as done in [38].
Results. In Table 3, we Ô¨Ånd that with only 250 queries (293x
smaller label set than the SVHN oracle and 200x smaller
for CIFAR10), MixMatch reaches 95.82% test accuracy on
SVHN and 87.98% accuracy on CIFAR10. This is higher
than fully supervised training that uses 4000 queries. With
4000 queries, MixMatch is within 0.29% of the accuracy of
the oracle on SVHN, and 2.46% on CIFAR10. The variance
of MixMatch is slightly higher than that of fully supervised
training, but is much smaller than the performance gap. These
gains come from the prior MixMatch is able to build using the
unlabeled data, making it effective at exploiting few labels.
We observe similar gains in test set Ô¨Ådelity.
5 Limitations of Learning-Based Extraction
Learning-based approaches have several sources of non-
determinism: the random initializations of the model parame-
ters, the order in which data is assembled to form batches for
SGD, and even non-determinism in GPU instructions [41,42].
Non-determinism impacts the model parameter values ob-
tained from training. Therefore, even an adversary with full ac-
cess to the oracle‚Äôs training data, hyperparameters, etc., would
still need all of the learner‚Äôs non-determinism to achieve the
functionally equivalent extraction goal described in Section 3.
In this section, we will attempt to quantify this: for a strong
adversary, with access to the exact details of the training
setup, we will present an experiment to determine the limits
of learning-based algorithms to achieving Ô¨Ådelity extraction.
We perform the following experiment. We query an ora-
cle to obtain a labeled substitute dataset D. We use Dfor
a learning-based extraction attack which produces a model
f1
q(x). We run the learning-based attack a second time using
D, but with different sources of non-determinism to obtainQuery Set Init & SGD Same SGD Same Init Different
Test 93.7% 93.2% 93.1% 93.4%
Adv Ex 73.6% 65.4% 65.3% 67.1%
Uniform 65.7% 60.2% 59.0% 60.2%
Table 4: Impact of non-determinism on extraction Ô¨Ådelity.
Even models extracted using the same SGD and initialization
randomness as the oracle do not reach 100% Ô¨Ådelity.
a new set of parameters f2
q(x). If there are points xsuch that
f1
q(x)6=f2
q(x), then the prediction on xis dependent not on
the oracle, but on the non-determinism of the learning-based
attack strategy‚Äîwe are unable to guarantee Ô¨Ådelity.
We independently control the initialization randomness and
batch randomness during training on Fashion-MNIST [43]
with fully supervised SGD (we use Fashion-MNIST for train-
ing speed). We repeated each run 10 times and measure agree-
ment between the ten obtained models on the test set, adver-
sarial examples generated by running FGSM with e=25=255
with the oracle model and the test set, and uniformly random
inputs. The oracle uses initialization seed 0 and SGD seed
0‚Äîwe also use two different initialization and SGD seeds.
Even when both training and initialization randomness are
Ô¨Åxed (so that only GPU non-determinism remains), Ô¨Ådelity
peaks at 93.7% on the test set (see Table 4). With no random-
ness Ô¨Åxed, extraction achieves 93.4% Ô¨Ådelity on the test set.
(Agreement on the test set should should be considered in
reference to the base test accuracy of 90%.) Hence, even an
adversary who has the victim model‚Äôs exact training set will
be unable to exceed ~93.4% Ô¨Ådelity. Using prototypicality
metrics, as investigated in Carlini et al. [44], we notice that
test points where Ô¨Ådelity is easiest to achieve are also the
most prototypical (i.e., more representative of the class it is la-
beled as). This connection is explored further in Appendix B.
The experiment of this section is also related to uncertainty
estimation using deep ensembles [42]; we believe a deeper
connection may exist between the Ô¨Ådelity of learning-based
approaches and uncertainty estimation. Also relevant is the
work mentioned earlier in Section 3, that shows that random
networks are hard for learning-based approaches to extract.
Here, we Ô¨Ånd that learning-based approaches have limits even
for trained networks, on some portion of the input space.
It follows from these arguments that non-determinism of
both the victim and extracted model‚Äôs learning procedures
potentially compound, limiting the effectiveness of using a
learning-based approach to reaching high Ô¨Ådelity.
6 Functionally Equivalent Extraction
Having identiÔ¨Åed fundamental limitations that prevent
learning-based approaches from perfectly matching the or-
acle‚Äôs mistakes, we now turn to a different approach where
the adversary extracts the oracle‚Äôs weights directly, seekingto achieve functionally-equivalent extraction.
This attack can be seen as an extension of two prior works.
Milli et al. [19] introduce an attack to extract neural net-
work weights under the assumption that the adversary
is able to make gradient queries . That is, each query
the adversary makes reveals not only the prediction of
the neural network, but also the gradient of the neural
network with respect to the query. To the best of our
knowledge this is the only functionally-equivalent ex-
traction attack on neural networks with one hidden layer,
although it was not actually implemented in practice.
Batina et al. [25], at USENIX Security 2019, develop a
side-channel attack that extracts neural network weights
through monitoring the power use of a microprocessor
evaluating the neural network. This is a much more pow-
erful threat model than made by any of the other model
extraction papers. To the best of our knowledge this is
the only practical direct model extraction result‚Äîthey
manage to extract essentially arbitrary depth networks.
In this section we introduce an attack which only requires
standard queries (i.e., that return the model‚Äôs prediction in-
stead of its gradients) and does not require any side-channel
leakages, yet still manages to achieve higher Ô¨Ådelity extraction
than the side-channel extraction work for two-layer networks,
assuming double-precision inference.
Attack Algorithm Intuition. As in [19], our attack is tai-
lored to work on neural networks with the ReLU activation
function (the ReLU is an effective default choice of activation
function [20]). This makes the neural network a piecewise
linear function. Two samples are within the same linear region
if all ReLU units have the same sign, illustrated in Figure 2.
By Ô¨Ånding adjacent linear regions, and computing the differ-
ence between them, we force a single ReLU to change signs.
Doing this, it is possible to almost completely determine the
weight vector going into that ReLU unit. Repeating this attack
for all ReLU units lets us recover the Ô¨Årst weight matrix com-
pletely. (We say almost here, because we must do some work
to recover the sign of the weight vector.) Once the Ô¨Årst layer of
the two-layer neural network has been determined, the second
layer can be uniquely solved for algebraically through least
squares. This attack is optimal up to a constant factor‚Äîthe
query complexity is discussed in Appendix D.
6.1 Notation and Assumptions
As in [19], we only aim to extract neural networks with one
hidden layer using the ReLU activation function. We denote
the model weights by A(0)2Rdh;A(1)2RhKand biases
byB(0)2Rh;B(1)2RK. Here, d;h, and Krespectively referSymbol DeÔ¨Ånition
d Input dimensionality
h Hidden layer dimensionality ( h0)A(0)
k j
 √•
kA(1)
k1(A(0)
k(xi cej)+B(0)
k>0)A(0)
k j
=A(1)
i
1(A(0)
iej>0)  1( A(0)
iej>0)
A(0)
ji
=(A(0)
jiA(1)
i)
for a c>0small enough so that xicejdoes not Ô¨Çip
any other ReLU. Because xiis a critical point and cis small,
the sums in the second line differ only in the contribution ofReLU i. However at this point we only have a product involv-
ing both weight matrices. We now show this information is
useful.
If we computejA(0)
1iA(1)jandjA(0)
2iA(1)jby querying along
directions e1ande2, we can divide these quantities to obtain
the valuejA(0)
1i=A(0)
2ij, the ratio of the two weights. By repeat-
ing the above process for each input direction we can, for all
k, obtain the pairwise ratios jA(0)
1i=A(0)
kij.
Recall from Section 3 that obtaining the ratios of weights
is the theoretically optimal result we could hope to achieve. It
is always possible to multiply all of the weights intoa ReLU
by a constant c>0and then multiply all of the weights out
of the ReLU byc 1. Thus, without loss of generality, we can
assign A(0)
1i=1and scale the remaining entries accordingly.
Unfortunately, we have lost a small amount of information
here. We have only learned the absolute value of the ratio,
and not the value itself.
6.4.2 Weight Sign Recovery
Once we reconstruct the values jA(0)
ji=A(0)
1ijfor all jwe need
to recover the sign of these values. To do this we consider the
following quantity:
¬∂2OL
¬∂(ej+ek)2
xi=(A(0)
jiA(1)
iA(0)
kiA(1)
i):
That is, we consider what would happen if we take the second
partial derivative in the direction (ej+ek). Their contributions
to the gradient will either cancel out, indicating A0)
jiandA(0)
ki
are of opposite sign, or they will compound on each other,
indicating they have the same sign. Thus, to recover signs, we
can perform this comparison along each direction (e1+ej).
Here we encounter one Ô¨Ånal difÔ¨Åculty. There are a total
ofnsigns we need to recover, but because we compute the
signs by comparing ratios along different directions, we can
only obtain n 1relations. That is, we now know the correct
signed value of A(0)
iup to a single sign for the entire row.
It turns out this is to be expected. What we have computed
is the normal direction to the hyperplane, but because any
given hyperplane can be described by an inÔ¨Ånite number of
normal vectors differing by a constant scalar, we can not hope
to use local information to recover this Ô¨Ånal sign bit.
Put differently, while it is possible to push a constant c>
0through from the Ô¨Årst layer to the second layer, it is not
possible to do this for negative constants, because the ReLU
function is not symmetric. Therefore, it is necessary to learn
the sign of this row.
6.5 Global Sign Recovery
Once we have recovered the input vector‚Äôs weights, we still
don‚Äôt know the sign for the given inputs‚Äîwe only measure the
difference between linear functions at each critical point, butdo not know which side is the positive side of the ReLU [19].
Now, we need to leverage global information in order to rec-
oncile all of inputs‚Äô signs.
Notice that recovering ÀÜA(0)
iallows us to obtain B(0)
iby
using the fact that A(0)
ixi+B(0)
i=0. Then we can compute
ÀÜB(0)
iup to the same global sign as is applied to ÀÜA(0)
i.
Now, to begin recovering sign, we search for a vector z
that is in the null space of ÀÜA(0), that is, ÀÜA(0)z=~0. Because
the neural network has h0.
This allows us to recover the sign bit for ReLU i.
6.6 Last Layer Extraction
Given the completely extracted Ô¨Årst layer, the logit function
of the network is just a linear transformation which we can
recover with least squares, through making hqueries where
each ReLU is active at least once. In practice, we use the
critical points discovered in the previous section so that we
do not need to make additional neural network queries.
6.7 Results
Setup. We train several one-layer fully-connected neu-
ral networks with between 16 and 512 hidden units (for
12,000 and 100,000 trainable parameters, respectively) on the
MNIST [45] and CIFAR-10 datasets [40]. We train the mod-
els with the Adam [23] optimizer for 20 epochs at batch size
128 until they converge. We train Ô¨Åve networks of each size
to obtain higher statistical signiÔ¨Åcance. Accuracies of these
networks can be found in the supplement in Appendix C. In
Section 4, we used 140,000 217queries for ImageNet model
extraction. This is comparable to the number of queries used
to extract the smallest MNIST model in this section, high-
lighting the advantages of both approaches.
MNIST Extraction. We implement the functionally-
equivalent extraction attack in JAX [46] and run it on each
trained oracle. We measure the Ô¨Ådelity of the extracted model,
comparing predicted labels, on the MNIST test set.
Results are summarized in Table 6. For smaller networks,
we achieve 100% Ô¨Ådelity on the test set: every single oneof the 10;000test examples is predicted the same. As the
network size increases, low-probability errors we encounter
become more common, but the extracted neural network still
disagrees with the oracle on only 2 of the 10 ;000 examples.
Inspecting the weight matrix that we extract and comparing
it to the weight matrix of the oracle classiÔ¨Åer, we Ô¨Ånd that we
manage to reconstruct the Ô¨Årst weight matrix to an average
precision of 23 bits‚Äîwe provide more results in Appendix C.
CIFAR-10 Extraction. Because this attack is data-
independent, the underlying task is unimportant for how well
the attack works; only the number of parameters matter. The
results for CIFAR-10 are thus identical to MNIST when con-
trolling for model size: we achieve 100% test set agreement on
models with fewer than 200;000parameters and and greater
than 99% test set agreement on larger models.
Comparison to Prior Work. To the best of our knowledge,
this is by orders of magnitude the highest Ô¨Ådelity extraction
of neural network weights.
The only fully-implemented neural network extraction at-
tack we are aware of is the work of Batina et al. [25], who
uses an electromagnetic side channels and differential power
analysis to recover an MNIST neural network with neural
network weights with an average error of 0.0025. In com-
parison, we are able to achieve an average error in the Ô¨Årst
weight matrix for a similarly sized neural network of just
0.0000009‚Äîover two thousand times more precise. To the
best of our knowledge no functionally-equivalent CIFAR-10
models have been extracted in the past.
We are unable to make a comparison between the Ô¨Ådelity
of our extraction attack and the Ô¨Ådelity of the attack presented
in Batina et al. because they do not report on this number:
they only report the accuracy of the extracted model and show
it is similar to the original model. We believe this strengthens
our observation that comparing across accuracy and Ô¨Ådelity
is not currently widely accepted as best practice.
Investigating Errors. We observe that as the number of pa-
rameters that must be extracted increases, the Ô¨Ådelity of the
model decreases. We investigate why this happens and discov-
ered that a small fraction of the time (roughly 1 in 10,000) the
gradient estimation procedure obtains an incorrect estimate
of the gradient and therefore one of the extracted weights ÀÜA(0)
i j
is incorrect by a non-insigniÔ¨Åcant margin.
Introducing an error into just one of the weights of the
Ô¨Årst matrix ÀÜA(0)should not induce signiÔ¨Åcant further errors.
However, because of this error, when we solve for the bias
vector, the extracted bias ÀÜB(0)
iwill have error proportional to
the error of ÀÜA(0)
i j. And when the bias is wrong, it impacts every
calculation, even those where this edge is not in use.
Resolving this issue completely either requires reducing
the failure rate of gradient estimation from 1 in 10,000 to
practically 0, or would require a complex error-recovery pro-
cedure. Instead, we will introduce in the following section an
improvement which almost completely solves this issue.# of Parameters 12,500 25,000 50,000 100,000
Fidelity 100% 100% 100% 99.98%
Queries 217:2218:2219:2220:2
Table 6: Fidelity of the functionally-equivalent extraction
attack across different test distributions on an MNIST victim
model. Results are averaged over Ô¨Åve extraction attacks. For
small models, we achieve perfect Ô¨Ådelity extraction; larger
models have near-perfect Ô¨Ådelity on the test data distribution,
but begins to lose accuracy at 100 ;000 parameters.
DifÔ¨Åculties Extending the Attack. The attack is speciÔ¨Åc to
two layer neural networks; deeper networks pose multiple
difÔ¨Åculties. In deep networks, the critical point search step of
Section 6.3 will result in critical points from many different
layers, and determining which layer a critical point is on is
nontrivial. Without knowing which layer a critical point is on,
we cannot control inputs to the neuron, which we need to do
to recover the weights in Section 6.4. Even given knowledge
of what layer a critical point is on, the inputs of any neuron
past layer 1 are the outputs of other neurons, so we only
have indirect control over their inputs. Finally, even with the
ability to recover these weights, small numerical errors occur
in the Ô¨Årst layer extraction. These cause errors in every Ô¨Ånite
differences computation in further layers, causing the second
layer to have even larger numerical errors than the Ô¨Årst (and
so on). Therefore, extending the attack to deeper networks
will require at least solving each of the following: producing
critical points belonging to a speciÔ¨Åc layer, recovering weights
for those neurons without direct control of their inputs, and
signiÔ¨Åcantly reducing numerical errors in these algorithms.
7 Hybrid Strategies
Until now the strategies we have developed for extraction
have been pure and focused entirely on learning or entirely
on direct extraction. We now show that there is a continuous
spectrum from which we can draw attack strategies, and these
hybrid strategies can leverage both the query efÔ¨Åciency of
learning extraction, and the Ô¨Ådelity of direct extraction.
7.1 Learning-Based Extraction with Gradient
Matching
Milli et al. demonstrate that gradient matching helps extrac-
tion by optimizing the objective function
n
√•
i=1H(O(xi);f(xi))+aj√ëxO(xi) √ëxf(xi)j2
2;
assuming the adversary can query the model for √ëxO(x). This
is more model access than we permit our adversary, but is an# of Parameters 50,000 100,000 200,000 400,000
Fidelity 100% 100% 99.95% 99.31%
Queries 219:2220:2221:2222:2
Table 7: Fidelity of extracted MNIST model is improved with
the hybrid strategy. Note when comparing to Table 6 the
model sizes are 4larger.
example of using intuition from direct recovery to improve
extraction. We found in preliminary experiments that this
technique can improve Ô¨Ådelity on small datasets (increasing
Ô¨Ådelity from 95% to 96.5% on Fashion-MNIST), but we leave
scaling and removing the model access assumption of this
technique to future work. Next, we will show another com-
bination of learning and direct recovery, using learning to
alleviate some of the limitations of the previous functionally-
equivalent extraction attack.
7.2 Error Recovery through Learning
Recall from earlier that the functionally-equivalent extraction
attack Ô¨Ådelity degrades as the model size increases. This is
a result of low-probability errors in the Ô¨Årst weight matrix
inducing incorrect biases on the Ô¨Årst layer, which in turn
propagates and causes worse errors in the second layer.
We now introduce a method for performing a learning-
based error recovery routine. While performing a fully-
learning-based attack leaves too many free variables so that
functionally-equivalent extraction is not possible, if we Ô¨Åx
many of the variables to the values extracted through the di-
rect recovery attack, we now show it is possible to learn the
remainder of the variables.
Formally, let ÀÜA(0)be the extracted weight matrix for the Ô¨Årst
layer and ÀÜB(0)be the extracted bias vector for the Ô¨Årst layer.
Previously, we used least squares to directly solve for ÀÜA(1)
and ÀÜB(1)assuming we had extracted the Ô¨Årst layer perfectly.
Here, we relax this assumption. Instead, we perform gradient
descent optimizing for parameters W0::2that minimize
Ex2Dfq(x) W1ReLU (ÀÜA(0)x+ÀÜB(0)+W0)+W2
That is, we use a single trainable parameter to adjust the
bias term of the Ô¨Årst layer, and then solve (via gradient descent
with training data) for the remaining weights accordingly.
This hybrid strategy increases the Ô¨Ådelity of the ex-
tracted model substantially, detailed in Table 8. In the worst-
performing example from earlier (with only direct extraction)
the extracted 128-neuron network had 80% Ô¨Ådelity agreement
with the victim model. When performing learning-based re-
covery, the Ô¨Ådelity agreement jumps all the way to 99 :75%.# of Parameters 50,000 100,000 200,000 400,000
Transferability 100% 100% 100% 100%
Table 8: Transferability rate of adversarial examples using the
extracted neural network from our Section 7 attack.
7.2.1 Transferability
Adversarial examples transfer : an adversarial example [47]
generated on one model often fools different models, too.
Transferability is higher when the models are more similar [7].
We should therefore expect that we can generate adversar-
ial examples on our extracted model, and that these will fool
the remote oracle nearly always. In order to measure transfer-
ability, we run 20 iterations of PGD [48] with `¬•distortion
set to the value most often used in the literature: for MNIST:
0:1, and for CIFAR-10: 0 :03.
The attack achieves functionally equivalent extraction
(modulo Ô¨Çoating point precision errors in the extracted
weights), so we expect it to have high adversarial example
transferability. Indeed, we Ô¨Ånd we achieve a 100% transfer-
ability success rate for all extracted models.
8 Related Work
Defenses for model extraction have fallen into two camps:
limiting the information gained per query, and differentiating
extraction adversaries from benign users. Approaches to lim-
iting information include perturbing the probabilities returned
by the model [11,13,49], removing the probabilities for some
of the model‚Äôs classes [11], or returning only the class out-
put [11, 13]. Another proposal has considered sampling from
a distribution over model parameters [13,50]. The other camp,
differentiating benign from malicious users, has focused on
analyzing query patterns [51, 52]. Non-adaptive attacks (such
as supervised or MixMatch extraction) bypass query pattern-
based detection, and are weakened by information limiting.
We demonstrate the impact of removing complete access to
probability values by considering only access to top 5 prob-
abilities from WSL in Table 2. Our functionally-equivalent
attack is broken by all of these measures. We leave considera-
tion of defense-aware attacks to future work.
Queries to a model can also reveal hyperparameters [53] or
architectural information [14]. Adversaries can use side chan-
nel attacks to do the same [18, 25]. These are orthogonal to,
but compatible with, our work‚Äîinformation about a model,
such as assumptions made in Section 6, empowers extraction.
Watermarking neural networks has been proposed [54, 55]
to identify extracted models. Model extraction calls into ques-
tion the utility of cryptographic protocols used to protect
model weights. One unrealized approach is obfuscation [56],
where an equivalent program could be released and queried asmany times as desired. A practical approach is secure multi-
party computation, where each query is computed by running
a protocol between the model owner and querier [57].
9 Conclusion
This paper characterizes and explores the space of model
extraction attacks on neural networks. We focus this paper
speciÔ¨Åcally around the objectives of accuracy , to measure the
success of a theft-motivated adversary, and Ô¨Ådelity , an often-
overlooked measure which compares the agreement between
models to reÔ¨Çect the success of a recon-motivated adversary.
Our learning-based methods can effectively attack a model
with several millions of parameters trained on a billion images,
and allows the attacker to reduce the error rate of their model
by 10%. This attack does not match perfect Ô¨Ådelity with the
victim model due to what we show are inherent limitations of
learning-based approaches: nondeterminism (including only
the nondeterminism on the GPU) prohibits training identical
models. In contrast, our direct functionally-equivalent extrac-
tion returns a neural network agreeing with the victim model
on100% of the test samples and having 100% Ô¨Ådelity on
transfered adversarial examples.
We then propose a hybrid method which uniÔ¨Åes these two
attacks, using learning-based approaches to recover from nu-
merical instability errors when performing the functionally-
equivalent extraction attack.
Our work highlights many remaining open problems in
model extraction, such as reducing the capabilities required
by our attacks and scaling functionally-equivalent extraction.
Acknowledgements
We would like to thank Ilya Mironov for lengthy and fruitful
discussions regarding the functionally equivalent extraction
attack. We also thank √ölfar Erlingsson for helpful discussions
on positioning the work, and Florian Tram√®r for his comments
on an early draft of this paper.
References
[1]E. Strubell, A. Ganesh, and A. McCallum, ‚ÄúEnergy and
policy considerations for deep learning in nlp,‚Äù arXiv
preprint arXiv:1906.02243 , 2019.
[2]Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhut-
dinov, and Q. V . Le, ‚ÄúXlnet: Generalized autoregressive
pretraining for language understanding,‚Äù in Advances in
neural information processing systems , 2019, pp. 5754‚Äì
5764.
[3]A. Halevy, P. Norvig, and F. Pereira, ‚ÄúThe unreasonable
effectiveness of data,‚Äù 2009.[4]J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and
L. Fei-Fei, ‚ÄúImagenet: A large-scale hierarchical image
database,‚Äù in 2009 IEEE conference on computer vision
and pattern recognition . Ieee, 2009, pp. 248‚Äì255.
[5]I. Sutskever, O. Vinyals, and Q. V . Le, ‚ÄúSequence to
sequence learning with neural networks,‚Äù in Neural in-
formation processing systems , 2014, pp. 3104‚Äì3112.
[6]A. Van Den Oord, S. Dieleman, H. Zen, K. Simonyan,
O. Vinyals, A. Graves, N. Kalchbrenner, A. W. Senior,
and K. Kavukcuoglu, ‚ÄúWavenet: A generative model for
raw audio.‚Äù SSW, vol. 125, 2016.
[7]N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B.
Celik, and A. Swami, ‚ÄúPractical black-box attacks
against machine learning,‚Äù in Proceedings of the 2017
ACM on Asia conference on computer and communica-
tions security . ACM, 2017, pp. 506‚Äì519.
[8]D. Lowd and C. Meek, ‚ÄúAdversarial learning,‚Äù in Pro-
ceedings of the eleventh ACM SIGKDD international
conference on Knowledge discovery in data mining .
ACM, 2005, pp. 641‚Äì647.
[9]R. Shokri, M. Stronati, C. Song, and V . Shmatikov,
‚ÄúMembership inference attacks against machine learn-
ing models,‚Äù in 2017 IEEE Symposium on Security and
Privacy (SP) . IEEE, 2017, pp. 3‚Äì18.
[10] A. Salem, Y . Zhang, M. Humbert, P. Berrang, M. Fritz,
and M. Backes, ‚ÄúMl-leaks: Model and data independent
membership inference attacks and defenses on machine
learning models,‚Äù arXiv preprint arXiv:1806.01246 ,
2018.
[11] F. Tram√®r, F. Zhang, A. Juels, M. K. Reiter, and T. Ris-
tenpart, ‚ÄúStealing machine learning models via pre-
diction apis,‚Äù in 25thfUSENIXgSecurity Symposium
(fUSENIXgSecurity 16) , 2016, pp. 601‚Äì618.
[12] T. Orekondy, B. Schiele, and M. Fritz, ‚ÄúKnockoff nets:
Stealing functionality of black-box models,‚Äù in Proceed-
ings of the IEEE Conference on Computer Vision and
Pattern Recognition , 2019, pp. 4954‚Äì4963.
[13] V . Chandrasekaran, K. Chaudhuri, I. Giacomelli, S. Jha,
and S. Yan, ‚ÄúModel extraction and active learning,‚Äù
CoRR , vol. abs/1811.02054, 2018. [Online]. Available:
http://arxiv.org/abs/1811.02054
[14] S. J. Oh, M. Augustin, B. Schiele, and M. Fritz, ‚ÄúTo-
wards reverse-engineering black-box neural networks,‚Äù
arXiv preprint arXiv:1711.01768 , 2017.
[15] S. Pal, Y . Gupta, A. Shukla, A. Kanade, S. K. Shevade,
and V . Ganapathy, ‚ÄúA framework for the extraction
of deep neural networks by leveraging public data,‚ÄùCoRR , vol. abs/1905.09165, 2019. [Online]. Available:
http://arxiv.org/abs/1905.09165
[16] J. R. Correia-Silva, R. F. Berriel, C. Badue, A. F.
de Souza, and T. Oliveira-Santos, ‚ÄúCopycat cnn: Steal-
ing knowledge by persuading confession with random
non-labeled data,‚Äù in 2018 International Joint Confer-
ence on Neural Networks (IJCNN) . IEEE, 2018.
[17] C. Song and V . Shmatikov, ‚ÄúOverlearning reveals sensi-
tive attributes,‚Äù arXiv preprint arXiv:1905.11742 , 2019.
[18] S. Hong, M. Davinroy, Y . Kaya, S. N. Locke, I. Rackow,
K. Kulda, D. Dachman-Soled, and T. Dumitra¬∏ s, ‚ÄúSecu-
rity analysis of deep neural networks operating in the
presence of cache side-channel attacks,‚Äù arXiv preprint
arXiv:1810.03487 , 2018.
[19] S. Milli, L. Schmidt, A. D. Dragan, and M. Hardt,
‚ÄúModel reconstruction from model explanations,‚Äù arXiv
preprint arXiv:1807.05185 , 2018.
[20] V . Nair and G. E. Hinton, ‚ÄúRectiÔ¨Åed linear units im-
prove restricted boltzmann machines,‚Äù in Proceedings
of the 27th international conference on machine learn-
ing (ICML-10) , 2010, pp. 807‚Äì814.
[21] Y . E. Nesterov, ‚ÄúA method for solving the convex pro-
gramming problem with convergence rate o (1/kÀÜ 2),‚Äù in
Dokl. akad. nauk Sssr , vol. 269, 1983, pp. 543‚Äì547.
[22] J. Duchi, E. Hazan, and Y . Singer, ‚ÄúAdaptive subgradient
methods for online learning and stochastic optimization,‚Äù
Journal of Machine Learning Research , vol. 12, no. Jul,
pp. 2121‚Äì2159, 2011.
[23] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic
optimization,‚Äù arXiv preprint arXiv:1412.6980 , 2014.
[24] G. Hinton, O. Vinyals, and J. Dean, ‚ÄúDistilling the
knowledge in a neural network,‚Äù arXiv preprint
arXiv:1503.02531 , 2015.
[25] L. Batina, S. Bhasin, D. Jap, and S. Picek, ‚ÄúCsi neu-
ral network: Using side-channels to recover your ar-
tiÔ¨Åcial neural network information,‚Äù arXiv preprint
arXiv:1810.09076 , 2018.
[26] P. Kocher, J. Jaffe, and B. Jun, ‚ÄúDifferential power anal-
ysis,‚Äù in Annual International Cryptology Conference .
Springer, 1999, pp. 388‚Äì397.
[27] A. Das, S. Gollapudi, R. Kumar, and R. Panigrahy, ‚ÄúOn
the learnability of deep random networks,‚Äù CoRR , vol.
abs/1904.03866, 2019.
[28] D. Mahajan, R. Girshick, V . Ramanathan, K. He,
M. Paluri, Y . Li, A. Bharambe, and L. van der Maaten,
‚ÄúExploring the limits of weakly supervised pretraining,‚ÄùinProceedings of the European Conference on Com-
puter Vision (ECCV) , 2018, pp. 181‚Äì196.
[29] P. Micaelli and A. Storkey, ‚ÄúZero-shot knowledge trans-
fer via adversarial belief matching,‚Äù arXiv preprint
arXiv:1905.09768 , 2019.
[30] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and
I. Sutskever, ‚ÄúLanguage models are unsupervised multi-
task learners,‚Äù OpenAI Blog , vol. 1, no. 8, 2019.
[31] A. Sharif Razavian, H. Azizpour, J. Sullivan, and
S. Carlsson, ‚ÄúCnn features off-the-shelf: an astounding
baseline for recognition,‚Äù in Proceedings of the IEEE
conference on computer vision and pattern recognition
workshops , 2014, pp. 806‚Äì813.
[32] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,
‚ÄúBert: Pre-training of deep bidirectional transform-
ers for language understanding,‚Äù arXiv preprint
arXiv:1810.04805 , 2018.
[33] D. Angluin, ‚ÄúQueries and concept learning,‚Äù Machine
learning , vol. 2, no. 4, pp. 319‚Äì342, 1988.
[34] A. Blum and T. Mitchell, ‚ÄúCombining labeled and un-
labeled data with co-training,‚Äù in Proceedings of the
eleventh annual conference on Computational learning
theory . Citeseer, 1998, pp. 92‚Äì100.
[35] S. Song, D. Berthelot, and A. Rostamizadeh, ‚ÄúCom-
bining mixmatch and active learning for better
accuracy with fewer labels,‚Äù 2020. [Online]. Available:
https://openreview.net/forum?id=HJxWl0NKPB
[36] O. Sim√©oni, M. Budnik, Y . Avrithis, and G. Gravier,
‚ÄúRethinking deep active learning: Using unlabeled
data at model training,‚Äù 2020. [Online]. Available:
https://openreview.net/forum?id=rJehllrtDS
[37] X. Zhai, A. Oliver, A. Kolesnikov, and L. Beyer,
‚ÄúS4l: Self-supervised semi-supervised learning,‚Äù arXiv
preprint arXiv:1905.03670 , 2019.
[38] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot,
A. Oliver, and C. Raffel, ‚ÄúMixmatch: A holistic ap-
proach to semi-supervised learning,‚Äù arXiv preprint
arXiv:1905.02249 , 2019.
[39] Y . Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and
A. Y . Ng, ‚ÄúReading digits in natural images with unsu-
pervised feature learning,‚Äù 2011.
[40] A. Krizhevsky et al. , ‚ÄúLearning multiple layers of fea-
tures from tiny images,‚Äù Citeseer, Tech. Rep., 2009.
[41] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips,
D. Ebner, V . Chaudhary, M. Young, J.-F. Crespo, andD. Dennison, ‚ÄúHidden technical debt in machine learn-
ing systems,‚Äù in Advances in neural information pro-
cessing systems , 2015, pp. 2503‚Äì2511.
[42] B. Lakshminarayanan, A. Pritzel, and C. Blundell, ‚ÄúSim-
ple and scalable predictive uncertainty estimation using
deep ensembles,‚Äù in Advances in Neural Information
Processing Systems , 2017, pp. 6402‚Äì6413.
[43] H. Xiao, K. Rasul, and R. V ollgraf. (2017) Fashion-
mnist: a novel image dataset for benchmarking machine
learning algorithms.
[44] N. Carlini, U. Erlingsson, and N. Papernot, ‚ÄúPrototypical
examples in deep learning: Metrics, characteristics, and
utility,‚Äù 2019. [Online]. Available: https://openreview.
net/forum?id=r1xyx3R9tQ
[45] Y . LeCun, L. Bottou, Y . Bengio, P. Haffner et al. ,
‚ÄúGradient-based learning applied to document recog-
nition,‚Äù Proceedings of the IEEE , vol. 86, no. 11, pp.
2278‚Äì2324, 1998.
[46] Google, ‚ÄúJax,‚Äù https://github.com/google/jax, 2019.
[47] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Er-
han, I. Goodfellow, and R. Fergus, ‚ÄúIntriguing properties
of neural networks,‚Äù arXiv preprint arXiv:1312.6199 ,
2013.
[48] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and
A. Vladu, ‚ÄúTowards deep learning models resistant to
adversarial attacks,‚Äù arXiv preprint arXiv:1706.06083 ,
2017.
[49] T. Lee, B. Edwards, I. Molloy, and D. Su, ‚ÄúDefending
against model stealing attacks using deceptive perturba-
tions,‚Äù arXiv preprint arXiv:1806.00054 , 2018.
[50] I. M. Alabdulmohsin, X. Gao, and X. Zhang, ‚ÄúAdding
robustness to support vector machines against adver-
sarial reverse engineering,‚Äù in Proceedings of the 23rd
ACM International Conference on Conference on Infor-
mation and Knowledge Management . ACM, 2014, pp.
231‚Äì240.
[51] M. Juuti, S. Szyller, A. Dmitrenko, S. Marchal, and
N. Asokan, ‚ÄúPrada: protecting against dnn model steal-
ing attacks,‚Äù arXiv preprint arXiv:1805.02628 , 2018.
[52] M. Kesarwani, B. Mukhoty, V . Arya, and S. Mehta,
‚ÄúModel extraction warning in mlaas paradigm,‚Äù in Pro-
ceedings of the 34th Annual Computer Security Appli-
cations Conference . ACM, 2018, pp. 371‚Äì380.
[53] B. Wang and N. Z. Gong, ‚ÄúStealing hyperparameters in
machine learning,‚Äù in 2018 IEEE Symposium on Secu-
rity and Privacy (SP) . IEEE, 2018, pp. 36‚Äì52.[54] J. Zhang, Z. Gu, J. Jang, H. Wu, M. P. Stoecklin,
H. Huang, and I. Molloy, ‚ÄúProtecting intellectual prop-
erty of deep neural networks with watermarking,‚Äù in
Proceedings of the 2018 on Asia Conference on Com-
puter and Communications Security . ACM, 2018, pp.
159‚Äì172.
[55] Y . Uchida, Y . Nagai, S. Sakazawa, and S. Satoh, ‚ÄúEm-
bedding watermarks into deep neural networks,‚Äù in Pro-
ceedings of the 2017 ACM on International Conference
on Multimedia Retrieval . ACM, 2017, pp. 269‚Äì277.
[56] B. Barak, O. Goldreich, R. Impagliazzo, S. Rudich,
A. Sahai, S. Vadhan, and K. Yang, ‚ÄúOn the (im) possi-
bility of obfuscating programs,‚Äù in Annual international
cryptology conference . Springer, 2001, pp. 1‚Äì18.
[57] M. Barni, C. Orlandi, and A. Piva, ‚ÄúA privacy-preserving
protocol for neural-network-based computation,‚Äù in Pro-
ceedings of the 8th workshop on Multimedia and secu-
rity. ACM, 2006, pp. 146‚Äì151.
[58] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J.
Kochenderfer, ‚ÄúReluplex: An efÔ¨Åcient smt solver for
verifying deep neural networks,‚Äù in International Con-
ference on Computer Aided VeriÔ¨Åcation . Springer,
2017, pp. 97‚Äì117.
A Formal Statements for Section 3.3
Here, we give the formal arguments for the difÔ¨Åculty of model
extraction to support informal statements from Section 3.3.
Theorem 1. There exists a class of width 3kand depth 2
neural networks on domain [0;1]d(with precision pnumbers)
with dkthat require, given logit access to the networks,
Q(pk)queries to extract.
In order to prove Theorem 1, we introduce a family of
functions we call k-rectangle bounded functions , which we
will show satisÔ¨Åes this property.
DeÔ¨Ånition A.1. A function fon domain [0;1]dwith range R
is arectangle bounded function if there exists two vectors a;b
such that f(x)6=0=)axb, wheredenotes element-
wise comparison. The function fis ak-rectangle bounded
function if there are k indices i such that a i6=0or b i6=1.
Intuitively, a k-rectangle function only outputs a non-zero
value on a multidimensional rectangle that is constrained
in only kcoordinates. We begin by showing that we can
implement k-rectangle functions for any a;busing a ReLU
network of width kand depth 2.
Lemma 1. For any a;bwith kindices isuch that ai6=0or
bi6=1, we can construct a k-rectangle bounded function for
a;b with a ReLU network of width 3k and depth 2.Proof. We will start by constructing a 3-ReLU gadget with
output1only when aixibi. We will then show how
to compose kof these gadgets, one for each index of the
k-rectangle, to construct the k-rectangle bounded function.
The 3-ReLU gadget only depends on xi, so weights for
all other ReLUs will be set to 0. Observe that the func-
tionTi(x;a;b) =ReLU (x a)+ReLU (xi bi) 2ReLU (xi 
(ai+bi)=2)is nonzero only on the interval (ai;bi). This is
easier to see when it is written as
ReLU (xi ai) ReLU (xi (ai+bi)=2)
 (ReLU (xi (ai+bi)=2) ReLU (xi bi)):
The function ReLU (x x1) ReLU (x x2)with x1<
>:0 xx1
x x1x1xx2
x2 x1xx
Now, Ti(x;ai;bi)1=(bi ai)has range [0;1]for any value
ofai;bi. Then the function
fa;b(x) =ReLU (√•
i(Ti(x;ai;bi)=(bi ai)) (k 1))
isk-rectangle bounded for vectors a;b. To see why,
we need that no input xnot satisfying axbhas
√•i(Ti(x;ai;bi)=(bi ai))>k 1. This is simply because each
term Ti(x;ai;bi)1, so unless all ksuch terms are >0, the
inequality cannot hold.
Now that we know how to construct a k-rectangle bounded
function, we will introduce a set of pkdisjoint k-rectangle
bounded functions, and then show that any one requires pk
queries to extract when the others are also possible functions.
Lemma 2. There exists a family of k-rectangle bounded func-
tionsFsuch that extracting an element of Frequires pk
queries in the worst case.
Here, pis the feature precision; images with 8-bit pixels
have p=256.
Proof. We begin by constructing F. The following pranges
are clearly pairwise disjoint: f(i 1
p;i
p)gp
i=1. Then pick any k
indices, and we can construct pkdistinct k-rectangle bounded
functions - one for each element in the Cartesian product of
each index‚Äôs set of ranges. Call this set F.
The set of inputs with non-zero output is distinct for each
function, because their rectangles are distinct. Now consider
the information gained from any query. If the query returns
a non-zero value, the function is learned. If not, at most one
function from Fis ruled out - the function whose rectangle
was queried. Then any sequence of nqueries to an oracle
can rule out at most nof the functions of F, so that at least
jFj=pkqueries are required in the worst case.
Figure 5: Fidelity is easier on more prototypical examples.
Putting Lemma 1 and 2 together gives us Theorem 1.
Theorem 2. Checking whether two networks with domains
f0;1gdare functionally equivalent is NP-hard.
Proof. We prove this by reduction to subset sum. A similar
reduction (reducing to 3-SAT instead of Subset Sum) for a
different statement appears in [58].
Suppose we receive a subset sum instance
T;p;[v1;v2;;vd]- the set is v, the target sum is T,
and the problem‚Äôs precision is p. We will construct networks
f1and f2such that checking if f1and f2are functionally
equivalent is equivalent to solving the subset sum instance.
We start by setting f1=0- it never returns a non-zero value.
We now construct a network f2that has nonzero output only
if the subset sum instance can be solved (and Ô¨Ånding an input
with nonzero output reveals the satisfying subset).
The network f2has three hidden units in the Ô¨Årst layer with
incoming weight for the ith feature equal to vi. This means
the dot product of the input xwith weights will be the sum of
the subsetfijxi=1g. We want to force this to accept iff there
is an input where this sum is T. To do so, we use the same
3-ReLU gadget as in the proof of Theorem 1:
f2(x;T;p;v) =ReLU (xv (T p=2))
+ReLU (xv (T+p=2)) 2ReLU (xv T):
As before, this will only be nonzero in the range [T p=2;T+
p=2], and we are done.
B Prototypicality and Fidelity
We know from Section 5 that learning strategies struggle to
achieve perfect Ô¨Ådelity due to non-determinism inherent in
learning. What remains to be understood is whether somesamples are more difÔ¨Åcult than others to achieve Ô¨Ådelity on.
We investigate using recent work on identifying prototypical
data points. Using each metric developed in Carlini et al. [44],
we can rank the Fashion-MNIST test set in order of increas-
ing prototypicality. Binning the prototypicality ranking into
percentiles, we can measure how many of the 90 models we
trained for Section 5 agree with the oracle‚Äôs prediction. The
intuition here is that more prototypical examples should be
more consistently learnable, whereas more outlying points
may be harder to consistently classify. Indeed, we Ô¨Ånd that
this is the case - all metrics Ô¨Ånd a correlation between proto-
typicality and model agreement (Ô¨Ådelity), as seen in Figure 5.
Interestingly, the metrics which do not use ensembles of mod-
els (adversarial distance and holdout-retraining) have the best
correlation with the model agreement metric‚Äîroughly the top
50% of prototypical examples by these metrics are classiÔ¨Åed
the same by nearly all 90 models.
C Supplement for Section 6
Accuracies for the oracles in Section 6 are found in Table 9.
MNIST CIFAR-10
Parameters Accuracy Parameters Accuracy
12,500 94.3% 49,000 29.2%
25,000 95.6% 98,000 34.2%
50,000 97.2% 196,000 40.3%
100,000 97.7% 393,000 42.6%
200,000 98.0% 786,000 43.1%
400,000 98.3% 1,572,000 45.9%
Table 9: Statistics for the oracle models we train to extract.
Figure 6 shows a distribution over the bits of precision in
the difference between the logits (i.e., pre-softmax prediction)
of the 16 neuron oracle neural network and the extracted
network. Formally, we measure the magnitude of the gap
jfq(x) fÀÜq(x)j. Notice that this is a different (and typically
stronger) measure of Ô¨Ådelity than used elsewhere in the paper.
D Query Complexity of Functionally Equiva-
lent Extraction
In this section, we brieÔ¨Çy analyze the query complexity of
the attack from Section 6. We assume that a simulated partial
derivative requires O(1)queries using Ô¨Ånite differences.
1.Critical Point Search. This step is the most nontrivial to
analyze, but fortunately this was addressed in [19]. They
found this step requires O(hlog(h))gradient queries,
which we simulate with O(hlog(h))model queries.
2.Weight Recovery. This piece is signiÔ¨Åcantly compli-
cated by not having access to gradient queries. For each
5 10 15
Bits of precision in logits0100200300400500FrequencyFigure 6: For a 16-neuron MNIST model the attack works.
Plotted here is number of bits of precision on the logits nor-
malized by the value of the lot as done in the prior Ô¨Ågure.
ReLU , absolute value recovery requires O(d)queries
and weight sign recovery requires an additional O(d),
making this step take O(dh)queries total.
3.Global Sign Recovery. For each ReLU , we require only
three queries. Then this step is O(h).
4.Last Layer Extraction. This step requires hqueries to
make the system of linear equations full rank (although
in practice we reuse previous queries here, making this
step require 0 queries).
Overall, the algorithm requires O(hlog(h) +dh+h) =
O(dh)queries. Extraction requires W(dh)queries without
auxillary information, as there are dhparameters in the model.
Then the algorithm is query-optimal up to a constant factor,
removing logarithmic factors from Milli et al. [19].