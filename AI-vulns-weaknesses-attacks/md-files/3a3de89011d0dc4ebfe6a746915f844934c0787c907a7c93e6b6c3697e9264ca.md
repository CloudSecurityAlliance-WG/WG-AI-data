3/22/24, 2:25 PM LLM Prompt Injection: Indirect | MITRE ATLAS™
https://atlas.mitre.org/techniques/AML.T0051.001/ 1/3Home Techniques LLM Prompt Injection
LLM Prompt Injection:
Indirect󰅂 󰅂 󰅂
Indirect 󰅂
Summary
An adversary may inject prompts indirectly via separate data channel ingested by
the LLM such as include text or multimedia pulled from databases or websites.
These malicious prompts may be hidden or obfuscated from the user . This type of
injection may be used by the adversary to gain a foothold in the system or to target
an unwitting user of the system.
ID: AML.T0051.001
Case Studies: Indirect Prompt Injection Threats: Bing Chat Data Pirate, ChatGPT Plugin
Privacy Leak
Other subtechniques: LLM Prompt Injection: Direct
Parent Technique: LLM Prompt Injection
Number of Tactics: 4
Case Studies 󰅃
Indirect Prompt Injection Threats: Bing Chat Data Pirate
ChatGPT Plugin Privacy Leak󰍜 󰇙3/22/24, 2:25 PM LLM Prompt Injection: Indirect | MITRE ATLAS™
https://atlas.mitre.org/techniques/AML.T0051.001/ 2/3Other Subtechniques 󰅃
LLM Prompt Injection: Direct
Parent Technique󰅃
LLM Prompt Injection
Tactics󰅃
Initial Access
Persistence
Defense Evasion
Privilege Escalation󰍜 󰇙3/22/24, 2:25 PM LLM Prompt Injection: Indirect | MITRE ATLAS™
https://atlas.mitre.org/techniques/AML.T0051.001/ 3/3Join our collaborative community
to shape future tool and
framework developments in AI
security , threat mitigation, bias,
privacy and other critical aspects
of AI assurance.www .mitre.org
© 2021-2024 The MITRE Corporation. All Rights Reserved.
Approved for Public Release; Distribution Unlimited. Case Number 21-2363.
MITRE ATLAS™ and MITRE ATT&CK are a trademark and registered trademark of The MITRE
Corporation.
Privacy Policy | Terms of Use | Manage CookiesCONNECT WITH US
󰅂
®󰍜 󰇙