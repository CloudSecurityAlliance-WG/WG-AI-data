Status: SUCCESS
Message:  # Summary

The document discusses the technique of poisoning training data used in machine learning models. It explains that adversaries may modify the underlying data or labels to embed vulnerabilities in models trained on that data. This allows them to later activate those vulnerabilities through malicious data samples. Poisoned data can be introduced through compromising the ML supply chain or by gaining initial access and modifying data directly. The document provides examples of poisoning attacks against VirusTotal and Microsoft's Tay chatbot. It recommends mitigations like limiting release of model artifacts, controlling access to models and data, and sanitizing training data. Overall, the document analyzes a method for developing persistent vulnerabilities in ML systems by manipulating training data.

# Summary bullet points

* Adversaries can poison ML training data to embed vulnerabilities in models, without easy detection
* Modifying data or labels allows later activation of vulnerabilities using malicious data samples 
* Poisoning can occur via supply chain compromise or by gaining initial access to modify data
* Examples include attacks against VirusTotal and Microsoft's Tay chatbot
* Mitigations include limiting model release, restricting access, and sanitizing training data
* Data poisoning provides a way to develop persistent access to ML systems

# Geographic information

Not applicable

# Type of content

Webpage
================================================================================
METADATA:
prompt_tokens: 620
answer_tokens: 211
time_taken: 13.29 seconds
